---
title: 'Production Analysis | Last updated: `r format(Sys.time(),"%m.%d.%Y")`'
author: "Dorothy Sweet  \nUniversity of Minnesota | Applied Plant Sciences  \nPlant
  Breeding and Plant Molecular Genetics Track\n"
output: 
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '5'
  html_notebook:
    code_folding: hide
    highlight: zenburn
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 5
    toc_float: yes
abstract: "These are the scripts and notes written for the analysis of production field flights. \n"
---
# Data Description and Quality Control
## Remove Outlier GCP Heights

  - This script reads in the GCP heights and creates a histogram of them. - "Histogram_Extracted_GCP_Height_",temp.date,".pdf"
  - Remove outlying GCP values for normalization using this histogram
  
```{r}
# clear the global environment
rm(list=ls(all=TRUE))

library(ggplot2)
library(tidyverse)
##### INPUTS TO MANIPULATE #####
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- c("2020","2021","2021","2022","2022")
Field <- c("1","1","2","1","2")
GCP_1_2020 <- c("06042020","06182020","06232020","06302020","07062020")
H_GCP_1_2020 <- c(0.08,5,5,5,10)
GCP_1_2021 <- c("06022021","06082021","06162021","06222021","06302021","07122021")
H_GCP_1_2021 <- c(0.08,5,5,5,10,10)
GCP_2_2021 <- c("06022021","06082021","06162021","06222021","06302021")
H_GCP_2_2021 <- c(0.08,5,5,5,10,10)
GCP_1_2022 <- c("06102022","06152022","06232022","06282022","07052022","07132022","07182022")
H_GCP_1_2022 <- c(5,5,5,5,10,10,10)
GCP_2_2022 <- c("06102022","06152022","06232022","06282022","07052022","07132022","07252022")
H_GCP_2_2022 <- c(5,5,5,5,10,10,10)
GCP_Lists <- list(GCP_1_2020,GCP_1_2021,GCP_2_2021,GCP_1_2022,GCP_2_2022)
GCP_H_Lists <- list(H_GCP_1_2020,H_GCP_1_2021,H_GCP_2_2021,H_GCP_1_2022,H_GCP_2_2022)
##### READ IN GCP VALUES AND CREATE HISTOGRAM #####
# for loop to go through all years
for (a in 1:length(Year)) {
  # assign the date list for the particular year and field
  temp.list <- GCP_Lists[a]
  # assign the GCP height list for the particular year and field
  temp.h.list <- GCP_H_Lists[a]
  # for loop to go through all the dates in one field and year
  for (b in 1:length(unlist(temp.list))) {
    # identify the individual date
    temp.date <- unlist(temp.list)[b]
    # read in extracted GCP data
    temp.GCP <- read.delim(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Height/Plots/PlotMeans95Perc_",temp.date,"newAgisoft_GCP_plot_xdim20_ydim20.txt"), header = F, col.names = "Extracted")
    # plot a histogram of the GCP values
    c <- ggplot(temp.GCP, aes(x = Extracted)) +
      geom_histogram(binwidth = 0.005)
    # print histogram
    pdf(paste0(Path, Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Histogram_Extracted_GCP_Height_",temp.date,".pdf"))
    print(c)
    dev.off()
  }
}
```
## Normalize extracted plant height

  - normalizes plant height using zero value, base station, or GCP heights
  - exports: 
    - "Extracted_Manual_Plots/Normalized_ExtractwGround2Manual_",date,".pdf" - the manual vs extracted
    - "Extracted_Manual_Plots/Normalized_Heatmap_WholeField_",date,".pdf" - the heatmap of normalized height
    - "Extracted_Manual_Plots/Normalized_ExtractwGround2Manual_all_dates.pdf" - manual vs extracted for all dates in field
    - "Normalized_Plant_Height_WholeField_Waseca_",Year[a],"_",Field[a],".csv" - csv of all normalized values for field
    - "Normalized_Plant_Height_WholeField_Waseca_",Year[a],"_",Field[a],".pdf" - pdf of all normalized values over time
    - "Normalized_Plant_Height_ManualPlot_Waseca_",Year[a],"_",Field[a],".csv" - csv of all normalized values for manual plots
  
```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

library(ggplot2)
library(tidyverse)
##### INPUTS TO MANIPULATE #####
  ## Letter Key ##
  # A =  with 0 and with base station (All)
  # Z = with 0 and without base station (Zero)
  # B = without 0 and with base station (Base)
  # N = without 0 and without base station (Nothing)  

Actual.Base.Station <- 180 # base station height in cm
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- c("2020","2021","2021","2022","2022")
Field <- c("1","1","2","1","2")
dim.ra <- c(67,35,35,37,37) # number of ranges for each field 
dim.ro <- c(20,32,32,31,31) # number of rows for each field
GCP_1_2020 <- c("06042020","06182020","06232020","06302020","07062020","07132020","07202020")
H_GCP_1_2020 <- c(2.5,152.4,152.4,152.4,305,305,305)
Base_1_2020 <- c("A","A","Z","Z","Z","Z","Z")
GCP_1_2021 <- c("06022021","06082021","06162021","06222021","06302021","07122021")
H_GCP_1_2021 <- c(2.5,152.4,152.4,152.4,305,305) 
Base_1_2021 <- c("A","Z","Z","Z","A","A")
GCP_2_2021 <- c("06022021","06082021","06162021","06222021","06302021")
H_GCP_2_2021 <- c(2.5,152.4,152.4,152.4,305) 
Base_2_2021 <- c("B","N","N","Z","A")
GCP_1_2022 <- c("06102022","06152022","06232022","06282022","07052022","07132022","07182022")
H_GCP_1_2022 <- c(152.4,152.4,152.4,152.4,305,305,305)
Base_1_2022 <- c("Z","Z","Z","B","Z","B","Z")
GCP_2_2022 <- c("06102022","06152022","06232022","06282022","07052022","07132022","07252022")
H_GCP_2_2022 <- c(152.4,152.4,152.4,152.4,305,305,305) 
Base_2_2022 <- c("A","A","A","Z","B","Z","Z")
GCP_Lists <- list(GCP_1_2020,GCP_1_2021,GCP_2_2021,GCP_1_2022,GCP_2_2022) # lists the lists of data dates for production 
GCP_H_Lists <- list(H_GCP_1_2020,H_GCP_1_2021,H_GCP_2_2021,H_GCP_1_2022,H_GCP_2_2022) # lists the lists of heights of the GCPs
Base_Lists <- list(Base_1_2020,Base_1_2021,Base_2_2021,Base_1_2022,Base_2_2022) # lists the lists informing whether base station is visible in the DEM
################################
# for loop to go through all fields
for (a in 1:length(Field)) {
  # assign the date list for the particular year and field
  temp.list <- GCP_Lists[a]
  # assign the GCP height list for the particular year and field
  temp.h.list <- GCP_H_Lists[a]
  # assign the base station list for the particular year and field
  temp.b.list <- Base_Lists[a]
  # initiate file for all normalized values
  norm.whole.field <- as.data.frame(matrix(nrow = (dim.ra[a]*dim.ro[a]), ncol = 1))
  # initiate file for all manual values
  norm.manual.all <- as.data.frame(matrix(nrow = 0, ncol = 4), col.names = c("Plot","Normalized","average","date"))
  # initiate file for all manual plot values (not just ones with hand measurements)
  norm.manualplots.all <- as.data.frame(matrix(nrow = 0, ncol = 4), col.names = c("Plot","Normalized","average","date"))
  # for loop to go through all dates from particular year and field 
  for (b in 1:length(unlist(temp.list))) {
    # identify the individual date
    temp.date <- unlist(temp.list)[b]
    # Bring down the Actual GCP Heights for the flight day
    temp.h.gcp <- unlist(temp.h.list)[b]
    ##### READ IN NECESSARY EXTRACTED DATA #####
    # GCP height data
    temp.GCP <- read.delim(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Height/Plots/PlotMeans95Perc_",temp.date,"newAgisoft_GCP_plot_xdim20_ydim20.txt"), header = F, col.names = "Extracted")
    # Whole field height data
    temp.whole <- read.delim(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Height/Plots/PlotMeans95Perc_",temp.date,"newAgisoft_WholeField_plot_xdim20_ydim20.txt"), header = F, col.names = "Extracted")
    # Manual height data
    temp.manual <- read.delim(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Height/Plots/PlotMeans95Perc_",temp.date,"newAgisoft_Manual_plot_xdim20_ydim20.txt"), header = F, col.names = "Extracted")
    # Zero height data
    temp.zero <- read.delim(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Height/Plots/PlotMeans95Perc_",temp.date,"newAgisoft_Zero_plot_xdim20_ydim20.txt"), header = F, col.names = "Extracted")
    # Base station height data
    temp.base <- read.delim(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Height/Plots/PlotMeans95Perc_",temp.date,"newAgisoft_BaseStation_plot_xdim20_ydim20.txt"), header = F, col.names = "Extracted")
    ##### COMBINE EXTRACTED HEIGHTS #####
    # Make secondary file for GCP add column with data type
    GCP <- as.data.frame(temp.GCP)
    GCP$Data.Type <- "GCP"
    # Make secondary file for Whole field add column with data type
    Whole <- as.data.frame(temp.whole)
    Whole$Data.Type <- "Whole"
    # Make a secondary file for Manual add column with data type
    Manual <- as.data.frame(temp.manual)
    Manual$Data.Type <- "Manual"
    # Make a secondary file for Zero add column with data type
    Zero <- as.data.frame(temp.zero)
    Zero$Data.Type <- "Zero"
    # Make a secondary file for Base station add column with data type
    Base <- as.data.frame(temp.base)
    Base$Data.Type <- "Base"
    # combine all extracted heights into one data.frame
    all.values <- rbind(GCP,Whole,Manual,Zero,Base)
    ##### NORMALIZE THE EXTRACTED HEIGHTS #####
    # isolate value of the zero and base station
    zero.value <- as.numeric(temp.zero[1,1])
    base.value <- as.numeric(temp.base[1,1] - zero.value)
    # create the shifted to 0 values
    all.values$Shifted <- all.values$Extracted - zero.value # all.values$Shifted will be ignored if 0 isn't used
    # identify whether or not the base station is visible in the DEM
    temp.base.i <- unlist(temp.b.list)[b]    
    ### If statement - use 0 and base station for normalization? ###
    if (temp.base.i == "A") { # with 0 and with base station (All)
      # create the 0 to 1 normalization (zeroed) using the base station
       all.values$Zeroed <- all.values$Shifted/base.value
      
      #take the actual base station height divided by the zeroed base station height - save as Base.mean
      Base.mean <- Actual.Base.Station/all.values[nrow(all.values),4]
      #take all.values$Zeroed heights times Base.mean - save as all.values$Normalized
      all.values$Normalized <- all.values$Zeroed * Base.mean
    } else if (temp.base.i == "Z") { # with 0 and without base station (Zero)
      # calculate the values for one sd from the mean 
      GCP.descript <- all.values %>%
        filter(Data.Type == "GCP") %>%
        summarise(mean.gcp = mean(Shifted, na.rm = T),
                  sd.gcp = sd(Shifted, na.rm = T),
                  rem.high = mean.gcp + sd.gcp,
                  rem.low = mean.gcp - sd.gcp)
      # remove values higher or lower than 1 sd from the mean
      all.values$Shifted <-  ifelse((all.values$Data.Type == "GCP" & all.values$Shifted < as.numeric(GCP.descript$rem.low)), NA, all.values$Shifted)
      # filter all.values to Data.Type == GCP - save as temp.all
      temp.all <- all.values %>% 
        filter(Data.Type == "GCP")
      # calculate the average of temp.all$Shifted - save as GCP.mean
      GCP.mean <- mean(temp.all$Shifted, na.rm = T)
      # create the 0 to 1 normalization (zeroed) using the GCPs
      all.values$Zeroed <- all.values$Shifted/GCP.mean
      # take all.values$Zeroed times the real world GCP height
      all.values$Normalized <- all.values$Zeroed * temp.h.gcp
    } else if (temp.base.i == "B") { # without zero and with base station (Base)
      # create zeroed w/ base station but skip shifted
      all.values$Zeroed <- all.values$Extracted * (1/as.numeric(temp.base[1,1])) 
      # create normalized using zeroed and actual base station
      all.values$Normalized <- all.values$Zeroed * Actual.Base.Station
    } else if (temp.base.i == "N") { # without zero and without base station (Zero)
      # calculate the values for one sd from the mean 
      GCP.descript <- all.values %>%
        filter(Data.Type == "GCP") %>%
        summarise(mean.gcp = mean(Shifted, na.rm = T),
                  sd.gcp = sd(Shifted, na.rm = T),
                  rem.high = mean.gcp + sd.gcp,
                  rem.low = mean.gcp - sd.gcp)
      # remove values higher or lower than 1 sd from the mean
      all.values$Shifted <-  ifelse((all.values$Data.Type == "GCP" & all.values$Shifted < as.numeric(GCP.descript$rem.low)), NA, all.values$Shifted)
      # filter to just the GCP values
      temp.all <- all.values %>%
        filter(Data.Type == "GCP")
      # calculate the GCP quantities necessary for normalization
      temp.all <- temp.all %>%
        mutate(Normalized = temp.h.gcp/temp.all$Extracted)
      # save the aver GCP value necessary for normalization
      GCP.mean <- mean(temp.all$Normalized, na.rm = T)
      # calculate the normalized height values
      all.values$Normalized <- all.values$Extracted * GCP.mean
    }
    
    # filter all values to only manual values
    normalized.height <- all.values %>%
      filter(Data.Type == "Manual")
    # read in plot order file
    temp.order <- read.delim(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Plot_Order/Plots_Manual.txt"))
    # merge normalized height with temp order
    normalized.height <- cbind(normalized.height,temp.order)
    # make a subset of the file to print the normalized manaul values
    temp.print <- normalized.height[,c("Plot","Normalized")]
    # add a date column
    temp.print$date <- temp.date
    # add to normalized manual all dates (not just dates with hand measurements)
    norm.manualplots.all <- rbind(norm.manualplots.all,temp.print)
   ### Compare manual and extracted values ###
   # skip the first date in 2021 (no hand height data for that day)
   if (temp.date != "06022021") {
     # read in the hand height data
     temp.hand <- read.csv(paste0(Path,Year[a],"/Waseca/manual_measurements/",temp.date,"_manual.csv"))
     # calculate average manyal height measurements
     temp.hand$average <- rowMeans(temp.hand[,c(2:6)])
     # merge normalized.height and temp.hand based on Plot
     temp.graph <- merge(normalized.height[,c("Plot","Normalized")],temp.hand[,c("Plot","average")], by = "Plot")
     # graph extracted to manual
     temp.plot <- temp.graph %>%
       ggplot(aes(x = Normalized, y = average)) +
       geom_point() +
       theme_classic() +
       # geom_text(aes(label =Plot)) +
       geom_smooth(method = "lm", se = F) +
       xlab(paste0("Normalized Extracted Plant Height Values (cm) ",temp.date)) +
       ylab(paste0("Average Manual Plant Height Measurements (cm) ", temp.date)) +
       labs(title = paste("Adj R2 = ",summary(lm(average~Normalized, data = temp.graph))$r.squared," RMSE = ",summary(sqrt(mean((temp.graph$Normalized - temp.graph$average)^2))))) +
       theme(
         text = element_text(size = 7)
       )

     # export the plot 
     pdf(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Extracted_Manual_Plots/Normalized_ExtractwGround2Manual_",temp.date,".pdf"), height = 3, width = 4)
        print(temp.plot)
      dev.off()
      # add a column to temp.graph that contains the date
      temp.graph$date <- temp.date
      # add temp.graph to norm.manual.all
      norm.manual.all <- rbind(norm.manual.all,temp.graph)
   }
   ##### PLOT THE NORMALIZED WHOLE FIELD HEATMAP #####
   # assign variables for whole field dimensions based on year
    num.ra <- dim.ra[a]
    num.ro <- dim.ro[a]
    # initiate summary statistics file
    summary.stats <- data.frame()
    # normalized extracted plant height
    hts <- all.values %>%
      filter(Data.Type == "Whole") %>%
      dplyr::select(Normalized)
    # assign the row numbers
    hts$Row <- rep(1:num.ro, times = num.ra)
    # assign the range numbers
    range.num <- integer()
    for (k in c(num.ra:1)) {
      range.temp <- rep(k, times = num.ro)
      range.num <- append(range.num, range.temp)
    }
    hts$Range <- range.num
    # PLOT A HEATMAP OF THE FIELD and print
    pdf(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Extracted_Manual_Plots/Normalized_Heatmap_WholeField_",temp.date,".pdf"))
    temp.heat <- ggplot(hts, aes(x = Range, y = Row, fill = Normalized)) +
      geom_tile()
    print(temp.heat)
    dev.off()
    ##### ADD DATE TO A WHOLE FILE FOR FIELD OVER TIME
    # isolate just the whole field data
    temp.all.whole <- all.values %>%
      filter(Data.Type == "Whole") %>%
      dplyr::select(Normalized)
    # add the data to norm.whole.field 
    norm.whole.field <- cbind(norm.whole.field,temp.all.whole)
  }
  # pivot the plots wider
  norm.manualplots.all <- norm.manualplots.all %>%
    pivot_wider(names_from = "date", values_from = "Normalized")
  # export the normalized height measurements for just the manual plots
  write.csv(norm.manualplots.all, paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Normalized_Plant_Height_ManualPlot_Waseca_",Year[a],"_",Field[a],".csv"))
  ##### PLOT THE MANUAL VS EXTRACTED VALUES FOR ALL DAYS #####
  # plot manual vs extracted values
  temp.plot <- ggplot(norm.manual.all, aes(x = Normalized, y = average)) +
    geom_point(aes(color = date)) +
    geom_smooth(method = "lm", se = F) +
    theme_light() + 
    xlab("Extracted plant height (cm)") +
    ylab("Manual plant height (cm)") +
    labs(title = paste("Adj R2 = ",round(summary(lm(average~Normalized, data = norm.manual.all))$r.squared,2))) +
    theme(
      legend.position = "none"
    )
  # export the plot 
  pdf(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Extracted_Manual_Plots/Normalized_ExtractwGround2Manual_all_dates_",Field[a],"_",Year[a],".pdf"), height = 2.5, width = 2.5)
  print(temp.plot)
  dev.off()
  # fix norm.whole.field file
  norm.whole.field[,1] <- NULL
  colnames(norm.whole.field) <- unlist(temp.list)
  norm.whole.field$Range <- hts$Range
  norm.whole.field$Row <- hts$Row
  # EXPORT NORMALIZED file
  write.csv(norm.whole.field, paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Normalized_Plant_Height_WholeField_Waseca_",Year[a],"_",Field[a],".csv"))
  ##### PLOT THE GROWTH RATE OVER TIME #####
  # remove range and row
  norm.whole.field$Range <- NULL
  norm.whole.field$Row <- NULL
  # add plot number column to dataframe
  norm.whole.field$Plot <- c(1:nrow(norm.whole.field))
  # pivot longer
  norm.whole.field.long <- norm.whole.field %>%
    pivot_longer(cols = 1:(ncol(norm.whole.field)-1), names_to = "Date",values_to = "Height")
  # plot the data
  pdf(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Normalized_Plant_Height_WholeField_Waseca_",Year[a],"_",Field[a],".pdf"))
  temp.plot <- ggplot(norm.whole.field.long, aes(x = Date, y = Height, group = Plot)) +
    geom_line()
  print(temp.plot)
  dev.off()
  }

```
## Change Dates to GDD

  - Changes dates from Gregorian to Growing Degree Days
  - need planting dates for Production Fields:
    Production_2020: May 2, 2020
    Production_2021: April 30, 2021
    Production_2022: May 7, 2022
 
```{r}
library(tidyverse)
library(stringr)
library(data.table)
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- c("2020","2021","2021","2022","2022")
Field <- c("1","1","2","1","2")
plant.date <- c("05022020","04302021","04302021","05072022","05072022")

all.weather.GDD <- as.data.frame(matrix(nrow = 0, ncol = 5))
# for loop to go through each field
for (a in 1:length(Field)) {
  # read in normalized whole field plant height data
  norm.date <- read.csv(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Normalized_Plant_Height_WholeField_Waseca_",Year[a],"_",Field[a],".csv"))
  # rename first column to Square
  norm.date <- norm.date %>%
    rename("Square" = "X")
  
  ##### CHANGE DATES TO GROWING DEGREE DAYS #####
  temp.date <- norm.date %>%
    pivot_longer(cols = starts_with("X"), names_to = "Date", values_to = "Heights") %>%
    separate(col = "Date", into = c("Letters","Date"), sep = "X", remove = T)
  temp.date$Letters <- NULL
  
  # change date format in file
  temp.date$Date <- strptime(as.character(temp.date$Date), "%m%d%Y")
  temp.date$Date <- format(temp.date$Date, "%m%d%Y")
  
  # read in weather temperature data
  weather <- read.csv(paste0(Path,Year[a],"/Waseca/Temperature_Data_Waseca_",Year[a],".csv"))
  weather$Date <- strptime(as.character(weather$Date), "%m/%d/%y") #tell R that Date is actually a Date
  weather$Date <- format(weather$Date, "%m%d%Y") #change the date format to match the format in the other file

  # calculate the GDD for each day of the growing season
  weather$GDD <- 0 # initiate GDD column for use
  # for loop to calculate the GDD
  for (b in 1:nrow(weather)) {
    if (((weather[b,2] + weather[b,3])/2) > 50) {
      temp.max <- fifelse(weather[b,2] > 86, 86, weather[b,2])
      temp.min <- fifelse(weather[b,3] < 50, 50, weather[b,3])
      weather[b,4] = (((temp.max + temp.min)/2) - 50)
    } else {
      weather[b,4] = 0
    }
  }
  
  # calculate cumulative GDD after the planting date
  weather.dap <- weather[!(weather$Date < plant.date[a]), ] # remove all the days prior to planting
  weather.dap$cum.GDD <- weather.dap[2,4] # cumulative starts the day after planting
  
  # for loop to calculate cumulative GDD
  for (c in 3:nrow(weather.dap)) {
    weather.dap[c,5] <- (weather.dap[(c -1),5] + weather.dap[c,4])
  }
  
  # add removed days back on
  weather$cum.GDD <- 0
  weather.temp <- weather[!(weather$Date >= plant.date[a]), ]
  weather.tot <- rbind(weather.temp,weather.dap)
  
  # print weather.tot for later use
  write.csv(weather.tot, paste0(Path,Year[a],"/Waseca/GDD_Accumulation_",Year[a],".csv"))
  
  # append weather tot
  all.weather.GDD <- rbind(all.weather.GDD,weather.tot)
  
  # CHANGE THE FLIGHT DAY TO GDD
  temp.date.GDD <- data.frame()
  temp.date.GDD <- merge(temp.date, weather.tot[,c("Date", "cum.GDD")], by = "Date", all = F)
  full.GDD <- temp.date.GDD
  ######################### CAN LOOK AT DATE TO CUM.GDD IN temp.date.GDD ##############################
  full.GDD$Date <- NULL # get rid of date before changing back to wide format
  
  # print a growth rate with GDD
  pdf(paste0(Path, Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Full_field_growth_rate_GDD_Production_",Field[a],"_",Year[a],".pdf"), width = 10, height = 5)
  
  d <- ggplot(full.GDD, aes(x = cum.GDD, y = Heights, group = Square)) +
    geom_line() +
    geom_point() +
    theme_light() +
    xlab("Cumulative Growing Degree Days (GDD)") +
    ylab("Height (ft)")
  print(d)
  dev.off()

  # print the full GDD file for use in later scripts
  write.csv(full.GDD, paste0(Path, Year[a], "/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Full_Height_w_GDD_file_",Field[a],"_",Year[a],".csv"))
}

# export all weather data
write.csv(all.weather.GDD,paste0(Path,"/All_Production/GDD_Accumulation_AllYears.csv"))

```

** From this point on moving on with just field 1 for 2020, 2021, and 2022 - excluding field 2 for 2021 and 2022 **

## Identify Yield for each Height Extracted box

  - inputs: 
    - "Full_Height_w_GDD_file_",Field,"_",Year,".csv"
    - "Production_yield_",year,".csv"
    - "Plots_Matlab_Roi_",year,".csv"
  - finds 9 nearest yield data points to each plant height plot 
  - calculates weighted mean of 9 yield points based on distance
  - exports: "Full_Height_GDD_w_Yield_",Field,"_",Year,".csv"

```{r}
# load library
library(tidyverse)
library(sp)
library(rgeos)
## INPUTS ##
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- c("2020","2021","2022")
Field <- c("1","1","1")
###########
# for loop to go through each field
for (a in 1:length(Field)) {
  
  ## READ IN FILES AND MANIPULATE THEM FOR USE ##
  
  # read in normalized height file and remove unnecessary columns
  heights <- read.csv(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Full_Height_w_GDD_file_",Field[a],"_",Year[a],".csv"))
  heights$X <- NULL # remove unnecessary columns
  # pivot wider
  heights.1 <- heights %>%
    pivot_wider(names_from = cum.GDD, values_from = Heights)
  # read in Roi file from matlab and remove unnecessry columns
  roi <- read.csv(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Plot_Order/Plots_Matlab_Roi_",Year[a],".csv"), col.names = c("Square","BoundingBox","X","Y","id","left","top","right","bottom"))
  roi[,c(2:5)] <- NULL # remove unnecessary columns
  # calculate average latitude and longitude for each plot bounding box
  roi$Latitude <- rowMeans(roi[,c("top","bottom")])
  roi$Longitude <- rowMeans(roi[,c("left","right")])
  roi <- roi[,c("Square","Latitude","Longitude")]
  # add column with data type
  roi$data.type <- "ROI"
  # transform coordinate data into spatial objects
  coordinates(roi) <- ~Latitude+Longitude
  # read in yield data and remove unnecessary columns
  raw.yield <- read.csv(paste0(Path,Year[a],"/Waseca/Yield_data/Production_yield_",Year[a],".csv"))
  raw.yield <- raw.yield[,c(1,2,21,23)]# remove unnecessary columns
  # add column with data.type
  raw.yield$data.type <- "YIELD"
  raw.yield$Square <- c(1:nrow(raw.yield))
  # transform coordinate data into spatial objects
  coordinates(raw.yield) <- ~Latitude+Longitude
  
  ## ISOLATE YIELD POINTS ASSOCIATED WITH EACH PLANT HEIGHT AREA ##
  
  # calculate pairwise distance between points
  distance <- gDistance(spgeom1 = raw.yield, spgeom2 = roi, byid = T,)
  # find yield point shortest distance away
  min.distance <- apply(distance, 1, function(x) order(x, decreasing=F)[1])
  # initiate new.data with the closest yield points
  new.data <- cbind(roi, raw.yield[min.distance,], apply(distance, 1, function(x) sort(x, decreasing=F)[1]))
  # for loop to find 2-9 shortest distances and add them to new.data
  for (b in 2:9){
    # find b shortest away yield point
    min.distance <- apply(distance, 1, function(x) order(x, decreasing=F)[b])
    # add to new.data
    new.data <- cbind(new.data, raw.yield[min.distance,], apply(distance, 1, function(x) sort(x, decreasing=F)[b]))
  }
  # change new.data to a data frame for future use
  new.data.1 <- as.data.frame(new.data)
  # rename columns
  colnames(new.data.1) <- c("Square","data.type","Yld.lb.ac.1","Yld.bu.ac.1","data.type.1","Square.1","Distance.1","Yld.lb.ac.2","Yld.bu.ac.2","data.type.2","Square.2","Distance.2","Yld.lb.ac.3","Yld.bu.ac.3","data.type.3","Square.3","Distance.3","Yld.lb.ac.4","Yld.bu.ac.4","data.type.4","Square.4","Distance.4","Yld.lb.ac.5","Yld.bu.ac.5","data.type.5","Square.5","Distance.5","Yld.lb.ac.6","Yld.bu.ac.6","data.type.6","Square.6","Distance.6","Yld.lb.ac.7","Yld.bu.ac.7","data.type.7","Square.7","Distance.7","Yld.lb.ac.8","Yld.bu.ac.8","data.type.8","Square.8","Distance.8","Yld.lb.ac.9","Yld.bu.ac.9","data.type.9", "Square.9","Distance.9","Latitude","Longitude")
  # remove unnecessary columns
  new.data.1[,c("data.type.1","Square.1","data.type.2","Square.2","data.type.3","Square.3","data.type.4","Square.4","data.type.5","Square.5","data.type.6","Square.6","data.type.7","Square.7","data.type.8","Square.8","data.type.9","Square.9")] <- NULL
  # reorder columns
  new.data.1 <- new.data.1[,c(1,2,30,31,3,6,9,12,15,18,21,24,27,4,7,10,13,16,19,22,25,28,5,8,11,14,17,20,23,26,29)]
  
  ## CALCULATE WEIGHTED YIELD BASED ON DISTANCES ##
  
  # weighted yield for Yld.lb.ac
  new.data.1$Mean.Yld.lb.ac <- apply(new.data.1, 1, function(x) weighted.mean(as.numeric(x[5:13]), as.numeric(x[23:31])))
  # weighted yield for Yld.bu.ac
  new.data.1$Mean.Yld.bu.ac <- apply(new.data.1, 1, function(x) weighted.mean(as.numeric(x[14:22]), as.numeric(x[23:31])))
  # remove unnecessary columns
  new.data.2 <- new.data.1[,c(1,2,3,4,32,33)]
  
  ## COMBINE YIELD INFORMATION WITH HEIGHT INFORMATION ##
  
  # merge heights.1 and new.data.2
  height.yield <- merge(heights.1, new.data.2, by = "Square")
  # remove unnecessary columns
  height.yield[,c("data.type","Latitude","Longitude")] <- NULL
  # export plant height and yield data combined (height.yield)
  write.csv(height.yield,paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Full_Height_GDD_w_Yield_",Field[a],"_",Year[a],".csv"))
  
}
```

## Identify yield and NDVI for each plot

  - bring in extracted plot heights
    - reads in "Normalized_Plant_Height_ManualPlot_Waseca_",Year[a],"_",Field[a],".csv"
    - reads in "Manual_Matlab_Roi_",Year[a],".csv"
  - estimate the yield for each plot as above
    - reads in "/Waseca/Yield_data/Production_yield_",Year[a],".csv"
  - bring in NDVI measurements for plots
    - "Raw_NDVI_data.txt"
    - "manual_measurements/",date.list[c],"_ndvi.csv"
  - exports: file with manual plot extracted normalized data, yield, ndvi
    - "Full_Height_GDD_w_Yield_ManualPlots_",Field[a],"_",Year[a],".csv"
    - "Full_Height_GDD_w_Yield_NDVI_ManualPlots_",Field[a],"_",Year[a],".csv"
  
```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))
# load library
library(tidyverse)
library(sp)
library(rgeos)
## INPUTS ##
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- c("2020","2021","2022")
Field <- c("1","1","1")
###########
# for loop to go through each field
for (a in 1:length(Field)) {
  
  ## READ IN FILES AND MANIPULATE THEM FOR USE ##
  
  # read in normalized height file and remove unnecessary columns
  heights <- read.csv(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Normalized_Plant_Height_ManualPlot_Waseca_",Year[a],"_",Field[a],".csv"))
  heights$X <- NULL # remove unnecessary columns
  # read in the GDD accumulation file
  gdd.acc <- read.csv(paste0(Path,Year[a],"/Waseca/GDD_Accumulation_",Year[a],".csv"))
  # format the date correctly
  gdd.acc$Date <- paste0("0",gdd.acc$Date)
  # pivot longer and rename with cumulative GDD instead of date
  heights.temp <- heights %>%
    pivot_longer(cols = 2:ncol(heights), names_to = "Date", values_to = "Heights") %>%
    separate(Date, into = c(NA, "Date"), sep = "X", remove = T)
  # merge heights.temp and gdd.acc based on Date
  heights.temp <- merge(heights.temp, gdd.acc[,c("Date", "cum.GDD")], by = "Date")
  # remove Date column
  heights.temp$Date <- NULL
  # pivot wider
  heights.1 <- heights.temp %>%
    pivot_wider(names_from = cum.GDD, values_from = Heights)
  # read in Roi file from matlab and remove unnecessry columns
  roi <- read.csv(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Plot_Order/Manual_Matlab_Roi_",Year[a],".csv"), col.names = c("Plot","Geometry","BoundingBox","X","Y","id","left","top","right","bottom"))
  # remove unnecessary columns
  roi[,c(2,5)] <- NULL
  # # separate BoundingBox into left top right and bottom for use
  # roi <- roi %>%
  #   separate(BoundingBox, into =c("left","top","right","bottom"), sep = c(",|;")) 
  # # remove the brackets
  # roi$left <- str_replace_all(roi$left,"\\[","")
  # roi$bottom <- str_replace_all(roi$bottom,"\\]","")
  # # change columns to numeric
  # roi <- roi %>%
  #   mutate_at(c("left","top","right","bottom"), as.numeric)
  # calculate average latitude and longitude for each plot bounding box
  roi$Latitude <- rowMeans(roi[,c("top","bottom")])
  roi$Longitude <- rowMeans(roi[,c("left","right")])
  roi <- roi[,c("Plot","Latitude","Longitude")]
  # add column with data type
  roi$data.type <- "ROI"  
  # transform coordinate data into spatial objects
  coordinates(roi) <- ~Latitude+Longitude  
  # read in yield data and remove unnecessary columns
  raw.yield <- read.csv(paste0(Path,Year[a],"/Waseca/Yield_data/Production_yield_",Year[a],".csv"))
  raw.yield <- raw.yield[,c(1,2,21,23)]# remove unnecessary columns  
  # add column with data.type
  raw.yield$data.type <- "YIELD"
  raw.yield$Plot <- c(1:nrow(raw.yield))  
  # transform coordinate data into spatial objects
  coordinates(raw.yield) <- ~Latitude+Longitude
  
  ## ISOLATE YIELD POINTS ASSOCIATED WITH EACH PLANT HEIGHT AREA ##
  
  # calculate pairwise distance between points
  distance <- gDistance(spgeom1 = raw.yield, spgeom2 = roi, byid = T,)  
  # find yield point shortest distance away
  min.distance <- apply(distance, 1, function(x) order(x, decreasing=F)[1])
  # initiate new.data with the closest yield points
  new.data <- cbind(roi, raw.yield[min.distance,], apply(distance, 1, function(x) sort(x, decreasing=F)[1]))
  # for loop to find 2-9 shortest distances and add them to new.data
  for (b in 2:9){
    # find b shortest away yield point
    min.distance <- apply(distance, 1, function(x) order(x, decreasing=F)[b])
    # add to new.data
    new.data <- cbind(new.data, raw.yield[min.distance,], apply(distance, 1, function(x) sort(x, decreasing=F)[b]))
  }
  # change new.data to a data frame for future use
  new.data.1 <- as.data.frame(new.data)
  # rename columns
  colnames(new.data.1) <- c("Plot","data.type","Yld.lb.ac.1","Yld.bu.ac.1","data.type.1","Square.1","Distance.1","Yld.lb.ac.2","Yld.bu.ac.2","data.type.2","Square.2","Distance.2","Yld.lb.ac.3","Yld.bu.ac.3","data.type.3","Square.3","Distance.3","Yld.lb.ac.4","Yld.bu.ac.4","data.type.4","Square.4","Distance.4","Yld.lb.ac.5","Yld.bu.ac.5","data.type.5","Square.5","Distance.5","Yld.lb.ac.6","Yld.bu.ac.6","data.type.6","Square.6","Distance.6","Yld.lb.ac.7","Yld.bu.ac.7","data.type.7","Square.7","Distance.7","Yld.lb.ac.8","Yld.bu.ac.8","data.type.8","Square.8","Distance.8","Yld.lb.ac.9","Yld.bu.ac.9","data.type.9", "Square.9","Distance.9","Latitude","Longitude")
  # remove unnecessary columns
  new.data.1[,c("data.type.1","Square.1","data.type.2","Square.2","data.type.3","Square.3","data.type.4","Square.4","data.type.5","Square.5","data.type.6","Square.6","data.type.7","Square.7","data.type.8","Square.8","data.type.9","Square.9")] <- NULL
  # reorder columns
  new.data.1 <- new.data.1[,c(1,2,30,31,3,6,9,12,15,18,21,24,27,4,7,10,13,16,19,22,25,28,5,8,11,14,17,20,23,26,29)]

  ## CALCULATE WEIGHTED YIELD BASED ON DISTANCES ##
  
  # weighted yield for Yld.lb.ac
  new.data.1$Mean.Yld.lb.ac <- apply(new.data.1, 1, function(x) weighted.mean(as.numeric(x[5:13]), as.numeric(x[23:31])))
  # weighted yield for Yld.bu.ac
  new.data.1$Mean.Yld.bu.ac <- apply(new.data.1, 1, function(x) weighted.mean(as.numeric(x[14:22]), as.numeric(x[23:31])))
  # remove unnecessary columns
  new.data.2 <- new.data.1[,c(1,2,3,4,32,33)]  

  ## COMBINE YIELD INFORMATION WITH HEIGHT INFORMATION ##
  
  # merge heights.1 and new.data.2
  height.yield <- merge(heights.1, new.data.2, by = "Plot") 
  # remove unnecessary columns
  height.yield[,c("data.type","Latitude","Longitude")] <- NULL
  # export plant height and yield data combined (height.yield)
  write.csv(height.yield,paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Full_Height_GDD_w_Yield_ManualPlots_",Field[a],"_",Year[a],".csv"))
  
  ## READ IN NDVI INFORMATION AND COMBINE IT WITH HEIGHT AND YIELD INFORMATION ##
  
  # read in NDVI raw data
  ndvi.file <- read.delim(paste0(Path,"/All_Production/Raw_NDVI_data.txt"))
  # remove unnecessary columns
  ndvi.file[,c(2,3,7)] <- NULL
  # if statement to determine if there is NDVI for the year
  if (Year[a] == "2021") {
    # assign the date list based on the year
    date.list <- c("06162021","06222021","06302021")
  } else if (Year[a] == "2022") {
    # assign the date list based on the year
    date.list <- c("06102022","06152022","06232022","06282022")
  }
  # if state to include the NDVI
  if (Year[a] != "2020") {
    # for loop to go through the dates
    for (c in 1:length(date.list)) {
      # assign date
      for.loop.date <- date.list[c]
      # read in the manual measurement .csv file
      num.file <- read.csv(paste0(Path,Year[a],"/Waseca/manual_measurements/",date.list[c],"_ndvi.csv"))
      # expand num.file$NDVI.Num to contain all values individually
      num.file$NDVI.Num <- gsub(pattern = "-", replacement = ":", x = num.file$NDVI.Num)
      # for loop to go through each row
      for (d in 1:nrow(num.file)) {
        # save the value for the last column to save 
        end.col <- length(eval(parse(text = num.file[d,2]))) + 2
        # add columns with index values
        num.file[d,c(3:end.col)] <- eval(parse(text = num.file[d,2]))
      }
      # remove NDVI.Num
      num.file$NDVI.Num <- NULL
      # make V3 an integer
      num.file$V3 <- as.numeric(num.file$V3)
      # pivot longer & na omit
      num.file.1 <- num.file %>%
        pivot_longer(col = 2:ncol(num.file),names_to = "reps", values_to = "index") %>%
        dplyr::select(-reps) %>%
        na.omit()
      # merge num.file.1 and ndvi.file
      num.ndvi.file <- merge(num.file.1, ndvi.file, by = "index")
      # remove 0 values in NDVI
      num.ndvi.file <- num.ndvi.file %>%
        filter(NDVI != 0)
      # change date to Gdd
      for.loop.gdd <- gdd.acc %>%
        filter(Date == for.loop.date) %>%
        pull(cum.GDD)
      # average 760nm 635nm and NDVI values for each plot
      avg.ndvi.file <- num.ndvi.file %>%
        group_by(Plot) %>%
        summarise(mean760nm = mean(X760..nm.),
                  mean635nm = mean(X635..nm.),
                  meanNDVI = mean(NDVI))
      # add date to each column for 760nm 635nm and NDVI
      avg.ndvi.file <- avg.ndvi.file %>%
        mutate("X{for.loop.gdd}_760nm" := mean760nm,
               "X{for.loop.gdd}_635nm" := mean635nm,
               "X{for.loop.gdd}_NDVI" := meanNDVI)
      # add to height.yield
      height.yield <- merge(height.yield, avg.ndvi.file[,c(1,5:7)], by = "Plot")
    }
  }
  # export plant height, yield, and ndvi data combined (height.yield)
  write.csv(height.yield,paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Full_Height_GDD_w_Yield_NDVI_ManualPlots_",Field[a],"_",Year[a],".csv"))
}
```

## Quality Control and Data Point Removal

  - reads in "Full_Height_GDD_w_Yield_",Field,"_",Year,".csv"
  - removes whole flight days if their average is below 0
  - removes individual plots based on dips and peaks  
  - exports "Final_file_b4_ANOVA_", Year[a], ".csv"
  - exports "Normalized_Cleaned_Plant_Height_WholeField_",Year[a],".pdf"
  - exports Final_file_b4_ANOVA_w_Yield_", Year[a], ".csv"

```{r}
# clear the global environment
rm(list=ls(all=TRUE))

# load library
library(tidyverse)
## INPUTS ##
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- c("2020","2021","2022")
Field <- c("1","1","1")
###########
# for loop to go through all of the fields
for (a in 1:length(Field)) {
  ## READ IN FILES ##
  
  data <- read.csv(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Full_Height_GDD_w_Yield_",Field[a],"_",Year[a],".csv"), row.names =1)
  
  ## REMOVE WHOLE FLIGHT DAYS BASED ON ISSUES ##
  
  # separate height data from other data
  height.data <- data %>%
    dplyr::select("Square",starts_with("X"))
  descript.data <- data %>%
    dplyr::select(!starts_with("X"))
  # add Plot to Square
  descript.data <- descript.data %>%
    mutate(Square = paste0("Plot",Square))
  # calculate the mean value for all columns in data
  mean.col <- colMeans(height.data)
  # initiate bad flights list
  bad.flights <- list()
  # remove columns based on the average
  for (b in  1:length(mean.col)) {
    if (mean.col[b] < 0) {
      # add b to a list
      bad.flights <- append(bad.flights,b)
    } 
  }
  # remove all values of in list of bad b from days
  if (length(bad.flights) > 0) {
    height.data <- height.data[,-c(unlist(bad.flights))]
  }
  
  ## REMOVE INDIVIDUAL PLOTS ON SPECIFIC DAYS ##

  # add plot to first column
  height.data$Square <- paste0("Plot",height.data$Square)
  # Function to filter peaks and valleys
  big_data <- data.frame() #make empty data frame
  filter_peaks <- function(df) { #Input df into function
    for (i in seq_len(nrow(df))) { #For each row in data frame
      row <- df[i,] #Subset row 
      names <- colnames(row) # Make list of column names
      # Pivot row into long
      row <- pivot_longer(row, cols = names[2:length(names)], names_to = "flight", values_to = "height")
      # print(row)
      left <- 1 # Set left end of sliding evaluation window
      right <- 3 # Set right end of sliding evaluation window
    # Check for dips
      row[1,4] <- as.character(NA) # Nothing to compare first flight to, automatically label good
      for (j in 2:(length(names)-2)) { #For each column in row starting at 3
        if (is.na(row[j,3])) { # adding an if statement for dealing with NA values for height
          next()
        } else {
          # Set sliding evaluation window
          left <- j - 1
          right <- j + 1
          # Evaluate for a dip
          if(row[j,3] < (row[left,3] * 0.80))
            if(row[right,3] > row[j,3])
              if(row[left,3] < row[right,3])
              {row[j,4] <- "dip"}
          # Evaluate for a peak
          if(row[j,3] > (row[right,3] * 1.20))
            if (row[right,3] > row[left,3])
              if (row[left,3] < row[j,3])
              {row[j,4] <- "peak"}
        }
      }
      # Rules for checking if last data point is valley, can't check for peaks b/c nothing to compare to on right
      if (is.na(row[(length(names)-1),3 ])) {
        row[(length(names)-1),4] <- as.character(NA)
      } else if ((row[(length(names)-1),3]/row[(length(names)-2),3] < 0.80)){
        row[(length(names)-1),4] <- "dip"
      #((row[(length(names)-1), 3]/row[(length(names)-2), 3]) < .90) {
      #  row[(length(names)-1),4] <- "dip"
      } else {
        row[(length(names)-1),4] <- NA
      }
      big_data <- rbind(big_data, row) # Bind each row back to the empty data frame created outside the function
      }
      dipdrop <<- big_data # Export data frame with peaks and valleys labeled outside of function
  }  
  # Run Function
  filter_peaks(height.data)
  # Make new column copying height
  dipdrop$heightNA <- dipdrop$height
  # Replace data points with NA if tagged as peak or valley
  dipdrop$heightNA <- ifelse(is.na(dipdrop$...4), dipdrop$height, ifelse(dipdrop$...4 == "dip",  NA, ifelse(dipdrop$...4 == "peak", NA, dipdrop$height)))
  # Remove extra columns
  dipdrop <- dipdrop[,c(1, 2, 5)]
  # Pivot wide To count # of NA per row
  wide <- pivot_wider(dipdrop, names_from = "flight", values_from = "heightNA")
  # Make new column with na counts
  wide$na_count <- apply(wide, 1, function(x) sum(is.na(x)))
  # Pivot back to long to filter out na counts past certain threshold
  long <- wide %>%
    pivot_longer(cols = 2:(ncol(wide) -1), names_to = "flight", values_to = "height")
  # Count na
  plot_na <- long %>%
    summarise(sum(is.na(height))) 
  # Filter plots with na count > 3
  filter <- long
  filter$height <-  ifelse(filter$na_count >= 3, NA, filter$height)
  # Count na
  filter%>%
    summarise(sum(is.na(height))) # 212 NA data points
  # remove any plots below 0 
  filter[filter < 0] <- NA
  # Pivot back to wide
  final <- pivot_wider(filter, names_from = "flight", values_from = "height")
  # remove na_count column
  final$na_count <- NULL
  
  ## VISUALIZE AND SAVE DATA ##
  
  # Save data frame
  write.csv(final, paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a], "/Data_Analysis/Final_file_b4_ANOVA_", Year[a], ".csv"),  row.names = FALSE)
  # plot the data over time
  plot <- filter %>%
    separate(flight, into = c("X","flight"), sep = "X") %>%
    select(-c(na_count, X)) %>%
    ggplot(aes(x = as.numeric(flight), y = height)) +
      geom_line(aes(group = Square), color = 'gray') +
      stat_smooth(method="loess", formula = y ~ x) +
      stat_summary(aes(as.numeric(flight), height), geom = 'point', fun = mean, shape = 17, size = 3, col = "red") +
      theme_bw() +
      xlab('Flight (GDD)') +
      ylab('Plant Height') +
      ggtitle(paste0(Year, " Plant Height over Time - Outliers Removed"))
  # export the plot
  pdf(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Normalized_Cleaned_Plant_Height_WholeField_",Year[a],".pdf"))
    print(plot)
  dev.off()
  # add the decriptive data back in
  data.all <- merge(final, descript.data, by = "Square")
  # remove erroneous yield data before printing - first row of each combine pass (inaccurately low)
  if (a == 2) { 
    data.all <- data.all %>%
      filter(Range != 5)
  } else if (a == 3) {
    data.all <- data.all %>%
      filter(Range != 6)
  }
  # export the data frame with the descriptive data and yield
  write.csv(data.all, paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a], "/Data_Analysis/Final_file_b4_ANOVA_w_Yield_", Year[a], ".csv"),  row.names = FALSE)
}

```
## Fit Loess Models to Growth Curves

  - reads in "Final_file_b4_ANOVA_w_Yield_",Year,".csv" OR
             "Full_Height_GDD_w_Yield_ManualPlots_1_",Year[a],".csv"
  - exports:
    "All_Spans_Plot_Waseca_",Year,".pdf"
    "Best_Span_Plot_Waseca_",Year,".pdf"
    "Loess_Predictions_Waseca_",Year,".txt"
    "All_Spans_Plot_Manual_Waseca_",Year,".pdf"
    "Best_Span_Plot_Manual_Waseca_",Year,".pdf"
    "Loess_Predictions_Waseca_Manual_",Year,".txt"
    
```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

library(caret)
library(mlbench)
library(tidyverse)

############################ INPUTS ##############################
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- c("2020","2021","2022")
Field <- c("1","1","1")
##################################################################
# for loop to go through each year/location
for (a in 1:length(Year)) {
  # add a for loop for going through both whole field and manual plot data for each year
  for (e in 1:2) {
    if (e ==1){
      # read in the data
      data <- read.csv(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Final_file_b4_ANOVA_w_Yield_",Year[a],".csv"))
      # reorder data to Square Range Row Heights Mean.Yld.lb.ac Mean.Yld.bu.ac
      data <- data[,c(1,(ncol(data)-3),(ncol(data)-2),2:(ncol(data)-4),(ncol(data)-1),ncol(data))]
      # subset data to remove the yield data
      height.data <- data %>%
        dplyr::select(-c(Mean.Yld.lb.ac,Mean.Yld.bu.ac)) 
      # make a long version with Date being numeric
      height.long <- height.data %>%
        pivot_longer(cols = 4:ncol(height.data), names_to = "XDate", values_to = "Height") %>%
        separate(col = XDate, c(NA,"Date"), sep = "X") %>%
        mutate(Date = as.numeric(Date))
    } else {
      # read in the data
      data <- read.csv(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Full_Height_GDD_w_Yield_ManualPlots_1_",Year[a],".csv"))
      # subset data to remove the yield data
      height.data <- data %>%
        dplyr::select(-c(X,Mean.Yld.lb.ac,Mean.Yld.bu.ac)) 
      # remove 06082021
      if (Year[a] == "2021") {
        height.data$X501 <- NULL
      }
      # make a long version with Date being numeric
      height.long <- height.data %>%
        pivot_longer(cols = 2:ncol(height.data), names_to = "XDate", values_to = "Height") %>%
        separate(col = XDate, c(NA,"Date"), sep = "X") %>%
        mutate(Date = as.numeric(Date))
    }
    ## DETERMINE THE IDEAL SPAN WITH A K-MEANS CROSS-VALIDATION ##
    #height.long <- na.omit(height.long)
    # set up the cross validation
    span.seq <- seq(from = 0.15, to = 0.95, by = 0.05) # range of spans to be tested
    k <- 100 # number of folds to be tested
    set.seed(1) # makes the results repeatable
    folds <- sample(x = 1:k, size = nrow(height.long), replace = TRUE)
    cv.error.mtrx <- matrix(rep(x = NA, times = k * length(span.seq)), 
                            nrow = length(span.seq), ncol = k)
    # for loop to go through each span
    for (b in 1:length(span.seq)) {
      # for loop to go through all folds
      for (c in 1:k) {
        skip_to_next <- F
        loess.fit <- loess(formula = Height ~ Date, data = height.long[folds != c, ], span = span.seq[b])
        tryCatch(preds <- predict(object = loess.fit, newdata = height.long[folds == c, ], na.action = "na.pass"), error = function(e) { skip_to_next <<- T})
        if(skip_to_next) {next}
        cv.error.mtrx[b, c] <- mean((height.long$Height[folds == c] - preds)^2, na.rm = TRUE) 
        # some predictions result in 'NA' because of the ranges in each fold
      }
    }
    # mean error for each fold
    cv.errors <- rowMeans(cv.error.mtrx, na.rm = T)
    # determine which span was the best span
    best.span.i <- which.min(cv.errors) # save the ordinal number of best span
    span.seq[best.span.i] # print the best span
    # fit a loess curve to the entire dataset
    best.loess.fit <- loess(formula = Height ~ Date, data = height.long, span = span.seq[best.span.i])
    x.seq <- seq(from = min(as.numeric(height.long$Date)), to = max(as.numeric(height.long$Date)), by = 50) # set the intervals
    if (e==1){
      # plot the span errors and make the lowest error red
      pdf(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/All_Spans_Plot_Waseca_",Year[a],".pdf"), height = 3, width = 4)

      plot(x = span.seq, y = cv.errors, xlab = "Spans", ylab = "Mean Cross Validation Error",type = "l", main = "CV Plot")
      points(x = span.seq, y = cv.errors, pch = 20, cex = 2, col = "blue")
      points(x = span.seq[best.span.i], y = cv.errors[best.span.i], pch = 20, cex = 3, col = "red")

      dev.off()
      # plot the loess curve
      pdf(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Best_Span_Plot_Waseca_",Year[a],".pdf"), height = 3, width = 4)

      plot(x = height.long$Date, y = height.long$Height, xlab = "Date (GDDs)", ylab = "Normalized Height", main = "Best Span Plot")
      lines(x = height.long$Date, y = height.long$Height)
      lines(x = x.seq, y = predict(object = best.loess.fit, newdata = data.frame(Date = x.seq)), col = "red", lwd = 2)

      dev.off()
      ## FIT LOESS CURVES FOR EACH PLOT ##
      # for loop to go through every plot and fit individual loess curves 
      for (d in 1:nrow(height.data)) {
        # break out the individual plots
        indiv.long <- height.data %>%
          slice(d) %>%
          pivot_longer(cols = c(4:ncol(height.data)), names_to = "XDate", values_to = "Height") %>%
          separate(col = XDate, c(NA,"Date"), sep = "X")
        # count the NAs in Height
        NA.count <- sapply(indiv.long, function(x) sum(is.na(x)))
        # if statement to remove individual plot from loess fitting if too many NAs
        if (NA.count[5] > ((ncol(height.data)-3)/4)) {
          #Too.many.NA <- append(Too.many.NA, as.matrix(indiv.long[1,1]))
          if (d == 1) {
            # make a data frame holding the GDD dates
            predictions <- data.frame(seq(from = 50, 
                                        to = plyr::round_any(max(as.numeric(height.long$Date)),50,f=ceiling), 
                                        by = 50))
            predictions$GDD <- predictions[,1]
            predictions[,1] <- NULL
            # print the table so there are column names 
            write.table(t(predictions), paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Loess_Predictions_Waseca_",Year[a],".txt"), col.names = F, sep = "\t")
          }
        } else {
          # continue with fitting loess curves
          # identify individual plot name
          plot.name <- as.matrix(height.data[d,])
          # set up an if statement to determine span
          if (span.seq[best.span.i] < 0.6) {
            span = 0.6
          } else {
            span = span.seq[best.span.i]
          }
          # run the model
          plot.loess <- loess(Height ~ Date, data = indiv.long, model = T, span = span)
          # make a data frame holding the GDD dates
          predictions <- data.frame(seq(from = 50, 
                                        to = 1450, 
                                        by = 50))
          # predict the loess heights (predictions$Height) using plot.loess and the Dates from above
          predictions$Height <- predict(plot.loess, data.frame(
            Date = seq(from = 50, 
                      to = 1450, 
                      by = 50)
            ))
          # Rename and reorganize file
          predictions$GDD <- predictions[,1]
          predictions[,1] <- NULL
          predictions <- predictions[, c(2,1)]
          # plot the loess curve for that plot
          ggplot(predictions, aes(x = GDD, y = Height)) +
            geom_point() +
            geom_line()
          # rename height column to the name of the plot
          colnames(predictions)[which(colnames(predictions)=="Height")] <- plot.name
          # make a file with plot predictions in order to keep row names and not export the GDD repeatedly
          plot.predictions <- predictions
          plot.predictions$GDD <- NULL
          plot.predictions <- t(plot.predictions)
          # export loess predictions
          if (d == 1) {
            write.table(t(predictions), paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Loess_Predictions_Waseca_",Year[a],".txt"), col.names = F, sep = "\t")
          } else {
            write.table(plot.predictions,paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Loess_Predictions_Waseca_",Year[a],".txt"),append=T,row.names=T,col.names=F,sep="\t")
          }
        }
      }
    } else {
      # plot the span errors and make the lowest error red
      pdf(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/All_Spans_Manual_Waseca_",Year[a],".pdf"))

      plot(x = span.seq, y = cv.errors, type = "l", main = "CV Plot")
      points(x = span.seq, y = cv.errors, pch = 20, cex = 0.75, col = "blue")
      points(x = span.seq[best.span.i], y = cv.errors[best.span.i], pch = 20, cex = 1, col = "red")

      dev.off()
      # plot the loess curve
      pdf(paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Best_Span_Manual_Waseca_",Year[a],".pdf"))

        plot(x = height.long$Date, y = height.long$Height, main = "Best Span Plot")
        lines(x = height.long$Date, y = height.long$Height)
        lines(x = x.seq, y = predict(object = best.loess.fit, newdata = data.frame(Date = x.seq)), col = "red", lwd = 2)

      dev.off()
      ## FIT LOESS CURVES FOR EACH PLOT ##
      # for loop to go through every plot and fit individual loess curves 
      for (d in 1:nrow(height.data)) {
        # break out the individual plots
        indiv.long <- height.data %>%
          slice(d) %>%
          pivot_longer(cols = c(2:ncol(height.data)), names_to = "XDate", values_to = "Height") %>%
          separate(col = XDate, c(NA,"Date"), sep = "X")
        # count the NAs in Height
        NA.count <- sapply(indiv.long, function(x) sum(is.na(x)))
        # if statement to remove individual plot from loess fitting if too many NAs
        if (NA.count[3] > ((ncol(height.data)-3)/4)) {
          #Too.many.NA <- append(Too.many.NA, as.matrix(indiv.long[1,1]))
          if (d == 1) {
            # make a data frame holding the GDD dates
            predictions <- data.frame(seq(from = 50, 
                                        to = plyr::round_any(max(as.numeric(height.long$Date)),50,f=ceiling), 
                                        by = 50))
            predictions$GDD <- predictions[,1]
            predictions[,1] <- NULL
            # print the table so there are column names 
            write.table(t(predictions), paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Loess_Predictions_Waseca_Manual_",Year[a],".txt"), col.names = F, sep = "\t")
      }
        } else {
          # continue with fitting loess curves
          # identify individual plot name
          plot.name <- as.matrix(height.data[d,])
          # set up an if statement to determine span
          if (span.seq[best.span.i] < 0.6) {
            span = 0.6
          } else {
            span = span.seq[best.span.i]
          }
          # run the model
          plot.loess <- loess(Height ~ Date, data = indiv.long, model = T, span = span)
          # make a data frame holding the GDD dates
          predictions <- data.frame(seq(from = 50, 
                                        to = 1450, 
                                        by = 50))
          # predict the loess heights (predictions$Height) using plot.loess and the Dates from above
          predictions$Height <- predict(plot.loess, data.frame(
            Date = seq(from = 50, 
                      to = 1450, 
                      by = 50)
            ))
          # Rename and reorganize file
          predictions$GDD <- predictions[,1]
          predictions[,1] <- NULL
          predictions <- predictions[, c(2,1)]
          # plot the loess curve for that plot
          ggplot(predictions, aes(x = GDD, y = Height)) +
            geom_point() +
            geom_line()
          # rename height column to the name of the plot
          colnames(predictions)[which(colnames(predictions)=="Height")] <- plot.name
          # make a file with plot predictions in order to keep row names and not export the GDD repeatedly
          plot.predictions <- predictions
          plot.predictions$GDD <- NULL
          plot.predictions <- t(plot.predictions)
          # export loess predictions
          if (d == 1) {
            write.table(t(predictions), paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Loess_Predictions_Waseca_Manual_",Year[a],".txt"), col.names = F, sep = "\t")
          } else {
            write.table(plot.predictions,paste0(Path,Year[a],"/Waseca/Production_",Field[a],"_",Year[a],"/Data_Analysis/Loess_Predictions_Waseca_Manual_",Year[a],".txt"),append=T,row.names=T,col.names=F,sep="\t")
          }
        }
      }
    }
  }
}
```

## Loess Regression Window Analysis

  - Prints total Loess curves for each date before cutting down
  - Cuts Loess predictions down to window of interest (50-1450)
  - combines Loess curve data from all years of interest
  - Reads in:
    "Loess_Predictions_Waseca_",Year[x],".txt"
    "Final_File_b4_ANOVA_w_Yield_",Year[x],".csv"
    "Loess_Predictions_Waseca_Manual_",Year[x],".txt"
    "Full_Height_GDD_w_Yield_NDVI_ManualPlots_1_",Year[x],".csv"
  - Prints "All_loess_curves_plot.pdf" &
    "All_environment_GDD_Loess_Predictions.txt" &
    "All_environment_Interval_Loess_Predictions.txt"
    "Final_Height_Data_Manual_Plot_",Year[x],".txt"

```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

library(caret)
library(mlbench)
library(tidyverse)
library(gtools)
library(car)
library(viridis)
library(MASS)

#####################################
# Set Path and Read in Information #
####################################
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- c("2020", "2021", "2022")
Field <- c("1", "1", "1")
Prediction_list <- list()

# initialize data frames
All_env_data <- data.frame()
Final_Height_data <- data.frame()
Final_Genotypes_data <- data.frame()

####################################
# Read in Data and Make Some Files #
###################################
# for loop to go through both manual plots and whole field grid
for (a in 1:2) {
  # for loop to go through all years
  for (x in 1:length(Year)) {
    Date_year <- Year[x]
    # if statement to determine which files to read in
    if (a ==1) {
      ## Whole field grid ##
      # read in the files
      Loess_Predictions <- read.delim(paste0(Path,"/",Year[x],"/Waseca/Production_",Field[x],"_",Year[x],"/Data_Analysis/Loess_Predictions_Waseca_",Year[x],".txt"), header = T)
      # read in the genotype data
      Genotypes <- read.csv(paste0(Path,"/",Year[x],"/Waseca/Production_",Field[x],"_",Year[x],"/Data_Analysis/Final_File_b4_ANOVA_w_Yield_",Year[x],".csv"))
      # rename square column to Plot
      Genotypes$Plot <- Genotypes$Square
      Genotypes$Square <- NULL
    } else {
      ## Manual plots ##
      # read in the files
      Loess_Predictions <- read.delim(paste0(Path,"/",Year[x],"/Waseca/Production_",Field[x],"_",Year[x],"/Data_Analysis/Loess_Predictions_Waseca_Manual_",Year[x],".txt"), header = T)
      # read in the genotype data
      Genotypes <- read.csv(paste0(Path,"/",Year[x],"/Waseca/Production_",Field[x],"_",Year[x],"/Data_Analysis/Full_Height_GDD_w_Yield_NDVI_ManualPlots_1_",Year[x],".csv"))
      Genotypes$X <- NULL
    }
    # rename and reorganize the file to prepare to add genotype data back 
    Loess_Predictions$Plot <- Loess_Predictions$GDD
    Loess_Predictions$GDD <- NULL
    Loess_Predictions <- Loess_Predictions[,c(ncol(Loess_Predictions),1:(ncol(Loess_Predictions)-1))]
    # remove Genotype columns starting with 'X' - normalized extracted height values
    Genotypes <- Genotypes %>%
      dplyr::select(-starts_with("X"))
    # Merge the Genotype information with the Loess regressions and name the file based on the year
    temp_Geno_Pred <- merge(Genotypes, Loess_Predictions, by = "Plot") 
    # add a column for year
    temp_Geno_Pred$Year <- Year[x]
    # reorganize file
    temp_Geno_Pred <- temp_Geno_Pred[,c(1:(ncol(temp_Geno_Pred)-30),ncol(temp_Geno_Pred),(ncol(temp_Geno_Pred)-29):(ncol(temp_Geno_Pred)-1))]
    # long form for loess curves
    temp_long <- temp_Geno_Pred %>%
    pivot_longer(cols = ((ncol(temp_Geno_Pred)-28):ncol(temp_Geno_Pred)), names_to = "GDD", values_to = "Height") %>%
    separate(col = GDD, c(NA, "GDD"), sep = "X") %>%
    na.omit()  
    # if statement to print the file with the right name
    if (a==1) {
      # print the loess curves
      pdf(paste0(Path,Year[x],"/Waseca/Production_",Field[x],"_",Year[x],"/Data_Analysis/Loess_curves_plot.pdf")) 
      # plot the loess curves
      temp_plot <- temp_long %>%
        ggplot(aes(x = as.numeric(GDD), y = Height, group = Plot, na.rm = T)) +
          geom_point(na.rm = T) +
          geom_line(na.rm = T) +
          geom_vline(xintercept = 1450, color = "red") 
      # print the ggplot
      print(temp_plot)
      dev.off()
      # add the temp_Geno_Pred file for the year to All_env_data
      All_env_data <- rbind(All_env_data, temp_Geno_Pred)
      # Make a column identifying the final plant height
      #Genotypes$Final <- Genotypes[,ncol(Genotypes)-5]
      # Add the final plant height data for the year to Final_Height_data
      #Final_Height_data <- rbind(Final_Height_data, Genotypes[,c(ncol(Genotypes)-1,ncol(Genotypes))])
      # isolate important information for Genotype identification
      #Final_Genotypes_data <- rbind(Final_Genotypes_data, Genotypes[,c(ncol(Genotypes)-1, ncol(Genotypes)-5, ncol(Genotypes)-4)]) 
    } else {
      # print the loess curves
      pdf(paste0(Path,Year[x],"/Waseca/Production_",Field[x],"_",Year[x],"/Data_Analysis/Loess_curves_manual_plot.pdf")) 
      # plot the loess curves
      temp_plot <- temp_long %>%
        ggplot(aes(x = as.numeric(GDD), y = Height, group = Plot, na.rm = T)) +
          geom_point(na.rm = T) +
          geom_line(na.rm = T) +
          geom_vline(xintercept = 1450, color = "red") 
      # print the ggplot
      print(temp_plot)
      dev.off()
      # export loess curve data with plots
      write.table(temp_Geno_Pred, paste0(Path,Year[x],"/Waseca/Production_",Field[x],"_",Year[x],"/Data_Analysis/Final_Height_Data_Manual_Plot_",Year[x],".txt"), sep = "\t", row.names = F)
      ######################################################
      # Use Loess Predicted GDD to Determine Growth Curves #
      ######################################################
      # Make a matrix of the slopes between points
      slope.matrix <- matrix(data = NA, nrow = nrow(temp_Geno_Pred), ncol = (ncol(temp_Geno_Pred)-5))
      colnames(slope.matrix) <- paste0("X", seq(50,1400, by = 50), "_X", seq(100, 1450, by = 50))
      # Make a list of X values to determine slope
      columnNameList <- as.data.frame(colnames(temp_Geno_Pred))
      columnNameList_dates <- separate(columnNameList, col = `colnames(temp_Geno_Pred)`, c(NA, "Date"), sep = "X")
      # determine slope values 
      for (i in 1:nrow(temp_Geno_Pred)) {
        for (j in 5:(ncol(temp_Geno_Pred)-1)) {
          if (is.na(temp_Geno_Pred[i,(j+1)])){
            next()
          } else if (is.na(temp_Geno_Pred[i,j])) {
            next()
          }
          slope.matrix[i,(j-4)] <- (temp_Geno_Pred[i,(j+1)] - temp_Geno_Pred[i,j])/(as.numeric(columnNameList_dates[(j+1),1]) - as.numeric(columnNameList_dates[j,1]))
        }
      }
      # combine slope data with descriptive data
      slope.matrix <- cbind(temp_Geno_Pred[,c(1:4)],slope.matrix)
      # Export slope.matrix so it can be used later
      write.table(slope.matrix, file = paste0(Path,Year[x],"/Waseca/Production_",Field[x],"_",Year[x],"/Data_Analysis/Final_Interval_Data_Manual_Plot_",Year[x],".txt"), sep = "\t", row.names = T)
      }
    } # end of year for loop
    # if statement to only complete following for whole field data
  if (a==1) {
    # export the final height data
    #write.table(Final_Height_data, paste0(Path, "All_Production/Final_Height_Data_Plot.txt"), sep = "\t", row.names = F)
    # Filter All_env_data down to only the genotype info and the window we care about (150-1450)
    #All_env_data <- All_env_data[,c(1:9,12:38)]
    #######################################################
    # Deal with All Environments and Loess Predicted GDDs #
    ######################################################
    #change the variables to characters
    All_env_data[,1] <- as.character(All_env_data[,1])
    All_env_data[,2] <- as.character(All_env_data[,2])
    All_env_data[,3] <- as.character(All_env_data[,3])
    All_env_data[,4] <- as.character(All_env_data[,4])
    All_env_data[,5] <- as.character(All_env_data[,5])
    All_env_data[,6] <- as.character(All_env_data[,6])
    # Make a long version of all environment data
    All_long <- All_env_data %>%
      pivot_longer(cols = (7:ncol(All_env_data)), names_to = "GDD", values_to = "Height") %>%
      separate(col = GDD, c(NA, "GDD"), sep = "X") %>%
      na.omit()
    ## Export the Loess Predictions 
    pdf(paste0(Path, "All_Production/All_loess_curves_plot.pdf"), height = 10, width = 15)  
    # plot the loess predictions
    All_loess <- ggplot(All_long, aes(x = as.numeric(GDD), y = Height, group = Plot, na.rm = T)) +
      geom_point(na.rm = T) +
      geom_line(na.rm = T) +
      facet_wrap(~Year, scales = "free") +
      xlab("Growing Degree Days") +
      theme_light()
    # print the plot
    print(All_loess)
    dev.off()
    # isolate names of columns containing loess data
    traits <- colnames(All_env_data)[7:ncol(All_env_data)]
    # Export All_env_data so it can be used later
    write.table(All_env_data, file = paste0(Path, "All_Production/All_environment_GDD_Loess_Predictions.txt"), sep = "\t", row.names = T)
    ######################################################
    # Use Loess Predicted GDD to Determine Growth Curves #
    ######################################################
    # Make a matrix of the slopes between points
    slope.matrix <- matrix(data = NA, nrow = nrow(All_env_data), ncol = (ncol(All_env_data)-7))
    colnames(slope.matrix) <- paste0("X", seq(50,1400, by = 50), "_X", seq(100, 1450, by = 50))
    # Make a list of X values to determine slope
    columnNameList <- as.data.frame(colnames(All_env_data))
    columnNameList_dates <- separate(columnNameList, col = `colnames(All_env_data)`, c(NA, "Date"), sep = "X")
    # determine slope values 
    for (i in 1:nrow(All_env_data)) {
      for (j in 7:(ncol(All_env_data)-1)) {
        if (is.na(All_env_data[i,(j+1)])){
          next()
        } else if (is.na(All_env_data[i,j])) {
          next()
        }
        slope.matrix[i,(j-6)] <- (All_env_data[i,(j+1)] - All_env_data[i,j])/(as.numeric(columnNameList_dates[(j+1),1]) - as.numeric(columnNameList_dates[j,1]))
      }
    }
    # combine slope data with descriptive data
    slope.matrix <- cbind(All_env_data[,c(1:6)],slope.matrix)
    # Export slope.matrix so it can be used later
    write.table(slope.matrix, file = paste0(Path, "All_Production/All_environment_Interval_Loess_Predictions.txt"), sep = "\t", row.names = T)
  } # end of if statement to just do for whole field
} # end of manual/whole for loop
```

## Correlation Analysis 

  - Test the correlation between each plant height and yields
  
```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

# load libraries
library(tidyverse)
library(gplots)
library(ellipse)
library(RColorBrewer)
#####################################
# Set Path and Read in Information #
####################################
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- c("2020", "2021", "2022")
Field <- c("1", "1", "1")
# read in loess curves from all years
GDD <- read.delim(paste0(Path,"All_Production/All_environment_GDD_Loess_Predictions.txt"))
# read in loess curve rates from all years
Int <- read.delim(paste0(Path,"All_Production/All_environment_Interval_Loess_Predictions.txt"))
##################
## GRID SQUARES ##
##################
## SET NECESSARY CONSTANTS FOR FOLLOWING FOR LOOPS ##
data.types <- list(GDD,Int)
data.types.names <- c("GDD","Int")
# set up the years to run through
Year <- c("All","2020","2021","2022")
# Build a Panel of 100 colors with Rcolor Brewer for the correlelogram
my_colors <- brewer.pal(5, "Spectral")
my_colors <- colorRampPalette(my_colors)(100)
## RUN THE CORRELATION ##
# for loop to go through both plant heights and growth rates
for (a in 1:length(data.types)) {
  # make a separate file removing descriptive data for correlation analysis
  loess.cor <- as.data.frame(data.types[a]) %>%
    dplyr::select(-c("Plot","Range","Row")) %>%
    relocate(Year, .before = Mean.Yld.lb.ac)
  # initiate file to hold correlations
  corr.file <- as.data.frame(matrix(ncol = (ncol(loess.cor)-1), nrow = 0))
  # collect the column names from the loess file
  temp.file <- colnames(loess.cor)
  # label the columns in initiated file
  colnames(corr.file) <- c(temp.file[1], "Data.Type", temp.file[4:length(temp.file)])
  # initiate file to hold all correlations
  all.cor.file <- as.data.frame(matrix(ncol = (ncol(loess.cor)-1), nrow = 0))
  # label the columns in initiated file
  colnames(all.cor.file) <- c(temp.file[1], "Data.Type", temp.file[4:length(temp.file)])
  # for loop to go through all the years
  for (b in 1:length(Year)) {
    # assign the year variable
    year <- Year[b]
    # if statement to determine if it needs to be filtered by year 
    if (year == "All") {
      loess.temp <- loess.cor
    } else {
      loess.temp <- loess.cor %>%
        filter(Year == year)
    } # end of if statement 
    # remove NA columns
    loess.temp.plot <- loess.temp %>%
      select_if(function(x) any(!is.na(x))) %>%
      dplyr::select(-Year)
    # calculate the correlation for all values
    data <- cor(loess.temp.plot, use = "complete.obs")
    # Order the correlation matrix
    ord <- order(data[1, ])
    data_ord <- data[ord, ord]
    # plot the correlation matrix (change data to data_ord if want it organized by correlation)
    corr.plot.temp <- plotcorr(data, col=my_colors[data*50+50] , mar=c(1,1,1,1))
    # export the correlation plot
    pdf(paste0(Path, "All_Production/",data.types.names[a],"_correlations_plot_",year,".pdf"))
    plotcorr(data, col=my_colors[data*50+50] , mar=c(1,1,1,1))
    dev.off()
    # for loop to go through the two types of yield
    for (c in 2:3) {
      # assign the year column
      corr.file[1,"Year"] <- year
      # assign the data type column
      corr.file[1,"Data.Type"] <- colnames(loess.temp)[c]
      # for loop for the actual correlations
      for (d in 4:ncol(loess.temp)) {
        # calculate the correlation between the yield and the flight and save in appropriate space in corr.file
        corr.file[1,(d-1)] <- cor(loess.temp[,c],loess.temp[,d])
      } # end of ALL CORRELATIONS for loop
      #rbind to all.corr.file
      all.cor.file <- rbind(all.cor.file, corr.file)
    } # end of YIELD TYPES for loop
  } # end of YEAR for loop
  # export all.cor.file for later use
  write.table(all.cor.file, paste0(Path, "All_Production/",data.types.names[a],"_correlations_grid_all.txt"))
} # end of DATA TYPES for loop

GDD_correlations_all <- read.delim(paste0(Path, "All_Production/GDD_correlations_grid_all.txt"), sep = " ") %>%
  filter(Data.Type == "Mean.Yld.bu.ac") %>%
  mutate(Height.Type = "GDD") %>%
  pivot_longer(cols = 3:31, names_to = "GDD", values_to = "Height") %>%
  separate(GDD, into = c(NA, "GDD"), sep = "X")
Int_correlations_all <- read.delim(paste0(Path, "All_Production/Int_correlations_grid_all.txt"), sep = " ") %>%
  filter(Data.Type == "Mean.Yld.bu.ac") %>%
  mutate(Height.Type = "Int") %>%
  pivot_longer(cols = 3:30, names_to = "GDD", values_to = "Height") %>%
  separate(GDD, into = c("GDD",NA), sep = "_") %>%
  separate(GDD, into = c(NA, "GDD"), sep = "X") 

# rbind GDD_correlations_all and Int_correlations_all
corr.all <- rbind(GDD_correlations_all, Int_correlations_all)
# pivot_wider
corr.all <- as.data.frame(corr.all %>%
  pivot_wider(names_from = "GDD", values_from = "Height"))
# remove any columns with only NAs
corr.all <- corr.all %>%
  select_if(function(x) any(!is.na(x)))
# identify the row names
rownames(corr.all) <- paste0(corr.all$Height.Type,"_",corr.all$Year)
# make a matrix and remove descriptive columns
corr.all <- as.matrix(corr.all[,-c(1:3)])

# plot a heatmap
# pdf(paste0(Path, "All_Production/All_important_Correlations_Bu_acre.pdf"),width = 8, height = 5)
# heatmap.2(corr.all, 
#           Colv = F, 
#           Rowv = F,
#           dendrogram = "none", 
#           trace = "none", 
#           scale = "none", 
#           na.color = "gray", 
#           cellnote = round(corr.all,2),
#           notecol = "black",
#           notecex = 0.75,
#           col = (colorRampPalette(c("red","white","blue"))(n = 74)), 
#           breaks = c(seq(-max(corr.all,na.rm=T),-0.3001,length=25),
#                      seq(-0.3,0.3,length=25),
#                      seq(0.3001,max(corr.all,na.rm=T),length=25)),
#           keysize = 2,
#           xlab = "Growing Degree Days",
#           ylab = "Data Type and Year",
#           lmat = rbind(c(0,4,0),c(2,1,0),c(0,3,0)),
#           lhei = c(1.5,4,0.1),
#           lwid = c(0.25,4,0.25),
#           key.title = "Correlation with Mean Yield (bu/acre)",
#           key.xlab = NA,
#           margins = c(4,15),
#           labRow = c("Plant Height All Years", "Plant Height 2020", "Plant Height 2021", "Plant Height 2022", "Growth Rate All Years", "Growth Rate 2020","Growth Rate 2021","Growth Rate 2022"))
# dev.off()

pdf(paste0(Path, "All_Production/All_important_Correlations_Bu_acre.pdf"),width = 8, height = 2)
heatmap.2(corr.all[1:4,], 
          Colv = F, 
          Rowv = F,
          dendrogram = "none", 
          trace = "none", 
          scale = "none", 
          na.color = "gray", 
          cellnote = round(corr.all[1:4,],2),
          notecol = "black",
          notecex = 0.75,
          cexRow = 1,
          col = (colorRampPalette(c("red","white","blue"))), 
          # breaks = c(seq(-1.01,-0.25001,length=75),
          #            seq(-0.25,0.25,length=50),
          #            seq(0.25001,1.01,length=75)),
          # keysize = 2,
          key = F,
          density.info = "none",
          xlab = "Growing Degree Days",
          ylab = "Year",
          lmat = rbind(c(0,4,0),c(2,1,0),c(0,3,0)),
          lhei = c(0.1,4,0.1),
          lwid = c(0.25,4,0.15),
          key.title = "Correlation with Mean Yield (bu/acre)",
          key.xlab = NA,
          margins = c(4,8),
          labRow = c("All Years", "2020", "2021", "2022"))

heatmap.2(corr.all[5:8,], 
          Colv = F, 
          Rowv = F,
          dendrogram = "none", 
          trace = "none", 
          scale = "none", 
          na.color = "gray", 
          cellnote = round(corr.all[5:8,],2),
          notecol = "black",
          notecex = 0.75,
          cexRow = 1,
          col = (colorRampPalette(c("red","white","blue"))),
          # col = (colorRampPalette(c("red","white","blue"))(n = 199)), 
          # breaks = c(seq(-1.01,-0.25001,length=75),
          #            seq(-0.25,0.25,length=50),
          #            seq(0.25001,1.01,length=75)),
          # keysize = 2,
          key = F, 
          density.info = "none",
          xlab = "Growing Degree Days",
          ylab = "Year",
          lmat = rbind(c(0,4,0),c(2,1,0),c(0,3,0)),
          lhei = c(0.1,4,0.1),
          lwid = c(0.25,4,0.15),
          key.title = "Correlation with Mean Yield (bu/acre)",
          key.xlab = NA,
          margins = c(4,8),
          labRow = c("All Years", "2020", "2021", "2022"))

heatmap.2(corr.all[1:4,], 
          Colv = F, 
          Rowv = F,
          dendrogram = "none", 
          trace = "none", 
          scale = "none", 
          na.color = "gray", 
          cellnote = round(corr.all[1:4,],2),
          notecol = "black",
          notecex = 0.75,
          symkey = F,
          col = (colorRampPalette(c("red","white","blue"))), 
          #breaks = c(seq(-1.01,-0.25001,length=75),
                     #seq(-0.25,0.25,length=50),
                     #seq(0.25001,1.01,length=75)),
          keysize = 2,
          density.info = "none",
          xlab = "Growing Degree Days",
          ylab = "Year",
          lmat = rbind(c(0,4,0),c(2,1,0),c(0,3,0)),
          lhei = c(1,1.5,0.1),
          lwid = c(0.25,4,0.25),
          key.title = "Correlation with Mean Yield (bu/acre)",
          key.xlab = NA,
          margins = c(4,15),
          labRow = c("All Years", "2020", "2021", "2022"))

dev.off()

#########################
## MANUAL MEASUREMENTS ##
#########################
# Set constants for following for loops
Year <- c("2020","2021","2022")
# for loop to go through the data types
for (h in c("GDD","Int")) {
  # for loop to go through all the years
  for (e in 1:length(Year)) {
    year <- Year[e]
    # read in loess curves for manual files
    if (h == "GDD") {
      loess <- read.delim(paste0(Path,year,"/Waseca/Production_1_",year,"/Data_Analysis/Final_Height_Data_Manual_Plot_",year,".txt"))
    } else if (h == "Int") {
      loess <- read.delim(paste0(Path,year,"/Waseca/Production_1_",year,"/Data_Analysis/Final_Interval_Data_Manual_Plot_",year,".txt"))
    }
    # remove yield columns
    loess <- loess %>%
      dplyr::select(-c(Year, Mean.Yld.bu.ac, Mean.Yld.lb.ac))
    # # rename the Plot column to the proper name
    # loess <- rename(loess, Plot = GDD)
    # read in manual measurement files with yield and NDVI data 
    manual <- read.csv(paste0(Path,year,"/Waseca/Production_1_",year,"/Data_Analysis/Full_Height_GDD_w_Yield_NDVI_ManualPlots_1_",year,".csv"))
    # remove all height information
    manual <- manual %>%
      dplyr::select(Plot,Mean.Yld.lb.ac:tidyselect::last_col())
    # read in the last manual measurement file (actual terminal height)
    if (year == "2020") {
      term.height <- read.csv(paste0(Path,year,"/Waseca/manual_measurements/07282020_manual.csv"))
    } else if (year == "2021") {
      term.height <- read.csv(paste0(Path,year,"/Waseca/manual_measurements/07232021_manual.csv"))
    } else if (year == "2022") {
      term.height <- read.csv(paste0(Path,year,"/Waseca/manual_measurements/07252022_manual.csv"))
    }
    # average the terminal plant height that was measured for each plot
    term.height$Terminal <- rowMeans(term.height[2:6])
    # remove all initial heights
    term.height <- term.height %>%
      dplyr::select(-starts_with("H"))
    # combine all data within a year into a single data file 
    data <- merge(loess,manual, by = "Plot")
    data <- merge(data, term.height, by = "Plot")
    # remove all columns with only NAs
    data <- data %>%
      relocate(Mean.Yld.bu.ac, .after = Plot) %>%
      relocate(Mean.Yld.lb.ac, .after = Plot) %>%
      select_if(function(x) any(!is.na(x))) %>%
      dplyr::select(-Plot) 
    ## CORRELATIONS PDFS ##
    data.cor <- cor(data, use = "complete.obs")
    # order the correlation matrix
    ord <- order(data.cor[1, ])
    data_ord <- data.cor[ord,ord]
    # plot the correlation matrix (change data.cor to data_ord if want it organized by correlation)
    corr.plot.temp <- plotcorr(data.cor, col=my_colors[data.cor*50+50] , mar=c(1,1,1,1))
    # export the correlation plot
    pdf(paste0(Path, "All_Production/",h,"_correlations_plot_manual_",year,".pdf"))
      plotcorr(data.cor, col=my_colors[data.cor*50+50] , mar=c(1,1,1,1))
    dev.off()
    # initiate file to hold correlations
    corr.file <- as.data.frame(matrix(ncol = (ncol(data)), nrow = 0))
    # collect the column names from the loess file
    temp.file <- colnames(data)
    # label the columns in initiated file
    colnames(corr.file) <- c("Year","Data.Type", temp.file[3:length(temp.file)])
    # initiate file to hold all correlations
    all.cor.file <- as.data.frame(matrix(ncol = (ncol(data)), nrow = 0))
    # label the columns in initiated file
    colnames(all.cor.file) <- c("Year","Data.Type", temp.file[3:length(temp.file)])
    # for loop to go through the two types of yield
    for (f in 1:2) {
      # assign the Year column
      corr.file[1,"Year"] <- year
      # assign the data type column
      corr.file[1,"Data.Type"] <- colnames(data)[f]
      # for loop for the actual correlations
      for (g in 3:ncol(data)) {
        # calculate the correlation between the yield and the flight and save in appropriate space in corr.file
        corr.file[1,(g)] <- cor(data[,f],data[,g], use = "complete.obs")
      } # end of ALL CORRELATIONS for loop
      # rbind to all.cor.file
      all.cor.file <- rbind(all.cor.file, corr.file)
    } # end of YIELD TYPES for loop
    # export all.cor.file for later use
    write.table(all.cor.file, paste0(Path, "All_Production/",h,"_correlations_manual_",year,".txt"))
  } # end of YEAR for loop 
} # end of data type for loop


GDD_correlations_2020 <- read.delim(paste0(Path, "All_Production/GDD_correlations_manual_2020.txt"), sep = " ") %>%
  filter(Data.Type == "Mean.Yld.bu.ac") %>%
  mutate(Height.Type = "GDD") %>%
  pivot_longer(cols = 3:23, names_to = "GDD", values_to = "Height") %>%
  separate(GDD, into = c(NA, "GDD"), sep = "X") %>%
  mutate(NDVI_date = NA,
         NDVI = NA)
GDD_correlations_2021 <- read.delim(paste0(Path, "All_Production/GDD_correlations_manual_2021.txt"), sep = " ") %>%
  filter(Data.Type == "Mean.Yld.bu.ac") %>%
  mutate(Height.Type = "GDD") %>%
  pivot_longer(cols = 3:20, names_to = "GDD", values_to = "Height") %>%
  separate(GDD, into = c(NA, "GDD"), sep = "X") %>%
  pivot_longer(cols = 3:11, names_to = "NDVI_date", values_to = "NDVI") %>%
  separate(NDVI_date, into = c(NA,"NDVI_date"), sep = "X")
GDD_correlations_2022 <- read.delim(paste0(Path, "All_Production/GDD_correlations_manual_2022.txt"), sep = " ") %>%
  filter(Data.Type == "Mean.Yld.bu.ac") %>%
  mutate(Height.Type = "GDD") %>%
  pivot_longer(cols = 3:20, names_to = "GDD", values_to = "Height") %>%
  separate(GDD, into = c(NA, "GDD"), sep = "X") %>%
  pivot_longer(cols = 3:14, names_to = "NDVI_date", values_to = "NDVI") %>%
  separate(NDVI_date, into = c(NA,"NDVI_date"), sep = "X")

Int_correlations_2020 <- read.delim(paste0(Path, "All_Production/Int_correlations_manual_2020.txt"), sep = " ") %>%
  filter(Data.Type == "Mean.Yld.bu.ac") %>%
  mutate(Height.Type = "Int") %>%
  pivot_longer(cols = 3:22, names_to = "GDD", values_to = "Height") %>%
  separate(GDD, into = c("GDD",NA), sep = "_") %>%
  separate(GDD, into = c(NA, "GDD"), sep = "X") %>%
  mutate(NDVI_date = NA,
         NDVI = NA)
Int_correlations_2021 <- read.delim(paste0(Path, "All_Production/Int_correlations_manual_2021.txt"), sep = " ") %>%
  filter(Data.Type == "Mean.Yld.bu.ac") %>%
  mutate(Height.Type = "Int") %>%
  pivot_longer(cols = 3:19, names_to = "GDD", values_to = "Height") %>%
  separate(GDD, into = c("GDD",NA), sep = "_") %>%
  separate(GDD, into = c(NA, "GDD"), sep = "X") %>%
  pivot_longer(cols = 3:11, names_to = "NDVI_date", values_to = "NDVI") %>%
  separate(NDVI_date, into = c(NA,"NDVI_date"), sep = "X")
Int_correlations_2022 <- read.delim(paste0(Path, "All_Production/Int_correlations_manual_2022.txt"), sep = " ") %>%
  filter(Data.Type == "Mean.Yld.bu.ac") %>%
  mutate(Height.Type = "Int") %>%
  pivot_longer(cols = 3:19, names_to = "GDD", values_to = "Height") %>%
  separate(GDD, into = c("GDD",NA), sep = "_") %>%
  separate(GDD, into = c(NA, "GDD"), sep = "X") %>%
  pivot_longer(cols = 3:14, names_to = "NDVI_date", values_to = "NDVI") %>%
  separate(NDVI_date, into = c(NA,"NDVI_date"), sep = "X")


# rbind GDD_correlations_all and Int_correlations_all
corr.all <- rbind(GDD_correlations_2020,GDD_correlations_2021,GDD_correlations_2022,Int_correlations_2020,Int_correlations_2021,Int_correlations_2022)
# pivot_wider
corr.all <- as.data.frame(corr.all %>%
  pivot_wider(names_from = "GDD", values_from = "Height")) %>%
  pivot_wider(names_from = "NDVI_date", values_from = "NDVI")
# remove any columns with only NAs
corr.all <- corr.all %>%
  select_if(function(x) any(!is.na(x))) %>%
  as.data.frame()
# identify the row names
rownames(corr.all) <- paste0(corr.all$Height.Type,"_",corr.all$Year)
# make a matrix and remove descriptive columns
corr.all <- as.matrix(corr.all[,-c(1,2,4)])

# plot a heatmap
pdf(paste0(Path, "All_Production/All_important_Manual_Correlations_Bu_acre.pdf"),width = 8, height = 5)
heatmap.2(corr.all,
          Colv = F, 
          Rowv = F,
          dendrogram = "none", 
          trace = "none", 
          scale = "none", 
          na.color = "gray", 
          #cellnote = round(corr.all,2),
          #notecol = "black",
          #notecex = 0.75,
          col = (colorRampPalette(c("red","white","blue"))(n = 74)), 
          breaks = c(seq(-max(corr.all,na.rm=T),-0.3001,length=25),
                     seq(-0.3,0.3,length=25),
                     seq(0.3001,max(corr.all,na.rm=T),length=25)),
          keysize = 2,
          xlab = "Growing Degree Days & Data Type",
          ylab = "Data Type and Year",
          lmat = rbind(c(0,4,0),c(2,1,0),c(0,3,0)),
          lhei = c(1.5,5,0.2),
          lwid = c(0.25,4,0.25),
          key.title = "Correlation with Mean Yield (bu/acre)",
          key.xlab = NA,
          margins = c(12,10),
          labRow = c("Plant Height 2020", "Plant Height 2021", "Plant Height 2022", "Growth Rate 2020","Growth Rate 2021","Growth Rate 2022"),
          cexRow = 1.0,
          labCol = c("Terminal Plant Height","350 Plant Height/Growth Rate","400 Plant Height/Growth Rate","450 Plant Height/Growth Rate","500 Plant Height/Growth Rate","550 Plant Height/Growth Rate","600 Plant Height/Growth Rate","650 Plant Height/Growth Rate","700 Plant Height/Growth Rate","750 Plant Height/Growth Rate","800 Plant Height/Growth Rate","850 Plant Height/Growth Rate","900 Plant Height/Growth Rate","950 Plant Height/Growth Rate","1000 Plant Height/Growth Rate","1050 Plant Height/Growth Rate","1100 Plant Height/Growth Rate","1150 Plant Height/Growth Rate","1200 Plant Height/Growth Rate","1250 Plant Height/Growth Rate","1300 Plant Height/Growth Rate","1350 Plant Height/Growth Rate","695.5 760nm","695.5 635nm", "695.5 NDVI", "813 760nm", "813 635nm", "813 NDVI", "986.5 760nm", "986.5 635nm", "986.5 NDVI", "429 760nm", "429 635nm", "429 NDVI", "546.5 760nm", "546.5 635nm", "546.5 NDVI", "739.5 760nm", "739.5 635nm", "739.5 NDVI", "844 760nm", "844 760nm", "844 NDVI")
          )
dev.off()

```
## Pull Down RGB Values from MSI

```{bash}
scp -r kirsc168@mangi.msi.umn.edu:/home/hirschc1/kirsc168/Production_Project/All_Production/Vegetative_Indices/Data_Analysis/RGB_Values/ /Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/Vegetative_Indices/Data_Analysis/RGB_Values/
```
## Evaluate Plot RGB and VI's
```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

library(ggplot2)
library(tidyverse)
library(reshape2)

# Set Path
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"

## READ IN RGB FILES ##
# make a list of all RGB Value filenames
filenames <- list.files(paste0(Path,"All_Production/Vegetative_Indices/Data_Analysis/RGB_Values"), pattern="*.txt", full.names = T)
# read in all files in filenames and rename columns for each file
ldf <- lapply(filenames, read.table, col.names = c("Red","Green","Blue"))
# name the files based on the date from the original file name
names(ldf) <- substr(filenames, 115, 122)
# read in loess curves with yield data
all.data <- read.delim(paste0(Path,"All_Production/All_environment_GDD_Loess_Predictions.txt"))
# keep only important columns
all.yield <- all.data %>%
  dplyr::select(c("Plot","Range","Row","Year","Mean.Yld.bu.ac"))
# initiate file to hold RGB values from all files
all.RGB <- as.data.frame(matrix(ncol = 22, nrow = 0))
colnames(all.RGB) <- c("Plot","Year","Date","Red","Green","Blue","BI","GLI","NGRDI","VARI","BGI","ExG","ExR","ExB","ExGR","MGRVI","RGBVI","GRRI","VEG","Range","Row","Mean.Yld.bu.ac")
# initiate file to hold all correlations with yield
yield.cor <- as.data.frame(matrix(ncol = 1, nrow = 17))
yield.cor$Index <- c("Red","Green","Blue","BI","GLI","NGRDI","VARI","BGI","ExG","ExR","ExB","ExGR","MGRVI","RGBVI","GRRI","VEG","Mean.Yld.bu.ac")
yield.cor$V1 <- NULL
# for loop to combine all into one file
for (a in 1:length(ldf)) {
  # save just the dataframe
  temp.data <- data.frame(ldf[a])
  # make a temporary file to hold all the values
  temp <- as.data.frame(matrix(ncol = 3, nrow = dim(temp.data)[1]))
  # assign plot numbers
  temp[,1] <- paste0("Plot",c(1:dim(temp.data)[1]))
  # assign Date
  temp[,2] <- names(ldf[a])
  # assign Year
  temp[,3] <- substr(names(ldf[a]),5,8)
  # add RGB values
  temp <- cbind(temp,temp.data)
  # rename columns
  colnames(temp) <- c("Plot","Date","Year","Red","Green","Blue")
  
  ## CALCULATE VEGETATION INDICES ##
  temp$BI <- sqrt((temp$Red^2 + temp$Green^2 + temp$Blue^2)/3) # Brightness Index
  temp$GLI <- (2*temp$Green - temp$Red - temp$Blue)/(2*temp$Green + temp$Red+ temp$Blue) # Green Leaf Index
  temp$NGRDI <- (temp$Green - temp$Red)/(temp$Green + temp$Red) # Normalized Green Red Difference Index
  temp$VARI <- (temp$Green - temp$Red)/(temp$Green + temp$Red - temp$Blue) # Visible Atmospherically Resistant Index
  temp$BGI <- temp$Blue/temp$Green # Blue Green Pigment Index
  temp$ExG <- 2*temp$Green - temp$Red - temp$Blue # Excess Green Index
  temp$ExR <- (1.4*temp$Red - temp$Green)/(temp$Green + temp$Red + temp$Blue) # Excess Red Index
  temp$ExB <- (1.4*temp$Blue - temp$Green)/(temp$Green + temp$Red + temp$Blue) # Excess Blue Index
  temp$ExGR <- temp$ExG - temp$ExR # Excess Green minus Excess Red
  temp$MGRVI <- (temp$Green^2 - temp$Red^2)/(temp$Green^2 + temp$Red^2) # Modified Green Red Index
  temp$RGBVI <- (temp$Green^2 - temp$Blue * temp$Red)/(temp$Green^2 + temp$Blue * temp$Red) # Red Green Blue Index
  temp$GRRI <- temp$Green/temp$Red # Green - Red Ratio Index
  temp$VEG <- temp$Green/((temp$Red^0.667)*(temp$Blue^(1-0.667))) # Vegetativen
  
  ## CALCULATE VI'S CORRELATION WITH YIELD AND EACH OTHER ##
  # merge temp with all.yield
  temp.yield <- merge(temp,all.yield,by = c("Plot","Year"))
  # calculate correlation
  temp.cor <- cor(temp.yield[,c(4:19,22)])
  # melt correlation for heatmap
  melted.cor <- melt(temp.cor)
  # plot heatmap
  temp.plot <- ggplot(melted.cor, aes(x = Var1, y = Var2, fill = value)) +
    geom_tile()
  # print heatmap
  pdf(paste0(Path,"All_Production/Vegetative_Indices/Data_Analysis/Indice_Correlations/All_indice_",names(ldf[a]),".pdf"))
  print(temp.plot)
  dev.off()
  
  ## SAVE INFORMATION FOR LATER USE ##
  # add yield correlation to file for later
  yield.cor <- cbind(yield.cor,temp.cor[,17])
  # change column name 
  colnames(yield.cor)[a +1] <- paste0("X",names(ldf[a]))
  # rbind all of the files into all.RGB
  all.RGB <- rbind(all.RGB,temp.yield)
}

# plot correlation of each day 
melted.yield.cor <- melt(yield.cor)

# reorder columns
yield.cor <- yield.cor %>%
  dplyr::select("Index","X06182020","X06232020","X06302020","X07062020","X06082021","X06162021","X06222021","X06302021","X07122021","X06102022","X06152022","X06232022","X06282022","X07052022","X07132022","X07182022")


# export correlation with yield
pdf(paste0(Path,"All_Production/Vegetative_Indices/Data_Analysis/Indice_Correlations/Indice_Yield_byYear.pdf"),height=6, width=9)
temp.plot <- gplots::heatmap.2(as.matrix(yield.cor[2:ncol(yield.cor)]),
                  na.rm = T,
                  dendrogram = "none",
                  Rowv = T,
                  Colv=F,
                  labCol = c("June 18 2020", "June 23, 2020","June 30, 2020","July 6, 2020","June 8, 2021","June 16, 2021", "June 22, 2021", "June 30, 2021","July 12, 2021","June 10, 2022","June 15, 2022","June 23, 2022", "June 28, 2022","July 5, 2022","July 13, 2022","July 18, 2022"),
                  trace = "none",
                  scale = "none",
                  offsetRow = 0.3,
                  offsetCol = 0.3,
                  colsep = c(4,9),
                  margin = c(3,8),
                  xlab = "",
                  ylab = "",
                  key = F,
                  cellnote = round(as.matrix(yield.cor[2:ncol(yield.cor)]),2),
                  notecol = "black",
                  lmat = rbind(c(0,3), c(2,1),c(0,4)),
                  lhei = c(0.1,0.9,0.1),
                  lwid = c(0.2,0.8),
                  breaks= c(-0.5,-0.4,-0.3,-0.2,-0.1,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,1),
                  col=c("#FF6060","#FF8080","#FF9F9F","#FFBFBF","#FFDFDF","#DFE3FF","#BFC7FF","#9FABFF","#8090FF","#6074FF","#4058FF","#203CFF","#0020FF"))
                  plotrix::gradient.rect(0.08, 0.25, 0.09, 0.75, nslices=13, border=F, gradient="y", col=c("#FF6060","#FF8080","#FF9F9F","#FFBFBF","#FFDFDF","#DFE3FF","#BFC7FF","#9FABFF","#8090FF","#6074FF","#4058FF","#203CFF","#0020FF"))
                  text(x=rep(0.07, 7), y=seq(0.26, 0.74, by=0.04), adj=1, cex=0.8, labels=c( "-0.4", "-0.3", "-0.2","-0.1","0","0.1","0.2","0.3","0.4", "0.5", "0.6","0.7", ">0.7"))
                  text(x=0.09, y=0.8, labels="Pearson  \nCorrelation", adj=1, cex=0.85)

dev.off()


# export all RGB values and indices together for future use
write.csv(all.RGB,paste0(Path,"All_Production/Vegetative_Indices/Data_Analysis/All_Dates_RGB_Indices.csv"))
```
## Calculate FPCA for VI Comparison over years

```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

library(tidyverse)
library(fda)

Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/" # Set path

all.RGB <- read.csv(paste0(Path,"All_Production/Vegetative_Indices/Data_Analysis/All_Dates_RGB_Indices.csv"), row.names = 1) # read file

# read in Date to GDD
GDD.date <- read.csv(paste0(Path,"All_Production/GDD_Accumulation_AllYears.csv"), row.names = 1) %>%
  dplyr::select(Date,cum.GDD) %>% # select Date and cummulative GDD
  unique() # select unique rows
# merge to add the GDD
all.RGB.GDD <- merge(GDD.date, all.RGB, by = "Date")

####################################################
## CALCULATE EIGENVALUES TO DESCRIBE LOESS CURVES ##
####################################################
# initiate file to contain all data with the eigenvalues
all.data.eigen <- as.data.frame(matrix(nrow = 0, ncol = 5))
colnames(all.data.eigen) <- c("Plot","Year","veg.ind", "eigen.1", "eigen.2")

# set the seed
set.seed(7)

# for loop to go through each year
for (a in c("2020","2021","2022")) {
  all.RGB.year <- all.RGB.GDD %>%
    filter(Year == a)
  # for loop to go through each vegetative indices
  for (b in 5:(ncol(all.RGB.year)-3)) {
    all.RGB.ind <- all.RGB.year[,c(2,3,b)] %>% # select plot, Date, and indice
      arrange(Plot)
    # make a wide format of the data
    all.RGB.w <- all.RGB.ind %>%
      pivot_wider(names_from = "cum.GDD", values_from = colnames(all.RGB.ind[3]))
    # create necessary descriptive data outputs 
    knots    = c(seq(min(all.RGB.ind$cum.GDD,na.rm = T),max(all.RGB.ind$cum.GDD,na.rm = T),5)) #Location of knots
    n_knots   = length(knots) #Number of knots
    n_order   = 4 # order of basis functions: for cubic b-splines: order = 3 + 1
    n_basis   = length(knots) + n_order - 2;
    basis = create.bspline.basis(rangeval = c(min(all.RGB.ind$cum.GDD),max(all.RGB.ind$cum.GDD)), n_basis)
    argvals <- matrix(all.RGB.ind$cum.GDD, nrow = (ncol(all.RGB.w)-1), ncol = nrow(all.RGB.w))
    y_mat <- matrix(all.RGB.ind[,3], nrow = (ncol(all.RGB.w)-1), ncol = nrow(all.RGB.w))
    # put data frame in format understood by pca.fd function
    W.obj <- Data2fd(argvals = argvals, y = y_mat, basisobj = basis, lambda = 0.5)
    # calculate the eigenvalues
    fun_pca <- pca.fd(W.obj, nharm = 5) # nharm = 5 indicates the calculation of 5 eigenvalues
    # plot the eigenvectors
    plot(fun_pca$harmonics, lwd = 3)
    # obtain the eigenvalues
    fun_pca$values
    # proportion explained by each eigenvalue
    fun_pca$varprop
    
  # combine Plot with top two FPCA scores
  temp <- cbind(all.RGB.w[,1],fun_pca$scores[,1:2])
  temp <- temp %>%
    mutate(Year = a, # add column with year
           veg.ind = colnames(all.RGB.ind[3])) %>% # add column with vegetative indice
    relocate("Plot","Year","veg.ind","1","2") %>% # reorder them to match
    rename(eigen.1 = `1`, # rename eigen 1
           eigen.2 = `2`) # rename eigen 2
# combine the fpc
  all.data.eigen <- rbind(all.data.eigen, temp)
  } # end of vegetative indice loop
} # end of year loop

# export eigenvalue data
write.csv(all.data.eigen,paste0(Path,"All_Production/Vegetative_Indices/Data_Analysis/All_Vegetative_Indices_FPCA.csv"))
```
## Plot GDD by Year

  - figure out where to split flights for vegetative indice analysis
  
```{r}
library(tidyverse)

# set path
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"

# read in GDD accumulation data
gdd.2020 <- read.csv(paste0(Path,"2020/Waseca/GDD_Accumulation_2020.csv"), row.names = 1)
gdd.2021 <- read.csv(paste0(Path,"2021/Waseca/GDD_Accumulation_2021.csv"), row.names = 1)
gdd.2022 <- read.csv(paste0(Path,"2022/Waseca/GDD_Accumulation_2022.csv"), row.names = 1)

vi.all <- read.csv(paste0(Path,"All_Production/Vegetative_Indices/Data_Analysis/All_Dates_RGB_Indices.csv"),row.names = 1) %>%
  dplyr::select("Year","Date") %>%
  unique()

# combine all gdd accumulation files
gdd <- rbind(gdd.2020,gdd.2021,gdd.2022)

vi.gdd <- merge(vi.all,gdd, by = "Date")

# plot gdd x year
ggplot(vi.gdd, aes(x = cum.GDD, y = as.factor(Year))) +
  geom_point() +
  theme_classic()
```
## Group and Average VI's

  - Based on plot of GDD by Year, flights are grouped into 4 timepoints across the growing season. 
  - Values for flights within the same time point and same year will be averaged so there is one value 
  - Grouping is as follows:
    - Early: 06182020, 06082021, 06102022, 06152022
    - Early Exponential: 06232020, 06162021, 06222021, 06232022, 06282022
    - Late Exponential: 06302020, 06302021, 07052022
    - Late: 07062020, 07122021, 07132022, 07182022
  - Groups and outputs VI values from across years

```{r}
library(tidyverse)

# set path
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"

# read in GDD accumulation data
gdd.2020 <- read.csv(paste0(Path,"2020/Waseca/GDD_Accumulation_2020.csv"), row.names = 1)
gdd.2021 <- read.csv(paste0(Path,"2021/Waseca/GDD_Accumulation_2021.csv"), row.names = 1)
gdd.2022 <- read.csv(paste0(Path,"2022/Waseca/GDD_Accumulation_2022.csv"), row.names = 1)

# combine all gdd accumulation files
gdd <- rbind(gdd.2020,gdd.2021,gdd.2022)

vi.all <- read.csv(paste0(Path,"All_Production/Vegetative_Indices/Data_Analysis/All_Dates_RGB_Indices.csv"),row.names = 1)

vi.gdd <- merge(vi.all,gdd, by = "Date") %>%
  dplyr::select("Plot","Date","cum.GDD","Year","Range","Row","Mean.Yld.bu.ac","Red","Green","Blue","BI","GLI","NGRDI","VARI","BGI","ExG","ExR","ExB","ExGR","MGRVI","RGBVI","GRRI","VEG")

# turn dates into early, early.exp, late.exp, late
vi.gdd.n <- vi.gdd %>%
  mutate(across('Date', str_replace, '6182020', "early"),
         across('Date', str_replace, '6082021', 'early'),
         across('Date', str_replace, '6102022', 'early'),
         across('Date', str_replace, '6152022', 'early'),
         across('Date', str_replace, '6232020', 'early.exp'),
         across('Date', str_replace, '6162021', 'early.exp'),
         across('Date', str_replace, '6222021', 'early.exp'),
         across('Date', str_replace, '6232022', 'early.exp'),
         across('Date', str_replace, '6282022', 'early.exp'),
         across('Date', str_replace, '6302020', 'late.exp'),
         across('Date', str_replace, '6302021', 'late.exp'),
         across('Date', str_replace, '7052022', 'late.exp'),
         across('Date', str_replace, '7062020', 'late'),
         across('Date', str_replace, '7122021', 'late'),
         across('Date', str_replace, '7132022', 'late'),
         across('Date', str_replace, '7182022', 'late')
  )

# Make file useful for machine learning
vi.wide <- vi.gdd.n %>%
  pivot_longer(cols = 8:ncol(vi.gdd.n), names_to = "veg.ind", values_to = "ind.val") %>% # pivot longer
  group_by(Plot,Date,Year,veg.ind) %>% # group by timepoint and year to average the values
  mutate(ave.val = mean(ind.val)) %>% # average values for the same timepoint within a year
  dplyr::select(-c("cum.GDD","ind.val")) %>% # remove unnecessary column
  tidyr::unite(col = "time.veg.ind", c("Date","veg.ind"), sep = "_", remove = T) %>% # combine timepoint grouping with vegetative indice name
  unique() %>% # filter only to unique values
  pivot_wider(names_from = 'time.veg.ind', values_from = 'ave.val') # pivot back wider with columns of time and veg.ind

# export for use in Machine model
write.csv(vi.wide, paste0(Path, "All_Production/Vegetative_Indices/Data_Analysis/All_Vegetative_Indices_Across_Years_InTimepoints.csv"))

## calculate correlation with Mean.Yld.bu.ac ##
all.year.cor <- data.frame() # initiate file to hold correlations
Years <- c("2020","2021","2022") # list years
# for loop to go through years
for (a in 1:length(Years)) {
  year <- Years[a] # set year variable
  # filter the year
  temp.file <- vi.wide %>%
    filter(Year == year)
  # calculate correlation
  temp.cor <- as.data.frame(cor(temp.file[,5:ncol(temp.file)]))
  temp.cor$Year <- year
  all.year.cor <- rbind(all.year.cor, temp.cor[1,])
}

## Plot correlation ##
cor.matrix <- all.year.cor %>%
  dplyr::select(-"Mean.Yld.bu.ac") %>% # remove yield
  pivot_longer(cols = 1:(ncol(all.year.cor)-2), names_to = "name",values_to = "values") %>% # pivot longer
  separate(name, into = c("time","veg.ind"), sep = "_") %>% # separatte column name into timepoint and veg.ind
  tidyr::unite(timeframe, c("Year","time"),sep = "_", remove = T) %>% # combine Year and time into timeframe
  pivot_wider(names_from = veg.ind, values_from = values) %>% # pivot wider
  na.omit() %>% # remove row with all NAs
  as.data.frame() # make into a data frame

reord.cor.matrix <- cor.matrix %>%
  arrange(match(timeframe,c("2020_early","2021_early","2022_early","2020_early.exp","2021_early.exp","2022_early.exp","2020_late.exp","2021_late.exp","2022_late.exp","2021_late","2022_late")))
  

pdf(paste0(Path,"All_Production/Vegetative_Indices/Data_Analysis/Indice_Correlations/Indice_Yield_byTimepoint.pdf"),height=6, width=9)

row.names(cor.matrix) <- cor.matrix$timeframe

temp.plot <- gplots::heatmap.2(t(as.matrix(cor.matrix[2:ncol(cor.matrix)])),
                  na.rm = T,
                  dendrogram = "none",
                  Rowv = T,
                  Colv=F,
                  trace = "none",
                  scale = "none",
                  offsetRow = 0.3,
                  offsetCol = 0.3,
                  colsep = c(3,7),
                  margin = c(3,8),
                  xlab = "",
                  ylab = "",
                  key = F,
                  cellnote = t(round(as.matrix(cor.matrix[2:ncol(cor.matrix)]),2)),
                  notecol = "black",
                  lmat = rbind(c(0,3), c(2,1),c(0,4)),
                  lhei = c(0.1,0.9,0.2),
                  lwid = c(0.2,0.8),
                  breaks= c(-0.5,-0.4,-0.3,-0.2,-0.1,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,1),
                  col=c("#FF6060","#FF8080","#FF9F9F","#FFBFBF","#FFDFDF","#DFE3FF","#BFC7FF","#9FABFF","#8090FF","#6074FF","#4058FF","#203CFF","#0020FF"))
                  plotrix::gradient.rect(0.08, 0.25, 0.09, 0.75, nslices=13, border=F, gradient="y", col=c("#FF6060","#FF8080","#FF9F9F","#FFBFBF","#FFDFDF","#DFE3FF","#BFC7FF","#9FABFF","#8090FF","#6074FF","#4058FF","#203CFF","#0020FF"))
                  text(x=rep(0.07, 7), y=seq(0.26, 0.74, by=0.04), adj=1, cex=0.8, labels=c( "-0.4", "-0.3", "-0.2","-0.1","0","0.1","0.2","0.3","0.4", "0.5", "0.6","0.7", ">0.7"))
                  text(x=0.09, y=0.8, labels="Pearson  \nCorrelation", adj=1, cex=0.85)
                  title(main="Correlation with Yield Grouped by Year", line=1, oma=T, adj=0.21)
                  
temp.plot <- gplots::heatmap.2(t(as.matrix(reord.cor.matrix[2:ncol(reord.cor.matrix)])),
                  na.rm = T,
                  dendrogram = "none",
                  Rowv = T,
                  Colv=F,
                  trace = "none",
                  scale = "none",
                  offsetRow = 0.3,
                  offsetCol = 0.3,
                  colsep = c(3,6,9),
                  margin = c(3,8),
                  xlab = "",
                  ylab = "",
                  key = F,
                  cellnote = t(round(as.matrix(reord.cor.matrix[2:ncol(reord.cor.matrix)]),2)),
                  notecol = "black",
                  lmat = rbind(c(0,3), c(2,1),c(0,4)),
                  lhei = c(0.1,0.9,0.2),
                  lwid = c(0.2,0.8),
                  breaks= c(-0.5,-0.4,-0.3,-0.2,-0.1,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,1),
                  col=c("#FF6060","#FF8080","#FF9F9F","#FFBFBF","#FFDFDF","#DFE3FF","#BFC7FF","#9FABFF","#8090FF","#6074FF","#4058FF","#203CFF","#0020FF"))
                  plotrix::gradient.rect(0.08, 0.25, 0.09, 0.75, nslices=13, border=F, gradient="y", col=c("#FF6060","#FF8080","#FF9F9F","#FFBFBF","#FFDFDF","#DFE3FF","#BFC7FF","#9FABFF","#8090FF","#6074FF","#4058FF","#203CFF","#0020FF"))
                  text(x=rep(0.07, 7), y=seq(0.26, 0.74, by=0.04), adj=1, cex=0.8, labels=c( "-0.4", "-0.3", "-0.2","-0.1","0","0.1","0.2","0.3","0.4", "0.5", "0.6","0.7", ">0.7"))
                  text(x=0.09, y=0.8, labels="Pearson  \nCorrelation", adj=1, cex=0.85)
                  title(main="Correlation with Yield Grouped by Timepoint", line=1, oma=T, adj=0.21)

dev.off()
```
## Correlation Between Same VIs

```{r}
library(tidyverse)
vi.data <- read.csv("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/Vegetative_Indices/Data_Analysis/All_Dates_RGB_Indices.csv", row.names = 1) %>%
  dplyr::select(-c("Range","Row","Mean.Yld.bu.ac")) # remove non-vegetative indice columns

vi.data.long <- vi.data %>%
  pivot_longer(cols = 4:ncol(.), names_to = "Veg.Ind", values_to = "Ind.Value")

# initiate file to hold correlation data
all.correlations <- as.data.frame(matrix(nrow = 0, ncol = 5))
# for loop to go through each vegetative indice
for(a in 4:ncol(vi.data)) {
  indice <- colnames(vi.data)[a] # set the vegetative indice name constant
  vi.temp <- vi.data.long %>%
    filter(Veg.Ind == indice) %>% # filter to only values of the vegetative indice
    dplyr::select(-Year) %>% # remove the year column
    pivot_wider(names_from = "Date",values_from = "Ind.Value") # pivot wider with the dates as the column names

  # for loop to go through each pairing
  for(b in 3:ncol(vi.temp)) { # for loop for first value in pairing (b in 3:(ncol(vi.temp)-1))
    for(c in 3:ncol(vi.temp)) { # for loop for second value in pairing (c in (b+1):ncol(vi.temp))
      temp.pearson <- cor(vi.temp[,b],vi.temp[,c], use = "complete.obs", method = "pearson") # calculate pearson correlation
      temp.spearman <- cor(vi.temp[,b],vi.temp[,c], use = "complete.obs", method = "spearman") # calculate spearman correlation
      # append the information to all.correlations
      all.correlations <- rbind(all.correlations,list(indice,
                                                      colnames(vi.temp[b]),
                                                      colnames(vi.temp[c]),
                                                      temp.pearson,
                                                      temp.spearman))
    } # end of c for loop
  } # end of b for loop
} # end of a for loop
colnames(all.correlations) <- c("veg.ind","date.1","date.2","pearson.cor","spearman.cor") # change the column names

for(d in 1:length(unique(all.correlations$veg.ind))) {
  indice <- unique(all.correlations$veg.ind)[d]
  #plot the  correlation between dates of the same indice
  temp.plot <- all.correlations %>%
  dplyr::select(-spearman.cor) %>%
  pivot_wider(names_from = "veg.ind", values_from = "pearson.cor") %>%
  ggplot(aes(x = factor(date.1, levels = c("6182020","6232020","6302020","7062020","6082021","6162021","6222021","6302021","7122021","6102022","6152022","6232022","6282022","7052022","7132022","7182022")),
             y = factor(date.2,levels = c("6182020","6232020","6302020","7062020","6082021","6162021","6222021","6302021","7122021","6102022","6152022","6232022","6282022","7052022","7132022","7182022")),
             fill = get(indice))) +
  geom_tile() +
  geom_text(aes(label = round(get(indice),2))) +
  scale_fill_gradient2(low = "red",
                       mid = "white",
                       high = "blue")
  temp.plot.2 <- all.correlations %>%
  dplyr::select(-spearman.cor) %>%
  pivot_wider(names_from = "veg.ind", values_from = "pearson.cor") %>%
  ggplot(aes(x = date.1,
             y = date.2,
             fill = get(indice))) +
  geom_tile() +
  geom_text(aes(label = round(get(indice),2))) +
  scale_fill_gradient2(low = "red",
                       mid = "white",
                       high = "blue")
  
  # export the plot
  pdf(paste0("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/Vegetative_Indices/Data_Analysis/Indice_Correlations/",indice,"_correlation_all_dates.pdf"), width = 10,height = 5)
  print(temp.plot)
  print(temp.plot.2)
  dev.off()
}
```
# Data Analysis
## PLOT VARIATION ACROSS YEARS

  - yield variation within and across years
  - height variation within and across years
  - ndvi?

```{r}
library(tidyverse)
library(ggpmisc)
library(RColorBrewer)

Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/"

# read in data files
GDD <- read.delim(paste0(Path,"All_environment_GDD_Loess_Predictions.txt"))
Int <- read.delim(paste0(Path, "All_environment_Interval_Loess_Predictions.txt"))

# plot height distribution over time for each year
# temp.plot <- GDD %>%
#   dplyr::select(Plot, Year, starts_with("X")) %>%
#   pivot_longer(cols = 3:31, names_to = "GDD", values_to = "Height") %>%
#   separate(GDD, into = c(NA,"GDD"), sep = "X") %>%
#   ggplot(aes(x = as.numeric(GDD), y = Height)) +
#   geom_jitter(aes(color = as.character(Year)), alpha = 0.2, show.legend = F) +
#   geom_boxplot(aes(group = GDD), alpha = 0.2) +
#   theme_classic() +
#   xlim(300,1350) +
#   xlab("Growing Degree Days") +
#   scale_fill_discrete(breaks = c(350,400,450,500,550,600,650,700,750,800,850,900,950,1000,1050,1100,1150,1200,1250,1300,1350)) +
#   scale_color_manual(values = c("#00C8AF","#AF00C8","#C8AF00")) +
#   facet_wrap(~Year)

# pdf(paste0(Path,"Descriptive_Plots/PlantHeight_Distribution_Over_Time.pdf"),width = 8, height = 2.25)
#   print(temp.plot)
# dev.off()

temp.plot <- GDD %>%
  dplyr::select(Plot, Year, starts_with("X")) %>%
  pivot_longer(cols = 3:31, names_to = "GDD", values_to = "Height") %>%
  separate(GDD, into = c(NA,"GDD"), sep = "X") %>%
  ggplot(aes(x = as.numeric(GDD), y = Height)) +
  geom_jitter(aes(color = as.character(Year)), alpha = 0.2, show.legend = F) +
  geom_boxplot(aes(group = GDD), alpha = 0.2) +
  theme_classic() +
  xlim(300,1350) +
  xlab("Growing Degree Days") +
  scale_fill_discrete(breaks = c(350,400,450,500,550,600,650,700,750,800,850,900,950,1000,1050,1100,1150,1200,1250,1300,1350)) +
  scale_color_manual(values = c("#00C8AF","#AF00C8","#C8AF00")) +
  facet_wrap(~Year, nrow = 3)+ #, strip.position = "left") +
  theme(
  strip.background = element_blank(),
  strip.text.x = element_blank(),
  axis.title.y = element_blank(),
  #axis.text.y = element_blank(),
  #axis.ticks.y = element_blank()
)
# export
# pdf(paste0(Path,"Descriptive_Plots/PlantHeight_Distribution_Over_Time.pdf"),width = 15, height = 5)
pdf(paste0(Path,"Descriptive_Plots/PlantHeight_Distribution_Over_Time.pdf"),width = 2.25, height = 4)
  print(temp.plot)
dev.off()

# plot growth rate distribution over each year
temp.plot <- Int %>%
  dplyr::select(Plot, Year, starts_with("X")) %>%
  pivot_longer(cols = 3:30, names_to = "GDD", values_to = "Height") %>%
  separate(GDD, into = c("INT_int","INT_fin"), sep = "_") %>%
  separate(INT_int, into = c(NA,"INT_int"), sep = "X") %>%
  separate(INT_fin, into = c(NA, "INT_fin"), sep = "X") %>%
  unite(GDD, INT_int, INT_fin) %>%
  drop_na(Height) %>%
  separate(GDD, into = c("GDD",NA), sep = "_") %>%
  ggplot(aes(x = as.numeric(GDD), y = Height)) +
  geom_jitter(aes(color = as.character(Year)),alpha = 0.2, show.legend = F) +
  geom_boxplot(aes(group = GDD), alpha = 0.2) +
  geom_hline(yintercept = 0, color = 'red') +
  theme_classic() +
  xlim(300,1350) +
  xlab("Growing Degree Day Intervals") +
  ylab("Growth Rate") +
  scale_fill_discrete(breaks = c(350,400,450,500,550,600,650,700,750,800,850,900,950,1000,1050,1100,1150,1200,1250,1300,1350)) +
  scale_color_manual(values = c("#00C8AF","#AF00C8","#C8AF00")) +
  facet_wrap(~Year, nrow = 3) +
  theme(
  strip.background = element_blank(),
  strip.text.x = element_blank(),
  axis.title.y = element_blank()
)
#export
# pdf(paste0(Path,"Descriptive_Plots/GrowthRate_Distribution_Over_Time.pdf"),width = 15, height = 5)
pdf(paste0(Path,"Descriptive_Plots/GrowthRate_Distribution_Over_Time.pdf"),width = 2.25, height = 4)
  print(temp.plot)
dev.off()

# # plot growth rate distribution over each year
# temp.plot <- Int %>%
#   dplyr::select(Plot, Year, starts_with("X")) %>%
#   pivot_longer(cols = 3:30, names_to = "GDD", values_to = "Height") %>%
#   separate(GDD, into = c("INT_int","INT_fin"), sep = "_") %>%
#   separate(INT_int, into = c(NA,"INT_int"), sep = "X") %>%
#   separate(INT_fin, into = c(NA, "INT_fin"), sep = "X") %>%
#   unite(GDD, INT_int, INT_fin) %>%
#   drop_na(Height) %>%
#   separate(GDD, into = c("GDD",NA), sep = "_") %>%
#   ggplot(aes(x = as.numeric(GDD), y = Height)) +
#   geom_jitter(aes(color = as.character(Year)),alpha = 0.2, show.legend = F) +
#   geom_boxplot(aes(group = GDD), alpha = 0.2) +
#   geom_hline(yintercept = 0, color = 'red') +
#   theme_classic() +
#   xlim(300,1350) +
#   xlab("Growing Degree Day Intervals") +
#   ylab("Growth Rate") +
#   scale_fill_discrete(breaks = c(350,400,450,500,550,600,650,700,750,800,850,900,950,1000,1050,1100,1150,1200,1250,1300,1350)) +
#   scale_color_manual(values = c("#00C8AF","#AF00C8","#C8AF00")) +
#   facet_wrap(~Year)
# #export
# # pdf(paste0(Path,"Descriptive_Plots/GrowthRate_Distribution_Over_Time.pdf"),width = 15, height = 5)
# pdf(paste0(Path,"Descriptive_Plots/GrowthRate_Distribution_Over_Time.pdf"),width = 8, height = 2.25)
#   print(temp.plot)
# dev.off()

# plot yield distribution over each year
temp.plot <- GDD %>%
  dplyr::select(Plot,Year,Mean.Yld.bu.ac) %>%
  ggplot(aes(x = as.character(Year), y = Mean.Yld.bu.ac, fill = as.character(Year), group = Year)) +
  #geom_violin(aes(color = as.character(Year)),alpha = 0.5) +
  geom_violin(alpha = 0.5) +
  geom_jitter(aes(color = as.character(Year)),alpha = 0.2) +
  theme_classic() +
  xlab("Year") +
  ylab("Mean Yield (bu/acre)") +
  theme(
    legend.position = "none",
    axis.title.x = element_blank()
  ) +
  scale_fill_manual(values = c("#00C8AF","#AF00C8","#C8AF00")) +
  scale_color_manual(values = c("#00C8AF","#AF00C8","#C8AF00")) 
# export
pdf(paste0(Path,"Descriptive_Plots/Yield_Distribution_Over_Time.pdf"),width = 8, height = 2)
  print(temp.plot)
dev.off()

# set color palette for yield heatmaps
my.palette <- colorRampPalette(brewer.pal(9,"YlGn"))

# Plot heatmaps of the yield distribution
pdf(paste0(Path,"Descriptive_Plots/Yield_Heatmap.pdf"), height = 2.5, width = 2.29)
GDD %>%
  filter(Year == "2020") %>% 
  ggplot(aes(x = Range, y = Row, fill = as.numeric(Mean.Yld.bu.ac))) +
  geom_tile() +
  scale_x_reverse() +
    scale_y_reverse() +
  theme_classic()  +
  scale_fill_gradientn(colours = my.palette(100), limits = c(75,275)) +
  theme(
    legend.position = "none",
    #legend.position = "right",
    axis.title = element_blank()
  ) #+
  #labs(fill = "Mean Yield\n(bu/acre)")
  
GDD %>%
  filter(Year == "2021") %>% 
  ggplot(aes(x = Range, y = Row, fill = Mean.Yld.bu.ac)) +
  geom_tile() +
  scale_x_reverse() +
    scale_y_reverse() +
  theme_classic()  +
  scale_fill_gradientn(colours = my.palette(100), limits = c(75,275)) +
  theme(
    legend.position = "none",
    axis.title = element_blank()
  )


GDD %>%
  filter(Year == "2022") %>% 
  ggplot(aes(x = Range, y = Row, fill = Mean.Yld.bu.ac)) +
  geom_tile() +
  scale_x_reverse() + 
    scale_y_reverse() +
  theme_classic()  +
  scale_fill_gradientn(colours = my.palette(100), limits = c(75,275)) +
  theme(
    legend.position = "none",
    axis.title = element_blank()
    )

dev.off()

```

## Regression Analysis

  - make model using all rates of growth and plant heights 
  - make model using all rates or all plant heights
  - use aic or bic

```{r}
library(tidyverse)
library(MuMIn)
library(sensemakr)

Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Years <- c(2020,2021,2022)

###############################
## AIC WITH WHOLE FIELD DATA ##
###############################
# read in the files
GDD <- read.delim(paste0(Path, "All_Production/All_environment_GDD_Loess_Predictions.txt"))
Int <- read.delim(paste0(Path, "All_Production/All_environment_Interval_Loess_Predictions.txt"))
# list the data type files
data.types <- list(GDD,Int)
data.types.names <- c("GDD","Int")

# for loop to go through both static plant height and growth curves
for (a in 1:length(data.types)) {
  # assign data based on first for loop 
  data <- as.data.frame(data.types[a]) %>%
    dplyr::select(-Mean.Yld.lb.ac)
  # make Plot and Year rownames for data
  rownames(data) <- paste0(data$Plot,"_",data$Year)
  # select correct yield
  data.test <- data %>%
    select_if(~ !any(is.na(.))) %>%
    dplyr::select(Mean.Yld.bu.ac,Year, starts_with("X")) %>%
    na.omit()
  # test all of the years together (remove any columns with NA values)
  data.test.yield <- data.test %>%
    dplyr::select_if(~ !any(is.na(.))) %>%
    mutate(Year = factor(Year)) %>%
    # dplyr::select(-Year) %>%
    na.omit()
  # remove actual yield
  data.test.yield.1 <- data.test.yield %>%
    dplyr::select(-all_of("Mean.Yld.bu.ac")) #%>%
   # mutate(Year = as.numeric(Year))

  ## CALCULATE PARTIAL R2 ON A MULTIVARIATE REGRESSION SEPARATELY FOR EACH YEAR ##
  # for loop to separate each year
  for (b in 1:length(Years)) {
    # identify year variable
    year <- Years[b]
    # identify the data with yield
    data.test.yield.year <- data.test.yield %>%
      filter(Year == year) %>% # filter to just year of interest
      dplyr::select(-Year) # remove year column
    # identify the data without yield
    data.test.noyield.year <- data.test.yield.1 %>%
      filter(Year == year) %>% # filter to just year of interest
      dplyr::select(-Year) # remove year column
    # create the model for partial R2 regression
    model.year <- lm(get("Mean.Yld.bu.ac", data.test.yield.year) ~.-1, data = data.test.noyield.year, na.action = "na.fail")
    # Partial R2 on a multivariate regression
    output <- broom::tidy(partial_r2(model.year)) %>%
      mutate(names = ifelse(names == "(Intercept)", "Year2020", names)) %>%
      rename(.,R2 = x)
    # save Partial R2 for each for use later
  assign(paste0(data.types.names[a],".",year,".output"),output)
  # export partial R2 by each GDD
  write.table(output, paste0(Path, "All_Production/",data.types.names[a],"_",year,"_PartialR2_Grid_AllYears_Mean.Yld.bu.ac.txt"), sep = "\t", row.names = F)
  }
  
## AIC TESTING ##
  # create the global model for testing
  model <- lm(get("Mean.Yld.bu.ac", data.test.yield) ~., data = data.test.yield.1, na.action = "na.fail")
  # use dredge to test all possible combinations of predictors for global model
  combinations <- MuMIn::dredge(model)
  # assign the combinations to a data frame to look at after all the for loops
  assign(paste0("combinations_",data.types.names[a],"_AllYears_Mean.Yld.bu.ac"), combinations)
  # assign the combination with the lowest AIC 
  assign(paste0(data.types.names[a],"_AllYears_Mean.Yld.bu.ac_lowestAIC"), coefTable(combinations)[1])
  # export the combinations for later use
  write.table(combinations,paste0(Path,"All_Production/AIC_Combinations_",data.types.names[a],"_AllYears_Mean.Yld.bu.ac.txt"))
} # end of data type for loop

## FIND AND COMBINE SELECTED MODELS FOR GDD AND INT ##
# calculate sum of weights for each variable
gdd.sw <- as.data.frame(sw(combinations_GDD_AllYears_Mean.Yld.bu.ac)) %>%
  rownames_to_column(var = "variable")
int.sw <- as.data.frame(sw(combinations_Int_AllYears_Mean.Yld.bu.ac)) %>%
  rownames_to_column(var = "variable")
# save lowest AIC combination as a data frame
gdd.lowest <- as.data.frame(GDD_AllYears_Mean.Yld.bu.ac_lowestAIC) %>%
  rownames_to_column(var = "variable") %>%
  mutate(pres.abs = "present")
int.lowest <- as.data.frame(Int_AllYears_Mean.Yld.bu.ac_lowestAIC) %>%
  rownames_to_column(var = "variable") %>%
  mutate(pres.abs = "present")
# combine sw with picked models
gdd.variables <- merge(gdd.sw,gdd.lowest,all.x = T) %>%
  mutate(data = "GDD") %>% 
  rename(weight = `sw(combinations_GDD_AllYears_Mean.Yld.bu.ac)`) %>%
  dplyr::select(variable, weight, data, pres.abs) %>%
  separate(variable, into = c(NA, "variable"), sep = "X", fill = "left") %>%
  mutate(pres.abs = replace_na(pres.abs,"absent"))
int.variables <- merge(int.sw,int.lowest,all.x = T) %>%
  mutate(data = "INT") %>%
  rename(weight = `sw(combinations_Int_AllYears_Mean.Yld.bu.ac)`) %>%
  dplyr::select(variable, weight, data,pres.abs) %>%
  separate(variable, into = c("first", NA), sep = "_") %>%
  separate(first, into = c(NA, "variable"), sep = "X", fill = "left") %>%
  mutate(pres.abs = replace_na(pres.abs,"absent"))
# combine all variables
all.variables <- rbind(gdd.variables, int.variables)
all.variables <- all.variables %>%
  mutate(pres.abs = ifelse(variable == "Year", "present", pres.abs))

## PLOTS ##
# Plot chosen models and variables' weights
model.plot <- ggplot(all.variables, aes(x = factor(variable,levels = c("Year","450","500","550","600","650","700","750","800","850","900","950","1000","1050","1100","1150","1200")), y = data)) +
  geom_point(aes(fill = weight, shape = pres.abs, color = data), size = 2, stroke = 1.25) + #
  theme_light() +
  scale_shape_manual(values = c(13,21), guide = "none") +
  scale_fill_gradient2(low = "white", mid = "#f9a111", high = "#f44709",limits = c(0.68,1), midpoint = 0.95) + #, midpoint = 0.65
  scale_color_manual(values = c("#d20069","#5d3fea"), labels = c("Height","Growth Rate")) +
  xlab("Variables") +
  ylab("Data Type") +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90)) +
  scale_y_discrete(labels = c("GDD" = "Height","INT" = "Growth Rate")) +
  labs(fill = "Weight", shape = "", color = "Data Type") +
  guides(color = guide_legend(order=1),fill=guide_colorbar(order=2)) # shape=guide_legend(order=2),

model.plot.1 <- ggplot(all.variables, aes(x = factor(variable,levels = c("Year","450","500","550","600","650","700","750","800","850","900","950","1000","1050","1100","1150","1200")), y = factor(data,levels = c("INT","GDD")))) +
  #geom_tile(aes(fill = weight, color = factor(pres.abs,c("absent","present")), size = factor(pres.abs,c("absent","present")))) + #, linewidth = 0.75) +
  geom_tile(aes(fill = weight)) +
  #geom_tile(aes(color = factor(pres.abs,c("absent","present")), size = factor(pres.abs,c("absent","present"))), alpha = 0) +
  geom_text(aes(label = round(weight,2)), size = 2) +
  theme_light() +
  xlab("Variables") +
  ylab("Data Type") +
  #scale_color_manual(values = c("red","#00000000")) +
  scale_y_discrete(labels = c( "GDD" = "Height","INT" = "Growth\nRate")) +
  scale_fill_gradient2(low = "white", mid = "#f9a111", high = "#f44709",limits = c(0.68,1), midpoint = 0.95) +
  scale_size_manual(values = c(0.75,0)) +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 90)
        )

pdf(paste0(Path,"All_Production/Descriptive_Plots/AIC_Presence_Absence.pdf"), height = 2, width = 4)
print(model.plot)
print(model.plot.1)
dev.off()

## PARTIAL R2 PLOTS ##
GDD.pr2 <- GDD.2020.output %>%
  rename(X2020 = R2) %>%
  full_join(GDD.2021.output, by = "names") %>%
  rename(X2021 = R2) %>%
  full_join(GDD.2022.output, by = "names") %>%
  rename(X2022 = R2) %>%
  pivot_longer(cols = 2:4, names_to = "Year", values_to = "R2") %>%
  separate(Year, into = c(NA,"Year"), sep = "X", fill = "left") %>%
  separate(names, into = c(NA,"term"), sep = "X", fill = "left") %>%
  mutate(data = "GDD")
INT.pr2 <- Int.2020.output %>%
  rename(X2020 = R2) %>%
  full_join(Int.2021.output, by = "names") %>%
  rename(X2021 = R2) %>%
  full_join(Int.2022.output, by = "names") %>%
  rename(X2022 = R2) %>%
  pivot_longer(cols = 2:4, names_to = "Year", values_to = "R2") %>%
  separate(Year, into = c(NA,"Year"), sep = "X", fill = "left") %>%
  separate(names, into = c("first",NA), sep = "_", fill = "right") %>%
  separate(first, into = c(NA, "term"), sep = "X", fill = "left") %>%
  mutate(data = "INT")
all.pr2 <- rbind(GDD.pr2, INT.pr2)

# Plot partial R2
pr2.plot <- all.pr2 %>%
  ggplot(aes(x = factor(as.numeric(term)), y = R2, color = Year, shape = data)) + #, group = data)) +
 # geom_bar(stat = 'identity', position = "dodge", aes(fill = data), width = 0.25, show.legend = F) + 
  geom_point(alpha = 0.75, size = 3,show.legend = F) + #, position =position_dodge(width = (0.25))
  theme_light() +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5),
    legend.position = "bottom"
  ) +
  scale_color_manual(values = c("#00C8AF","#AF00C8","#C8AF00"), labels = c("2020","2021","2022")) +
  scale_shape_manual(values = c(16,17),labels = c("Height", "Growth Rate"), name = "Data Type") +
  xlab("Variables") +
  ylab("Partial R^2") #+

pdf(paste0(Path,"All_Production/Descriptive_Plots/Both_PartialR2.pdf"), height = 2, width = 4)
print(pr2.plot)
dev.off()
```
## Weather Data Compilation

  - reads in "Weather_",Year[a],".csv"
    "Solar_Radiation_Data_",Year[a],".csv"
  - exports "All_Weather_Information_",Year[a],".csv"

```{r}
library(tidyverse)
# set values
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- c("2020", "2021", "2022")
Field <- c("1", "1", "1")

daily.weather.all <- as.data.frame(matrix(nrow = 0, ncol = 5))
colnames(daily.weather.all) <- c("Date","Precip","Max.Temp","Min.Temp","Solar.cal.cm.2")
gdd.acc.all <- as.data.frame(matrix(nrow = 0, ncol = 5))
colnames(gdd.acc.all) <- c("Date","Max.Deg.F","Min.Deg.F","GDD","cum.GDD")
# for loop to go through all years
for (a in 1:length(Year)) {
  # read in the all of the data files for the year (temperature, precipitation, and solar)
  precip <- read.csv(paste0(Path,Year[a],"/Waseca/Weather_",Year[a],".csv"))
    # edit the columns
    precip$newdate <- strptime(as.character(precip$Date), "%m/%d/%y")
    precip$Date <- format(precip$newdate, "%m%d%Y")
    precip$Precip <- precip$Precipitation..inches.
    precip$Max.Temp <- precip$Maximum.Temperature.degrees..F.
    precip$Min.Temp <- precip$Minimum.Temperature.degrees..F.
    precip[,c("Precipitation..inches.","newdate", "Maximum.Temperature.degrees..F.","Minimum.Temperature.degrees..F.","Snow..inches.", "Snow.Depth..inches.")] <- NULL  
  
  solar <- read.csv(paste0(Path,Year[a],"/Waseca/Solar_Radiation_Data_",Year[a],".csv"))
    # edit the columns
    solar$newdate <- strptime(as.character(solar$Date), "%m/%d/%y")
    solar$Date <- format(solar$newdate, "%m%d%Y")
    solar$Solar <- solar$Solar..cal.cm.2.
    solar[,c("Solar..cal.cm.2.","newdate")] <- NULL
    
  # merge the data
  daily.weather <- merge(precip,solar, by = "Date")
  
  write.csv(daily.weather,paste0(Path,Year[a],"/Waseca/All_Weather_Information_",Year[a],".csv"))
  
  daily.weather.all <- rbind(daily.weather.all,daily.weather)
  
  gdd.acc <- read.csv(paste0(Path,Year[a],"/Waseca/GDD_Accumulation_",Year[a],".csv"),row.names = 1)
  
  gdd.acc.all <- rbind(gdd.acc.all,gdd.acc)
}

pdf(paste0(Path, "All_Production/Descriptive_Plots/Precipitation_Accumulation_OverTime_EveryYear.pdf"), height = 3, width = 4)
daily.weather.all %>%
  mutate(Year = paste0("20",as.numeric(Date) %% 100)) %>%
  mutate(Date = mdy(Date)) %>%
  mutate(newDate = format(Date, format = "%m/%d")) %>%
  filter(newDate < "11/01" & newDate > "04/30") %>%
  group_by(Year) %>%
  mutate(Precip = replace_na(as.numeric(Precip),0),
    cum.Precip = cumsum(as.numeric(Precip, na.omit = T))) %>%
  ggplot(aes(x = newDate, y = as.numeric(cum.Precip))) +
  geom_line(aes(group = Year, color = Year)) +
  geom_point(aes(color = Year)) +
  # horizontal lines for harvest dates
  geom_hline(aes(yintercept = 27.26), color = "#00C8AF")+
  geom_text(aes(160,27.26,label = "Harvest", vjust = +1.1)) + #, color = "#00C8AF") + # Oct 31, 2020
  geom_hline(aes(yintercept = 13.89), color = "#AF00C8")+
  geom_text(aes(160,13.89,label = "Harvest", vjust = +1.1)) + #, color = "#AF00C8") + # Oct 10, 2021
  geom_hline(aes(yintercept = 20.23), color = "#C8AF00")+
  geom_text(aes(160,20.23,label = "Harvest", vjust = +1.5)) + #, color = "#C8AF00") + # Oct 14, 2022
  # vertical lines for last extracted heights
  geom_vline(aes(xintercept = "07/20"), linetype = "dotted",color = "#00C8AF") + # 2020
  geom_vline(aes(xintercept = "07/12"), linetype = "dotted",color = "#AF00C8") + # 2021
  geom_vline(aes(xintercept = "07/18"), linetype = "dotted",color = "#C8AF00") + # 2022
  # vertical lines for last manual heights
  geom_text(aes(65,22, label = "Last\nExtracted\nHeight")) +
  geom_vline(aes(xintercept = "07/28"), linetype = "F1",color = "#00C8AF") + # 2020
  geom_vline(aes(xintercept = "07/23"), linetype = "F1",color = "#AF00C8") + # 2021
  geom_vline(aes(xintercept = "07/25"), linetype = "F1",color = "#C8AF00") + # 2022
  geom_text(aes(100,22, label = "Last\nManual\nHeight")) +
  theme_classic() +
  theme(
    axis.ticks.x = element_blank(),
    legend.position = "none"
  ) +
  xlab("Date") +
  ylab("Accumulated Precipitation") +
  scale_color_manual(values = c("#00C8AF","#AF00C8","#C8AF00")) +
  scale_x_discrete(labels = c("May","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","", "November","","","","","","","","","","","",""))
dev.off()

pdf(paste0(Path, "All_Production/Descriptive_Plots/Solar_Accumulation_OverTime_EveryYear.pdf"), height = 3, width = 4)
daily.weather.all %>%
  mutate(Year = paste0("20",as.numeric(Date) %% 100)) %>%
  mutate(Date = mdy(Date)) %>%
  mutate(newDate = format(Date, format = "%m/%d")) %>%
  filter(newDate < "11/01" & newDate > "04/30") %>%
  group_by(Year) %>%
  mutate(Solar = replace_na(as.numeric(Solar),0),
    cum.Solar = cumsum(as.numeric(Solar, na.omit = T))) %>%
  ggplot(aes(x = newDate, y = as.numeric(cum.Solar))) +
  geom_line(aes(group = Year, color = Year)) +
  geom_point(aes(color = Year)) +
  # horizontal lines for harvest dates
  geom_hline(aes(yintercept = 74857), color = "#00C8AF")+
  geom_text(aes(160,74857,label = "Harvest", vjust = +1.1)) +#, color = "#00C8AF") + # Oct 31, 2020
  geom_hline(aes(yintercept = 67873.01), color = "#AF00C8")+
  # geom_text(aes(160,67873.01,label = "Harvest", vjust = +1.1), color = "#AF00C8") + # Oct 10, 2021
  geom_hline(aes(yintercept = 80506), color = "#C8AF00")+
  # geom_text(aes(160,80506,label = "Harvest", vjust = +1.5), color = "#C8AF00") + # Oct 14, 2022
  # vertical lines for last extracted heights
  geom_vline(aes(xintercept = "07/20"), linetype = "dotted",color = "#00C8AF") + # 2020
  geom_vline(aes(xintercept = "07/12"), linetype = "dotted",color = "#AF00C8") + # 2021
  geom_vline(aes(xintercept = "07/18"), linetype = "dotted",color = "#C8AF00") + # 2022
  # vertical lines for last manual heights
  geom_text(aes(65,19000, label = "Last\nExtracted\nHeight")) +
  geom_vline(aes(xintercept = "07/28"), linetype = "F1",color = "#00C8AF") + # 2020
  geom_vline(aes(xintercept = "07/23"), linetype = "F1",color = "#AF00C8") + # 2021
  geom_vline(aes(xintercept = "07/25"), linetype = "F1",color = "#C8AF00") + # 2022
  geom_text(aes(100,19000, label = "Last\nManual\nHeight")) +
  theme_classic() +
  theme(
    axis.ticks.x = element_blank(),
    legend.position = "none"
  ) +
  scale_y_continuous(breaks = c(0, 20000, 40000, 60000, 80000),labels = c("0","20", "40", "60", "80")) +
  xlab("Date") +
  ylab("Accumulated Solar (1000 cal/cm^2)") +
  scale_color_manual(values = c("#00C8AF","#AF00C8","#C8AF00")) +
  scale_x_discrete(labels = c("May","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","November","","","","","","","","","","","",""))
dev.off()

pdf(paste0(Path, "All_Production/Descriptive_Plots/Solar_OverTime_EveryYear.pdf"), height = 3, width = 4)
daily.weather.all %>%
  mutate(Year = paste0("20",as.numeric(Date) %% 100)) %>%
  mutate(Date = mdy(Date)) %>%
  mutate(newDate = format(Date, format = "%m/%d")) %>%
  filter(newDate < "11/01" & newDate > "04/30") %>%
  group_by(Year) %>%
  ggplot(aes(x = newDate, y = as.numeric(Solar), group = Year)) +
  # geom_line(aes(group = Year, color = Year)) +
  geom_point(aes(color = Year)) +
  geom_smooth(aes(color = Year, fill = Year)) +
  theme_classic() +
  theme(
    axis.ticks.x = element_blank(),
    legend.position = "none"
  ) +
  xlab("Date") +
  ylab("Daily Solar (cal/cm^2)") +
  scale_x_discrete(labels = c("May","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","", "November"))
dev.off()

# pdf(paste0(Path, "All_Production/Descriptive_Plots/GDD_Accumulation_OverTime_EveryYear.pdf"), height = 6, width = 10)
pdf(paste0(Path, "All_Production/Descriptive_Plots/GDD_Accumulation_OverTime_EveryYear.pdf"), height = 3, width = 4)
gdd.acc.all %>%
  mutate(Year = paste0("20",as.numeric(Date)%%100)) %>%
  filter(cum.GDD != 0) %>%
  mutate(Date = mdy(Date)) %>%
  mutate(newDate = format(Date, format = "%m/%d")) %>%
  filter(newDate < "11/01") %>%
  ggplot(aes(x = newDate, y = cum.GDD)) +
  geom_line(aes(group = Year, color = Year))+
  geom_point(aes(color = Year)) +
  # horizontal lines for harvest dates
  geom_hline(aes(yintercept = 2642.5), color = "#00C8AF")+
  #geom_text(aes(118,2642.5,label = "Harvest 2020", vjust = +1.1), color = "#5000F3") + # Oct 31, 2020
  geom_hline(aes(yintercept = 2902.5), color = "#AF00C8")+
  #geom_text(aes(118,2902.5,label = "Harvest 2021", vjust = +1.1), color = "#F35000") + # Oct 10, 2021
  geom_hline(aes(yintercept = 2709.5), color = "#C8AF00")+
  #geom_text(aes(118,2709.5,label = "Harvest 2022", vjust = +1.1), color = "#00F350") + # Oct 14, 2022
  geom_text(aes(160,2909.5,label = "Harvest", vjust = +1.1)) +
  # vertical lines for last extracted heights
  geom_vline(aes(xintercept = "07/20"), linetype = "dotted",color = "#00C8AF") + # 2020
  geom_vline(aes(xintercept = "07/12"), linetype = "dotted",color = "#AF00C8") + # 2021
  geom_vline(aes(xintercept = "07/18"), linetype = "dotted",color = "#C8AF00") + # 2022
  # vertical lines for last manual heights
  geom_text(aes(65,600, label = "Last\nExtracted\nHeight")) +
  geom_vline(aes(xintercept = "07/28"), linetype = "F1",color = "#00C8AF") + # 2020
  geom_vline(aes(xintercept = "07/23"), linetype = "F1",color = "#AF00C8") + # 2021
  geom_vline(aes(xintercept = "07/25"), linetype = "F1",color = "#C8AF00") + # 2022
  geom_text(aes(100,600, label = "Last\nManual\nHeight")) +
  theme_classic() +
  theme(
    axis.ticks.x = element_blank(),
    legend.position = "none"
  ) +
  scale_y_continuous(breaks = c(0, 1000, 2000, 3000),labels = c("0","10", "20", "30")) +
  xlab("Date") +
  ylab("GDD Accumulation (100 GDU)") +
  scale_color_manual(values = c("#00C8AF","#AF00C8","#C8AF00")) +
  scale_x_discrete(labels = c("May","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","", "November")) +
  guides(color = guide_legend(title = "Year"))
dev.off()

```
## Machine Learning using caret
### Input Files

  - brings in/ calculates:
    - plant height data
    - growth rate data
    - slope of exponential growth
    - times of beginning or end of exponential growth
    - FPCA to describe full growth curve
    - FPCA of vegetative indices
  - exports:
    - All_Data_Machine_Model_Input.csv
    - All_Data_Types_Models_RMSE_and_R.csv
              
```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))
library(tidyverse)
library(caret)
library(AppliedPredictiveModeling)
library(mlbench)
library(plsRglm) # for Partial Least Squares Regression
# library(elasticnet) # for Least Absolute Shrinkage and Selection Operator Regression
# library(nnet) # for Artificial Neural Network
library(randomForest)
library(ranger) # for Random Forest
library(e1071) # for Random Forest
# library(kernlab) # for Support Vector Machines
library(pls) # pls
library(fda) # calculate eigenvalues
library(ggfortify)

# set path
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"


#########################################
## FORMAT EACH INDIVIDUAL TYPE OF DATA ##
#########################################

## READ IN PLANT HEIGHT AND GROWTTH RATE TO MAKE THEM USABLE IN MACHINE LEARNING MODELS
# read in the files
GDD <- read.delim(paste0(Path,"All_Production/All_environment_GDD_Loess_Predictions.txt")) # plant height
INT <- read.delim(paste0(Path,"All_Production/All_environment_Interval_Loess_Predictions.txt")) # growth rate
# Make the files usable for future
GDD <- GDD %>% # plant height
  unite("Plot.Year", Plot,Year, remove = F) %>% # combine plot number and year
  select_if(function(x) any(!is.na(x))) %>% # remove columns with NAs
  dplyr::select(-c("Plot", "Range", "Row", "Mean.Yld.lb.ac")) # remove Plot, Range, Row, and Yield columns
INT <- INT %>% # growth rate
  unite("Plot.Year", Plot,Year, remove = F) %>% # combine plot number and year
  select_if(function(x) any(!is.na(x))) %>% # remove columns with NAs
  dplyr::select(-c("Plot", "Range", "Row", "Mean.Yld.lb.ac")) # remove Plot, Range, Row, and Yield columns

## CALCULATE ADDITIONAL DESCRIPTIVE INFORMATION FOR EACH PLOT
# calculate slope of exponential growth
Slope.Exponential <- GDD %>%
  pivot_longer(cols = 4:24, names_to = "GDD", values_to = "Height") %>% # pivot longer for all height values in one column
  separate(GDD, into = c(NA,"GDD"), sep = "X") %>% # turn GDD titles into numbers
  filter(Height > 50 & Height < 250) %>% # remove heights below 50 and above 250 (the limits of exponential growth period)
  group_by(Plot.Year) %>% # group by plot
  mutate(exponential.growth = (max(Height) - min(Height))/(max(as.numeric(GDD))-min(as.numeric(GDD)))) %>% # calculate the slope
  dplyr::select(c("Plot.Year","exponential.growth")) %>% # keep just the plot and slope of exponential growth
  unique() # filter to unique rows
# Find date of 'take off' and 'end' for exponential growth
expo.desc <- GDD %>%
  pivot_longer(cols = 4:24, names_to = "GDD", values_to = "Height") %>% # pivot plant height to its own column
  separate(GDD, into = c(NA,"GDD"), sep = "X") %>% # make GDD into numbers
  filter(Height > 50 & Height < 250) %>% # remove heights below 50 and above 250 (the limits of exponential growth period)
  group_by(Plot.Year) %>% # group by plot
  mutate(start.expo = min(as.numeric(GDD)), # find the GDD of the beginning of the exponential slope
         end.expo = max(as.numeric(GDD))) %>% # find the GDD of the end of the exponential slope
  dplyr::select(c("Plot.Year","start.expo", "end.expo")) %>% # keep only plot, start, and end of exponential slope
  unique() # filter to unique rows

## CALCULATE EIGENVALUES TO DESCRIBE LOESS CURVES 
# initiate file for eigenvalues
all.eigen <- as.data.frame(matrix(nrow = 0, ncol = 3))
colnames(all.eigen) <- c("Plot.Year","eigen.1","eigen.2")
# for loop to go through all years
for (a in 1:length(unique(GDD$Year))) { 
  year <- unique(GDD$Year)[a] # set year variable
  # create data frames for eigenvalue calculation
  data.sub <- GDD %>%
    filter(Year == year) %>% # filter to just the year of interest
    dplyr::select(-c(Mean.Yld.bu.ac,Year)) %>% # remove yield, year, and exponential growth
    dplyr::select(where(~ !any(is.na(.)))) # remove columns that are all NAs
  data.sub.l <- data.sub %>%
    pivot_longer(cols = 2:ncol(.), names_to = "GDD", values_to = "Height") %>% # Make GDD and Height their own column
    separate(GDD, into = c(NA,"GDD"),sep = "X") %>% # remove X from GDDs
    mutate(GDD = as.numeric(GDD)) # make GDD numeric
  # create necessary descriptive data outputs for calculating eigenvalues
  knots = c(seq(min(data.sub.l$GDD),max(data.sub.l$GDD),5)) # location of knots
  n_knots = length(knots) # number of knots
  n_order = 4 # order of basis functions: for cubic b-splines: order = 3 + 1
  n_basis = length(knots) + n_order - 2;
  basis = create.bspline.basis(rangeval = c(min(data.sub.l$GDD),max(data.sub.l$GDD)), n_basis)
  argvals <- matrix(data.sub.l$GDD, nrow = (ncol(data.sub)-1), ncol = nrow(data.sub))
  y_mat <- matrix(data.sub.l$Height, nrow = (ncol(data.sub)-1), ncol = nrow(data.sub))
  # put data frame in format understood by pca.fd function
  W.obj <- Data2fd(argvals = argvals, y = y_mat, basisobj = basis, lambda = 0.5)
  # calculate the eigenvalues
  fun_pca <- pca.fd(W.obj, nharm = 5) # nharm = 5 indicates the calculation of 5 eigenvalues
  # plot the eigenvectors
  plot(fun_pca$harmonics, lwd = 3)
  # obtain the eigenvalues
  fun_pca$values
  # proportion explained by each eigenvalue
  fun_pca$varprop
  # combine the top two functional principle components with the original plot data
  temp <- cbind(data.sub[1],fun_pca$scores[,1:2])
  # combine the chosen functional principle components with the original plot data
  all.eigen <- rbind(all.eigen, temp)
}
# rename eigenvalue columns 
all.eigen <- all.eigen %>%
  rename(eigen.1 = `1`,
         eigen.2 = `2`)

## READ IN VEGETATIVE INDICE FUNCTIONAL PRINCIPAL COMPONENT VALUES (Calculated in: Calculate FPCA for VI Comparison over years)
# read in vegetative indices
vi <- read.csv(paste0(Path,"All_Production/Vegetative_Indices/Data_Analysis/All_Vegetative_Indices_FPCA.csv"),row.names = 1) %>%
  pivot_longer(cols = 4:5, names_to = "eigen",values_to = "value") %>% # pivot longer so eigen titles are in the same column
  tidyr::unite(titles,veg.ind,eigen) %>% # combine the vegetative indice and the eigen version
  pivot_wider(names_from = "titles",values_from = "value") %>% # pivot wider (column for each vegetative indice and eigenvalue)
  unite(Plot.Year,Plot,Year) %>% # combine plot and year
  data.frame() # make it a data frame


#############################################
## MANIPULATE THE DATA TO PUT ALL TOGETHER ##
#############################################
## Put data types together into all.data
all.data <- merge(GDD, INT[,c(1,4:23)], by = "Plot.Year") # combine GDD & INT
all.data <- merge(all.data, Slope.Exponential, by = "Plot.Year") # add slope of exponential growth 
all.data <- merge(all.data, expo.desc, by = "Plot.Year") # add exponential start and end
all.data <- merge(all.data, all.eigen, by = "Plot.Year") # add growth curve eigen values
all.data <- merge(all.data, vi, by = "Plot.Year") # add vegetative indice growth curves
# remove columns containing nas
all.data <- all.data %>%
  dplyr::select(where(~ !any(is.na(.))))

## EXPORT ALL MACHINE MODEL DATA
# export all.data.combined for use later
write.csv(all.data, paste0(Path, "All_Production/Machine_Models/All_Data_Machine_Model_Input.csv"))

## MAKE A FILE TO STORE ALL RMSE AND R VALUES FOR ALL ITERATIONS 
stored.data <- as.data.frame(matrix(ncol = 5, nrow = 0))
colnames(stored.data) <- c("Data", "Model", "Year", "RMSE","R")
# export stored.data for column names in .csv
write.table(stored.data,file=paste0(Path,"All_Production/Machine_Models/All_Data_Types_Models_RMSE_and_R.csv"),sep = ",")
# sample csv name
csv_fname = paste0(Path,"All_Production/Machine_Models/All_Data_Types_Models_RMSE_and_R.csv")
```
### Machine Learning - Pick Models

  - reads in All_Data_Types_Models_RMSE_and_R.csv
  - tests models:
    - lasso
    - pcaNNet
    - pls
    - plsRglm
    - rf
    - svmRadial
    - svmPoly
  - adds to All_Data_Types_Models_RMSE_and_R.csv

```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))
library(tidyverse)
library(caret)
library(AppliedPredictiveModeling)
library(mlbench)
library(plsRglm) # for Partial Least Squares Regression
library(elasticnet) # for Least Absolute Shrinkage and Selection Operator Regression
library(nnet) # for Artificial Neural Network
library(randomForest)
library(ranger) # for Random Forest
library(e1071) # for Random Forest
library(kernlab) # for Support Vector Machines
library(pls) # pls
library(fda) # calculate eigenvalues
library(ggfortify)

# set path
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
csv_fname = paste0(Path,"All_Production/Machine_Models/All_Data_Types_Models_RMSE_and_R.csv") # sample csv name
# read in all data
all.data <- read.csv(paste0(Path, "All_Production/Machine_Models/All_Data_Machine_Model_Input.csv"), row.names = 1)

###############################
## TESTING DIFFERENT METHODS ## 'plsRglm','lasso','pcaNNet','svmLinear','svmPoly','svmRadial','pls','rf'
###############################
# for loop to go through each year as test
for (a in c("2020","2021","2022")) {
  # identify the train set
  train <- all.data %>%
    filter(Year != a)
  # identify the test set
  test <- all.data %>%
    filter(Year == a)
  
  ## RF - mtry
  rf.model = train(Mean.Yld.bu.ac ~ .,
                   data = train %>%
                     dplyr::select(c(2,4:ncol(train))),
                   method = 'rf',
                   metric = "RMSE",
                   trControl = trainControl(## 10-fold CV
                     method = "none",
                     index = groupKFold(train$Year, k = 2)))  
  #rf.train.RMSE <- min(rf.model$resample[1])
  # predict on test set using model
  rf.preds <- predict(rf.model,test) # will automatically take columns specified in train
  # test using cor
  rf.test.train.cor <- cor(rf.preds, test$Mean.Yld.bu.ac)
  pdf(paste0(Path, "All_Production/Machine_Models/Correlation_PredictedvsActual_ModelPick_rf_",a,".pdf"))
  plot(rf.preds, test$Mean.Yld.bu.ac,
       xlab = "Predicted Yield",
       ylab = "Actual Yield",
       col = test$Year,
       main = paste0("rf model R^2:",round(rf.test.train.cor,2)^2)) +
    #abline(lm(test$Mean.Yld.bu.ac ~ rf.preds), col = "red")
  dev.off()
  # get RMSE between test predictions and real
  rf.test.train.RMSE <- RMSE(rf.preds,test$Mean.Yld.bu.ac)
  # tells the overall variable importance in the chosen model
  rf.importance <- varImp(rf.model, scale = T)
  rf.importance <- as.data.frame(rf.importance$importance)
  rf.importance$predictors <- rownames(rf.importance)
  rf.importance$model <- "rf"
  # export the variable importance
  write.csv(rf.importance, paste0(Path, "All_Production/Machine_Models/Variable_Importance_ModelPick_rf_",a,".csv"), row.names = F)
  # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
  row <- data.frame('ModelPick', 'rf', a, rf.test.train.RMSE, rf.test.train.cor)
  # writing row in the csv file
  write.table(row, file = csv_fname, sep = ",",
              append = TRUE, quote = FALSE,
              col.names = FALSE, row.names = FALSE)
  
  ## svmRadial - sigma, C
  svmRadial.model = train(Mean.Yld.bu.ac ~ .,
                          data = train %>%
                            dplyr::select(c(2,4:ncol(train))),
                          method = 'svmRadial',
                          metric = "RMSE",
                          trControl = trainControl(## 10-fold CV
                            method = "none",
                            index = groupKFold(train$Year, k = 2))) 
  
  #svmRadial.train.RMSE <- min(svmRadial.model$resample[1])
  # predict on test set using model
  svmRadial.preds <- predict(svmRadial.model, test) # will take the columns specified in train
  # test using cor
  svmRadial.test.train.cor <- cor(svmRadial.preds, test$Mean.Yld.bu.ac)
  # plot correlation between test predictions and test actual yield
  pdf(paste0(Path, "All_Production/Machine_Models/Correlation_PredictedvsActual_ModelPick_svmRadial_",a,".pdf"))
  plot(svmRadial.preds, test$Mean.Yld.bu.ac,
       xlab = "Predicted Yield",
       ylab = "Actual Yield",
       col = test$Year,
       main = paste0("svmRadial model R^2:",round(svmRadial.test.train.cor,2)^2)) +
    #abline(lm(test$Mean.Yld.bu.ac ~ svmRadial.preds), col = "red")
  dev.off()
  # get RMSE between test predictions and real
  svmRadial.test.train.RMSE <- RMSE(svmRadial.preds,test$Mean.Yld.bu.ac)
  # tells the overall variable importance in the chosen model
  svmRadial.importance <- varImp(svmRadial.model, scale = T)
  svmRadial.importance <- as.data.frame(svmRadial.importance$importance)
  svmRadial.importance$predictors <- rownames(svmRadial.importance)
  svmRadial.importance$model <- "svmRadial"
  # export the variable importance
  write.csv(svmRadial.importance, paste0(Path, "All_Production/Machine_Models/Variable_Importance_ModelPick_svmRadial_",a,".csv"), row.names = F)
  # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
  row <- data.frame('ModelPick', 'svmRadial', a, svmRadial.test.train.RMSE, svmRadial.test.train.cor)
  # writing row in the csv file
  write.table(row, file = csv_fname, sep = ",",
              append = TRUE, quote = FALSE,
              col.names = FALSE, row.names = FALSE)
  
  ## pls - ncomp
  pls.model = train(Mean.Yld.bu.ac ~ .,
                    data = train %>%
                      dplyr::select(c(2,4:ncol(train))),
                    method = 'pls',
                    metric = "RMSE",
                    trControl = trainControl(## 10-fold CV
                      method = "none",
                      index = groupKFold(train$Year, k = 2))) 
  #pls.train.RMSE <- min(pls.model$resample[1])
  # predict on test set using model
  pls.preds <- predict(pls.model, test) # will take the columns specified in train
  # test using cor
  pls.test.train.cor <- cor(pls.preds, test$Mean.Yld.bu.ac)
  # plot correlation between test predictions and test actual yield
  pdf(paste0(Path, "All_Production/Machine_Models/Correlation_PredictedvsActual_ModelPick_pls_",a,".pdf"))
  plot(pls.preds, test$Mean.Yld.bu.ac,
       xlab = "Predicted Yield",
       ylab = "Actual Yield",
       col = test$Year,
       main = paste0("pls model R^2:",round(pls.test.train.cor,2)^2)) +
    #abline(lm(test$Mean.Yld.bu.ac ~ pls.preds), col = "red")
  dev.off()
  # get RMSE between test predictions and real
  pls.test.train.RMSE <- RMSE(pls.preds,test$Mean.Yld.bu.ac)
  # tells the overall variable importance in the chosen model
  pls.importance <- varImp(pls.model, scale = T)
  pls.importance <- as.data.frame(pls.importance$importance)
  pls.importance$predictors <- rownames(pls.importance)
  pls.importance$model <- "pls"
  # export the variable importance
  write.csv(pls.importance, paste0(Path, "All_Production/Machine_Models/Variable_Importance_ModelPick_pls_",a,".csv"), row.names = F)
  # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
  row <- data.frame('ModelPick', 'pls', a, pls.test.train.RMSE, pls.test.train.cor)
  # writing row in the csv file
  write.table(row, file = csv_fname, sep = ",",
              append = TRUE, quote = FALSE,
              col.names = FALSE, row.names = FALSE)
  
  ## PCANNET - size, decay
  pcaNNet.model = train(Mean.Yld.bu.ac ~ .,
                        data = train %>%
                          dplyr::select(c(2,4:ncol(train))),
                        method = 'pcaNNet',
                        metric = "Rsquared",
                        trControl = trainControl(## 10-fold CV
                          method = "none",
                          index = groupKFold(train$Year, k = 2))) 
  
  #pcaNNet.train.RMSE <- min(pcaNNet.model$resample[1])
  # predict on test set using model
  pcaNNet.preds <- predict(pcaNNet.model, test) # will take the columns specified in train
  # test using cor
  pcaNNet.test.train.cor <- cor(pcaNNet.preds, test$Mean.Yld.bu.ac)
  # plot correlation between test predictions and test actual yield
  pdf(paste0(Path, "All_Production/Machine_Models/Correlation_PredictedvsActual_ModelPick_pcaNNet_",a,".pdf"))
  plot(pcaNNet.preds, test$Mean.Yld.bu.ac,
       xlab = "Predicted Yield",
       ylab = "Actual Yield",
       col = test$Year,
       main = paste0("pcaNNet model R^2:",round(pcaNNet.test.train.cor,2)^2)) +
    #abline(lm(test$Mean.Yld.bu.ac ~ pcaNNet.preds), col = "red")
  dev.off()
  # get RMSE between test predictions and real
  pcaNNet.test.train.RMSE <- RMSE(pcaNNet.preds,test$Mean.Yld.bu.ac)
  # tells the overall variable importance in the chosen model
  pcaNNet.importance <- varImp(pcaNNet.model, scale = T)
  pcaNNet.importance <- as.data.frame(pcaNNet.importance$importance)
  pcaNNet.importance$predictors <- rownames(pcaNNet.importance)
  pcaNNet.importance$model <- "pcaNNet"
  # export the variable importance
  write.csv(pcaNNet.importance, paste0(Path,"All_Production/Machine_Models/Variable_Importance_ModelPick_pcaNNet_",a,".csv"),row.names=F)
  # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
  row <- data.frame('ModelPick', 'pcaNNet', a, pcaNNet.test.train.RMSE, pcaNNet.test.train.cor)
  # writing row in the csv file
  write.table(row, file = csv_fname, sep = ",",
              append = TRUE, quote = FALSE,
              col.names = FALSE, row.names = FALSE)
  
  ## LASSO - fraction
  lasso.model = train(Mean.Yld.bu.ac ~ .,
                      data = train %>%
                        dplyr::select(c(2,4:ncol(train))),
                      method = 'lasso',
                      metric = "RMSE",
                      trControl = trainControl(## 10-fold CV
                        method = "none",
                        index = groupKFold(train$Year, k = 2))) 
  
  #lasso.train.RMSE <- min(lasso.model$resample[1])
  # predict on test set using model
  lasso.preds <- predict(lasso.model, test) # will take the columns specified in train
  # test using cor
  lasso.test.train.cor <- cor(lasso.preds, test$Mean.Yld.bu.ac)
  # plot correlation between test predictions and test actual yield
  pdf(paste0(Path, "All_Production/Machine_Models/Correlation_PredictedvsActual_ModelPick_lasso_",a,".pdf"))
  plot(lasso.preds, test$Mean.Yld.bu.ac,
       xlab = "Predicted Yield",
       ylab = "Actual Yield",
       col = test$Year,
       main = paste0("LASSO model R^2:",round(lasso.test.train.cor,2)^2)) +
    #abline(lm(test$Mean.Yld.bu.ac ~ lasso.preds), col = "red")
  dev.off()
  # get RMSE between test predictions and real
  lasso.test.train.RMSE <- RMSE(lasso.preds,test$Mean.Yld.bu.ac)
  # tells the overall variable importance in the chosen model
  lasso.importance <- varImp(lasso.model, scale = T)
  lasso.importance <- as.data.frame(lasso.importance$importance)
  lasso.importance$predictors <- rownames(lasso.importance)
  lasso.importance$model <- "lasso"
  # export the variable importance
  write.csv(lasso.importance, paste0(Path, "All_Production/Machine_Models/Variable_Importance_ModelPick_lasso_",a,".csv"), row.names = F)
  # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
  row <- data.frame('ModelPick', 'lasso', a, lasso.test.train.RMSE, lasso.test.train.cor)
  # writing row in the csv file
  write.table(row, file = csv_fname, sep = ",",
              append = TRUE, quote = FALSE,
              col.names = FALSE, row.names = FALSE)
  
  ## PLSRGLM - nt, alpha.pvals.expli
  plsRglm.model = train(Mean.Yld.bu.ac ~ .,
                        data = train %>%
                          dplyr::select(c(2,4:ncol(train))),
                        method = 'plsRglm',
                        metric = "RMSE",
                        tuneGrid = expand.grid(nt = 2, alpha.pvals.expli = 0.001),
                        trControl = trainControl(## 10-fold CV
                          method = "none",
                          index = groupKFold(train$Year, k = 2))) 
  
  plsRglm.train.RMSE <- min(plsRglm.model$resample[1])
  # predict on test set using model
  plsRglm.preds <- predict(plsRglm.model, test) # will take the columns specified in train
  # test using cor
  plsRglm.test.train.cor <- cor(plsRglm.preds, test$Mean.Yld.bu.ac)
  # plot correlation between test predictions and test actual yield
  pdf(paste0(Path, "All_Production/Machine_Models/Correlation_PredictedvsActual_ModelPick_plsRglm_",a,".pdf"))
  plot(plsRglm.preds, test$Mean.Yld.bu.ac,
       xlab = "Predicted Yield",
       ylab = "Actual Yield",
       col = test$Year,
       main = paste0("plsRglm model R^2:",round(plsRglm.test.train.cor,2)^2)) +
    #abline(lm(test$Mean.Yld.bu.ac ~ plsRglm.preds), col = "red")
  dev.off()
  # get RMSE between test predictions and real
  plsRglm.test.train.RMSE <- RMSE(plsRglm.preds,test$Mean.Yld.bu.ac)
  # tells the overall variable importance in the chosen model
  plsRglm.importance <- varImp(plsRglm.model, scale = T)
  plsRglm.importance <- as.data.frame(plsRglm.importance$importance)
  plsRglm.importance$predictors <- rownames(plsRglm.importance)
  plsRglm.importance$model <- "plsRglm"
  # export the variable importance
  write.csv(plsRglm.importance, paste0(Path,"All_Production/Machine_Models/Variable_Importance_ModelPick_plsRglm_",a,".csv"),row.names=F)
  # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
  row <- data.frame('ModelPick', 'plsRglm', a, plsRglm.test.train.RMSE, plsRglm.test.train.cor)
  # writing row in the csv file
  write.table(row, file = csv_fname, sep = ",",
              append = TRUE, quote = FALSE,
              col.names = FALSE, row.names = FALSE)

  
  ##############################
  ## EXPORT DESCRIPTIVE PLOTS ##
  ##############################
  ## ALL RMSE ##
  # combine all RMSE
  All.RMSE <- data.frame(
    Model = c("PLSGLM","LASSO", "ANN", "SVM (Radial)","PLS", "RF (rf)"),
    RMSE = c(plsRglm.test.train.RMSE,lasso.test.train.RMSE,pcaNNet.test.train.RMSE,svmRadial.test.train.RMSE,pls.test.train.RMSE,rf.test.train.RMSE))
  # Plot and Export ALL RMSE
  pdf(paste0(Path, "All_Production/Machine_Models/RMSE_PredictedvsActual_ModelPick_",a,".pdf"))
    temp <- All.RMSE %>%
      # filter(Model != "ANN") %>%
      ggplot(aes(x = Model, y = RMSE)) +
        geom_histogram(stat = "identity") +
        theme(axis.text.x = element_text(angle = 90))
    print(temp)
  dev.off()

  ## ALL CORRELATION ##
  # combine all correlations
  All.cor <- data.frame(
    Model = c("PLSGLM","LASSO", "ANN", "SVM (Radial)","PLS", "RF (rf)"),
    Correlation = c(plsRglm.test.train.cor*plsRglm.test.train.cor,lasso.test.train.cor*lasso.test.train.cor,pcaNNet.test.train.cor*pcaNNet.test.train.cor,svmRadial.test.train.cor*svmRadial.test.train.cor,pls.test.train.cor*pls.test.train.cor,rf.test.train.cor*rf.test.train.cor))
  # Plot and Export ALL CORRELATIONS
  pdf(paste0(Path, "All_Production/Machine_Models/Correlation_PredictedvsActual_ModelPick_",a,".pdf"))
    temp <- All.cor %>%
      ggplot(aes(x = Model, y = Correlation)) +
        geom_histogram(stat = "identity") +
        theme(axis.text.x = element_text(angle = 90)) +
        ylab("Correlation (R^2)")
    print(temp)
  dev.off()

  ## ALL VARIABLE IMPORTANCE ##
  # combine all variable importance
  All.var <- rbind(plsRglm.importance,lasso.importance,pcaNNet.importance,svmRadial.importance,pls.importance,rf.importance)

    # Plot and Export ALL VARIABLE IMPORTANCES
    pdf(paste0(Path, "All_Production/Machine_Models/Variable_Importance_ModelPick_",a,".pdf"))

    temp <- ggplot(All.var, aes(x = factor(predictors,levels = unique(predictors)), y = Overall, fill = model)) +
      geom_bar(stat = "identity", position = "dodge") +
      theme(
        axis.text.x = element_text(angle = 90, hjust = 1)
      ) +
      xlab("Predictors in use by each Model") +
      ylab("Variable Importance Scaled from 100 to 0") +
      facet_wrap(~ model)

    print(temp)
    dev.off()
}
```
  - Moving forward with PLS, PLSRGLM, & RF
  
### Machine Learning - Plant Height, Growth Rates, Growth Curve Eigenvalues, and Vegetative Indices Eigenvalues

  - train on multiple hyperparameters
  - export RMSE and R^2 values
  - Calculate Cor and RMSE for every hyperparameter value

```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))
library(tidyverse)
library(caret)
library(AppliedPredictiveModeling)
library(mlbench)
library(plsRglm) # for Partial Least Squares Regression
library(elasticnet) # for Least Absolute Shrinkage and Selection Operator Regression
library(nnet) # for Artificial Neural Network
library(randomForest)
library(ranger) # for Random Forest
library(e1071) # for Random Forest
library(kernlab) # for Support Vector Machines
library(pls) # pls
library(ggfortify)

Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
csv_fname = paste0(Path,"All_Production/Machine_Models/All_Data_Types_Models_RMSE_and_R.csv") # sample csv name
# read in all data
all.data <- read.csv(paste0(Path, "All_Production/Machine_Models/All_Data_Machine_Model_Input.csv"), row.names = 1)


############################
## CHANGE HYPERPARAMETERS ## 
############################
# for loop to go through each year as test
for (a in c("2020","2021","2022")) {
  # identify the train set
  train <- all.data %>%
    filter(Year != a)
  # identify the test set
  test <- all.data %>%
    filter(Year == a)
  # set rf hyperparameters to go through
  param.rf <- c(2,3,4,5,6,7,8,9,10,11)
  # for loop to go through hyperparameters
  for(b in 1:length(param.rf)) {
    mtry.param <- param.rf[b]
    ## RF - mtry ##
    rf.model = train(Mean.Yld.bu.ac ~ .,
                     data = train %>% 
                       dplyr::select(c(2,4:ncol(train))),
                     method = 'rf',
                     metric = "RMSE",
                     tuneGrid = expand.grid(mtry = mtry.param), #mtry = c(20:30)),
                     trControl = trainControl(
                       method = "none",
                       index = groupKFold(train$Year, k = 2)))
    rf.train.RMSE <- min(rf.model$results["RMSE"])
    # predict on test set using model
    rf.preds <- predict(rf.model,test) # will automatically take columns specified in train
    # test using cor
    rf.test.train.cor <- cor(rf.preds, test$Mean.Yld.bu.ac)
    pdf(paste0(Path, "All_Production/Machine_Models/All_Data/Correlation_PredictedvsActual_ParameterPick_rf_",a,"_",mtry.param,".pdf"))
    plot(rf.preds, test$Mean.Yld.bu.ac,
         xlab = "Predicted Yield",
         ylab = "Actual Yield",
         col = test$Year,
         main = paste0("rf model R^2:",round(rf.test.train.cor,2)^2)) +
      abline(lm(test$Mean.Yld.bu.ac ~ rf.preds), col = "red")
    dev.off()
    # get RMSE between test predictions and real
    rf.test.train.RMSE <- RMSE(rf.preds,test$Mean.Yld.bu.ac)
    # tells the overall variable importance in the chosen model
    rf.importance <- varImp(rf.model, scale = T)
    rf.importance <- as.data.frame(rf.importance$importance)
    rf.importance$predictors <- rownames(rf.importance)
    rf.importance$model <- "rf"
    # export the variable importance
    write.csv(rf.importance, paste0(Path, "All_Production/Machine_Models/All_Data/Variable_Importance_ParameterPick_rf_",a,"_",mtry.param,".csv"), row.names = F)
    # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
    row <- data.frame('All_Data', 'rf', paste0(a,"_",mtry.param), rf.test.train.RMSE, rf.test.train.cor)
    # writing row in the csv file
    write.table(row, file = csv_fname, sep = ",",
                append = TRUE, quote = FALSE,
                col.names = FALSE, row.names = FALSE)
  }
  
  # set pls hyperparameters to go through
  param.ncomp <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21)
  for (c in 1:length(param.ncomp)) {
    ncomp.param <- param.ncomp[c]
    ## PLS MODEL - ncomp ##
    pls.model = train(Mean.Yld.bu.ac ~ .,
                      data = train %>% 
                        dplyr::select(c(2,4:ncol(train))),
                      method = 'pls',
                      metric = "RMSE",
                      tuneGrid = expand.grid(ncomp=ncomp.param),
                      trControl = trainControl(## 10-fold CV
                        method = "none",
                        index = groupKFold(train$Year, k = length(unique(train$Year)))))
    pls.train.RMSE <- min(pls.model$resample[1])
    # predict on test set using model
    pls.preds <- predict(pls.model, test) # will take the columns specified in train
    # test using cor
    pls.test.train.cor <- cor(pls.preds, test$Mean.Yld.bu.ac)
    # plot correlation between test predictions and test actual yield
    pdf(paste0(Path, "All_Production/Machine_Models/All_Data/Correlation_PredictedvsActual_ParameterPick_pls_",a,"_",ncomp.param,".pdf"))
    plot(pls.preds, test$Mean.Yld.bu.ac,
         xlab = "Predicted Yield",
         ylab = "Actual Yield",
         col = test$Year,
         main = paste0("pls model R^2:",round(pls.test.train.cor,2)^2)) +
      abline(lm(test$Mean.Yld.bu.ac ~ pls.preds), col = "red")
    dev.off()
    # get RMSE between test predictions and real
    pls.test.train.RMSE <- RMSE(pls.preds,test$Mean.Yld.bu.ac)
    # tells the overall variable importance in the chosen model
    pls.importance <- varImp(pls.model, scale = T)
    pls.importance <- as.data.frame(pls.importance$importance)
    pls.importance$predictors <- rownames(pls.importance)
    pls.importance$model <- "pls"
    # export the variable importance
    write.csv(pls.importance, paste0(Path, "All_Production/Machine_Models/All_Data/Variable_Importance_ParameterPick_pls_",a,"_",ncomp.param,".csv"), row.names = F)
    # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
    row <- data.frame('All_Data', 'pls', paste0(a,"_",ncomp.param), pls.test.train.RMSE, pls.test.train.cor)
    # writing row in the csv file
    write.table(row, file = csv_fname, sep = ",",
                append = TRUE, quote = FALSE,
                col.names = FALSE, row.names = FALSE)
  }
  # set plsRglm hyperparameters to go through
  param.alpha <- c(0.001,0.01,0.1,1)
  param.nt <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)
  # for loop to go through alpha.pvals.expli hyperparameter values
  for(d in 1:length(param.alpha)) { 
    alpha.param <- param.alpha[d]
    # for loop to go through nt hyperparameter values
    for(e in 1:length(param.nt)) {
      nt.param <- param.nt[e]
      ## PLSRGLM - nt, alpha.pvals.expli ##
      plsRglm.model = train(Mean.Yld.bu.ac ~ .,
                            data = train %>% 
                              dplyr::select(c(2,4:ncol(train))),
                            method = 'plsRglm',
                            metric = "RMSE",
                            tuneGrid = expand.grid(nt = nt.param, alpha.pvals.expli = alpha.param),
                            trControl = trainControl(
                              method = "none",
                              index = groupKFold(train$Year, k = 2)))
      plsRglm.train.RMSE <- min(plsRglm.model$resample[1])
      # predict on test set using model
      plsRglm.preds <- predict(plsRglm.model, test) # will take the columns specified in train
      # test using cor
      plsRglm.test.train.cor <- cor(plsRglm.preds, test$Mean.Yld.bu.ac)
      # plot correlation between test predictions and test actual yield
      pdf(paste0(Path, "All_Production/Machine_Models/All_Data/Correlation_PredictedvsActual_ParameterPick_plsRglm_",a,"_",alpha.param,".",nt.param,".pdf"))
      plot(plsRglm.preds, test$Mean.Yld.bu.ac,
           xlab = "Predicted Yield",
           ylab = "Actual Yield",
           col = test$Year,
           main = paste0("plsRglm model R^2:",round(plsRglm.test.train.cor,2)^2)) +
        abline(lm(test$Mean.Yld.bu.ac ~ plsRglm.preds), col = "red")
      dev.off()
      # get RMSE between test predictions and real
      plsRglm.test.train.RMSE <- RMSE(plsRglm.preds,test$Mean.Yld.bu.ac)
      # tells the overall variable importance in the chosen model
      plsRglm.importance <- varImp(plsRglm.model, scale = T)
      plsRglm.importance <- as.data.frame(plsRglm.importance$importance)
      plsRglm.importance$predictors <- rownames(plsRglm.importance)
      plsRglm.importance$model <- "plsRglm"
      # export the variable importance
      write.csv(plsRglm.importance, paste0(Path,"All_Production/Machine_Models/All_Data/Variable_Importance_ParameterPick_plsRglm_",a,"_",alpha.param,".",nt.param,".csv"),row.names=F)
      # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
      row <- data.frame('All_Data', 'plsRglm', paste0(a,"_",alpha.param,".",nt.param), plsRglm.test.train.RMSE, plsRglm.test.train.cor)
      # writing row in the csv file
      write.table(row, file = csv_fname, sep = ",",
                  append = TRUE, quote = FALSE,
                  col.names = FALSE, row.names = FALSE)
    }
  }
}

```
### Machine Learning - Vegetative Indices Eigenvalues

  - train on multiple hyperparameters
  - export RMSE and R^2 values
  - Calculate Cor and RMSE for every hyperparameter value

```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))
library(tidyverse)
library(caret)
library(AppliedPredictiveModeling)
library(mlbench)
library(plsRglm) # for Partial Least Squares Regression
library(elasticnet) # for Least Absolute Shrinkage and Selection Operator Regression
library(nnet) # for Artificial Neural Network
library(randomForest)
library(ranger) # for Random Forest
library(e1071) # for Random Forest
library(kernlab) # for Support Vector Machines
library(pls) # pls
library(ggfortify)

Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
csv_fname = paste0(Path,"All_Production/Machine_Models/All_Data_Types_Models_RMSE_and_R.csv") # sample csv name
# read in all data
all.data <- read.csv(paste0(Path, "All_Production/Machine_Models/All_Data_Machine_Model_Input.csv"), row.names = 1) %>%
  dplyr::select(-c(starts_with("X"),"exponential.growth","start.expo","end.expo","eigen.1","eigen.2")) # remove non-vi predictors

############################
## CHANGE HYPERPARAMETERS ## 
############################
# for loop to go through each year as test
for (a in c("2020","2021","2022")) {
  # identify the train set
  train <- all.data %>%
    filter(Year != a)
  # identify the test set
  test <- all.data %>%
    filter(Year == a)
  # set rf hyperparameters to go through
  param.rf <- c(2,3,4,5,6,7,8,9,10,11)
  # for loop to go through hyperparameters
  for(b in 1:length(param.rf)) {
    mtry.param <- param.rf[b]
    ## RF - mtry ##
    rf.model = train(Mean.Yld.bu.ac ~ .,
                     data = train %>% 
                       dplyr::select(c(2,4:ncol(train))),
                     method = 'rf',
                     metric = "RMSE",
                     tuneGrid = expand.grid(mtry = mtry.param), #mtry = c(20:30)),
                     trControl = trainControl(
                       method = "none",
                       index = groupKFold(train$Year, k = 2)))
    rf.train.RMSE <- min(rf.model$results["RMSE"])
    # predict on test set using model
    rf.preds <- predict(rf.model,test) # will automatically take columns specified in train
    # test using cor
    rf.test.train.cor <- cor(rf.preds, test$Mean.Yld.bu.ac)
    pdf(paste0(Path, "All_Production/Machine_Models/VI_Eigen/Correlation_PredictedvsActual_ParameterPick_rf_",a,"_",mtry.param,".pdf"))
    plot(rf.preds, test$Mean.Yld.bu.ac,
         xlab = "Predicted Yield",
         ylab = "Actual Yield",
         col = test$Year,
         main = paste0("rf model R^2:",round(rf.test.train.cor,2)^2)) +
      abline(lm(test$Mean.Yld.bu.ac ~ rf.preds), col = "red")
    dev.off()
    # get RMSE between test predictions and real
    rf.test.train.RMSE <- RMSE(rf.preds,test$Mean.Yld.bu.ac)
    # tells the overall variable importance in the chosen model
    rf.importance <- varImp(rf.model, scale = T)
    rf.importance <- as.data.frame(rf.importance$importance)
    rf.importance$predictors <- rownames(rf.importance)
    rf.importance$model <- "rf"
    # export the variable importance
    write.csv(rf.importance, paste0(Path, "All_Production/Machine_Models/VI_Eigen/Variable_Importance_ParameterPick_rf_",a,"_",mtry.param,".csv"), row.names = F)
    # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
    row <- data.frame('VI_Eigen', 'rf', paste0(a,"_",mtry.param), rf.test.train.RMSE, rf.test.train.cor)
    # writing row in the csv file
    write.table(row, file = csv_fname, sep = ",",
                append = TRUE, quote = FALSE,
                col.names = FALSE, row.names = FALSE)
  }
  
  # set pls hyperparameters to go through
  param.ncomp <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21)
  for (c in 1:length(param.ncomp)) {
    ncomp.param <- param.ncomp[c]
    ## PLS MODEL - ncomp ##
    pls.model = train(Mean.Yld.bu.ac ~ .,
                      data = train %>% 
                        dplyr::select(c(2,4:ncol(train))),
                      method = 'pls',
                      metric = "RMSE",
                      tuneGrid = expand.grid(ncomp=ncomp.param),
                      trControl = trainControl(## 10-fold CV
                        method = "none",
                        index = groupKFold(train$Year, k = length(unique(train$Year)))))
    pls.train.RMSE <- min(pls.model$resample[1])
    # predict on test set using model
    pls.preds <- predict(pls.model, test) # will take the columns specified in train
    # test using cor
    pls.test.train.cor <- cor(pls.preds, test$Mean.Yld.bu.ac)
    # plot correlation between test predictions and test actual yield
    pdf(paste0(Path, "All_Production/Machine_Models/VI_Eigen/Correlation_PredictedvsActual_ParameterPick_pls_",a,"_",ncomp.param,".pdf"))
    plot(pls.preds, test$Mean.Yld.bu.ac,
         xlab = "Predicted Yield",
         ylab = "Actual Yield",
         col = test$Year,
         main = paste0("pls model R^2:",round(pls.test.train.cor,2)^2)) +
      abline(lm(test$Mean.Yld.bu.ac ~ pls.preds), col = "red")
    dev.off()
    # get RMSE between test predictions and real
    pls.test.train.RMSE <- RMSE(pls.preds,test$Mean.Yld.bu.ac)
    # tells the overall variable importance in the chosen model
    pls.importance <- varImp(pls.model, scale = T)
    pls.importance <- as.data.frame(pls.importance$importance)
    pls.importance$predictors <- rownames(pls.importance)
    pls.importance$model <- "pls"
    # export the variable importance
    write.csv(pls.importance, paste0(Path, "All_Production/Machine_Models/VI_Eigen/Variable_Importance_ParameterPick_pls_",a,"_",ncomp.param,".csv"), row.names = F)
    # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
    row <- data.frame('VI_Eigen', 'pls', paste0(a,"_",ncomp.param), pls.test.train.RMSE, pls.test.train.cor)
    # writing row in the csv file
    write.table(row, file = csv_fname, sep = ",",
                append = TRUE, quote = FALSE,
                col.names = FALSE, row.names = FALSE)
  }
  # set plsRglm hyperparameters to go through
  param.alpha <- c(0.001,0.01,0.1,1)
  param.nt <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)
  # for loop to go through alpha.pvals.expli hyperparameter values
  for(d in 1:length(param.alpha)) { 
    alpha.param <- param.alpha[d]
    # for loop to go through nt hyperparameter values
    for(e in 1:length(param.nt)) {
      nt.param <- param.nt[e]
      ## PLSRGLM - nt, alpha.pvals.expli ##
      plsRglm.model = train(Mean.Yld.bu.ac ~ .,
                            data = train %>% 
                              dplyr::select(c(2,4:ncol(train))),
                            method = 'plsRglm',
                            metric = "RMSE",
                            tuneGrid = expand.grid(nt = nt.param, alpha.pvals.expli = alpha.param),
                            trControl = trainControl(
                              method = "none",
                              index = groupKFold(train$Year, k = 2)))
      plsRglm.train.RMSE <- min(plsRglm.model$resample[1])
      # predict on test set using model
      plsRglm.preds <- predict(plsRglm.model, test) # will take the columns specified in train
      # test using cor
      plsRglm.test.train.cor <- cor(plsRglm.preds, test$Mean.Yld.bu.ac)
      # plot correlation between test predictions and test actual yield
      pdf(paste0(Path, "All_Production/Machine_Models/VI_Eigen/Correlation_PredictedvsActual_ParameterPick_plsRglm_",a,"_",alpha.param,".",nt.param,".pdf"))
      plot(plsRglm.preds, test$Mean.Yld.bu.ac,
           xlab = "Predicted Yield",
           ylab = "Actual Yield",
           col = test$Year,
           main = paste0("plsRglm model R^2:",round(plsRglm.test.train.cor,2)^2)) +
        abline(lm(test$Mean.Yld.bu.ac ~ plsRglm.preds), col = "red")
      dev.off()
      # get RMSE between test predictions and real
      plsRglm.test.train.RMSE <- RMSE(plsRglm.preds,test$Mean.Yld.bu.ac)
      # tells the overall variable importance in the chosen model
      plsRglm.importance <- varImp(plsRglm.model, scale = T)
      plsRglm.importance <- as.data.frame(plsRglm.importance$importance)
      plsRglm.importance$predictors <- rownames(plsRglm.importance)
      plsRglm.importance$model <- "plsRglm"
      # export the variable importance
      write.csv(plsRglm.importance, paste0(Path,"All_Production/Machine_Models/VI_Eigen/Variable_Importance_ParameterPick_plsRglm_",a,"_",alpha.param,".",nt.param,".csv"),row.names=F)
      # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
      row <- data.frame('VI_Eigen', 'plsRglm', paste0(a,"_",alpha.param,".",nt.param), plsRglm.test.train.RMSE, plsRglm.test.train.cor)
      # writing row in the csv file
      write.table(row, file = csv_fname, sep = ",",
                  append = TRUE, quote = FALSE,
                  col.names = FALSE, row.names = FALSE)
      
    }
  }
}

```
### Machine Learning - Growth Curve Eigenvalues

  - train on multiple hyperparameters
  - export RMSE and R^2 values
  - Calculate Cor and RMSE for every hyperparameter value

```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))
library(tidyverse)
library(caret)
library(AppliedPredictiveModeling)
library(mlbench)
library(plsRglm) # for Partial Least Squares Regression
library(elasticnet) # for Least Absolute Shrinkage and Selection Operator Regression
library(nnet) # for Artificial Neural Network
library(randomForest)
library(ranger) # for Random Forest
library(e1071) # for Random Forest
library(kernlab) # for Support Vector Machines
library(pls) # pls
library(ggfortify)

Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
csv_fname = paste0(Path,"All_Production/Machine_Models/All_Data_Types_Models_RMSE_and_R.csv") # sample csv name
# read in all data
all.data <- read.csv(paste0(Path, "All_Production/Machine_Models/All_Data_Machine_Model_Input.csv"), row.names = 1) %>%
  dplyr::select("Plot.Year","Mean.Yld.bu.ac","Year","eigen.1","eigen.2") # select growth curve eigenvalues

############################
## CHANGE HYPERPARAMETERS ## 
############################
# for loop to go through each year as test
for (a in c("2020","2021","2022")) {
  # identify the train set
  train <- all.data %>%
    filter(Year != a)
  # identify the test set
  test <- all.data %>%
    filter(Year == a)
  # set rf hyperparameters to go through
  param.rf <- c(2,3,4,5,6,7,8,9,10,11)
  # for loop to go through hyperparameters
  for(b in 1:length(param.rf)) {
    mtry.param <- param.rf[b]
    ## RF - mtry ##
    rf.model = train(Mean.Yld.bu.ac ~ .,
                     data = train %>% 
                       dplyr::select(c(2,4:ncol(train))),
                     method = 'rf',
                     metric = "RMSE",
                     tuneGrid = expand.grid(mtry = mtry.param), #mtry = c(20:30)),
                     trControl = trainControl(
                       method = "none",
                       index = groupKFold(train$Year, k = 2)))
    rf.train.RMSE <- min(rf.model$results["RMSE"])
    # predict on test set using model
    rf.preds <- predict(rf.model,test) # will automatically take columns specified in train
    # test using cor
    rf.test.train.cor <- cor(rf.preds, test$Mean.Yld.bu.ac)
    pdf(paste0(Path, "All_Production/Machine_Models/Growth_Eigen/Correlation_PredictedvsActual_ParameterPick_rf_",a,"_",mtry.param,".pdf"))
    plot(rf.preds, test$Mean.Yld.bu.ac,
         xlab = "Predicted Yield",
         ylab = "Actual Yield",
         col = test$Year,
         main = paste0("rf model R^2:",round(rf.test.train.cor,2)^2)) +
      abline(lm(test$Mean.Yld.bu.ac ~ rf.preds), col = "red")
    dev.off()
    # get RMSE between test predictions and real
    rf.test.train.RMSE <- RMSE(rf.preds,test$Mean.Yld.bu.ac)
    # tells the overall variable importance in the chosen model
    rf.importance <- varImp(rf.model, scale = T)
    rf.importance <- as.data.frame(rf.importance$importance)
    rf.importance$predictors <- rownames(rf.importance)
    rf.importance$model <- "rf"
    # export the variable importance
    write.csv(rf.importance, paste0(Path, "All_Production/Machine_Models/Growth_Eigen/Variable_Importance_ParameterPick_rf_",a,"_",mtry.param,".csv"), row.names = F)
    # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
    row <- data.frame('Growth_Eigen', 'rf', paste0(a,"_",mtry.param), rf.test.train.RMSE, rf.test.train.cor)
    # writing row in the csv file
    write.table(row, file = csv_fname, sep = ",",
                append = TRUE, quote = FALSE,
                col.names = FALSE, row.names = FALSE)
  }
  
  # set pls hyperparameters to go through
  param.ncomp <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21)
  for (c in 1:length(param.ncomp)) {
    ncomp.param <- param.ncomp[c]
    ## PLS MODEL - ncomp ##
    pls.model = train(Mean.Yld.bu.ac ~ .,
                      data = train %>% 
                        dplyr::select(c(2,4:ncol(train))),
                      method = 'pls',
                      metric = "RMSE",
                      tuneGrid = expand.grid(ncomp=ncomp.param),
                      trControl = trainControl(## 10-fold CV
                        method = "none",
                        index = groupKFold(train$Year, k = length(unique(train$Year)))))
    pls.train.RMSE <- min(pls.model$resample[1])
    # predict on test set using model
    pls.preds <- predict(pls.model, test) # will take the columns specified in train
    # test using cor
    pls.test.train.cor <- cor(pls.preds, test$Mean.Yld.bu.ac)
    # plot correlation between test predictions and test actual yield
    pdf(paste0(Path, "All_Production/Machine_Models/Growth_Eigen/Correlation_PredictedvsActual_ParameterPick_pls_",a,"_",ncomp.param,".pdf"))
    plot(pls.preds, test$Mean.Yld.bu.ac,
         xlab = "Predicted Yield",
         ylab = "Actual Yield",
         col = test$Year,
         main = paste0("pls model R^2:",round(pls.test.train.cor,2)^2)) +
      abline(lm(test$Mean.Yld.bu.ac ~ pls.preds), col = "red")
    dev.off()
    # get RMSE between test predictions and real
    pls.test.train.RMSE <- RMSE(pls.preds,test$Mean.Yld.bu.ac)
    # tells the overall variable importance in the chosen model
    pls.importance <- varImp(pls.model, scale = T)
    pls.importance <- as.data.frame(pls.importance$importance)
    pls.importance$predictors <- rownames(pls.importance)
    pls.importance$model <- "pls"
    # export the variable importance
    write.csv(pls.importance, paste0(Path, "All_Production/Machine_Models/Growth_Eigen/Variable_Importance_ParameterPick_pls_",a,"_",ncomp.param,".csv"), row.names = F)
    # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
    row <- data.frame('Growth_Eigen', 'pls', paste0(a,"_",ncomp.param), pls.test.train.RMSE, pls.test.train.cor)
    # writing row in the csv file
    write.table(row, file = csv_fname, sep = ",",
                append = TRUE, quote = FALSE,
                col.names = FALSE, row.names = FALSE)
  }
  # set plsRglm hyperparameters to go through
  param.alpha <- c(0.001,0.01,0.1,1)
  param.nt <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)
  # for loop to go through alpha.pvals.expli hyperparameter values
  for(d in 1:length(param.alpha)) { 
    alpha.param <- param.alpha[d]
    # for loop to go through nt hyperparameter values
    for(e in 1:length(param.nt)) {
      nt.param <- param.nt[e]
      ## PLSRGLM - nt, alpha.pvals.expli ##
      plsRglm.model = train(Mean.Yld.bu.ac ~ .,
                            data = train %>% 
                              dplyr::select(c(2,4:ncol(train))),
                            method = 'plsRglm',
                            metric = "RMSE",
                            tuneGrid = expand.grid(nt = nt.param, alpha.pvals.expli = alpha.param),
                            trControl = trainControl(
                              method = "none",
                              index = groupKFold(train$Year, k = 2)))
      plsRglm.train.RMSE <- min(plsRglm.model$resample[1])
      # predict on test set using model
      plsRglm.preds <- predict(plsRglm.model, test) # will take the columns specified in train
      # test using cor
      plsRglm.test.train.cor <- cor(plsRglm.preds, test$Mean.Yld.bu.ac)
      # plot correlation between test predictions and test actual yield
      pdf(paste0(Path, "All_Production/Machine_Models/Growth_Eigen/Correlation_PredictedvsActual_ParameterPick_plsRglm_",a,"_",alpha.param,".",nt.param,".pdf"))
      plot(plsRglm.preds, test$Mean.Yld.bu.ac,
           xlab = "Predicted Yield",
           ylab = "Actual Yield",
           col = test$Year,
           main = paste0("plsRglm model R^2:",round(plsRglm.test.train.cor,2)^2)) +
        abline(lm(test$Mean.Yld.bu.ac ~ plsRglm.preds), col = "red")
      dev.off()
      # get RMSE between test predictions and real
      plsRglm.test.train.RMSE <- RMSE(plsRglm.preds,test$Mean.Yld.bu.ac)
      # tells the overall variable importance in the chosen model
      plsRglm.importance <- varImp(plsRglm.model, scale = T)
      plsRglm.importance <- as.data.frame(plsRglm.importance$importance)
      plsRglm.importance$predictors <- rownames(plsRglm.importance)
      plsRglm.importance$model <- "plsRglm"
      # export the variable importance
      write.csv(plsRglm.importance, paste0(Path,"All_Production/Machine_Models/Growth_Eigen/Variable_Importance_ParameterPick_plsRglm_",a,"_",alpha.param,".",nt.param,".csv"),row.names=F)
      # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
      row <- data.frame('Growth_Eigen', 'plsRglm', paste0(a,"_",alpha.param,".",nt.param), plsRglm.test.train.RMSE, plsRglm.test.train.cor)
      # writing row in the csv file
      write.table(row, file = csv_fname, sep = ",",
                  append = TRUE, quote = FALSE,
                  col.names = FALSE, row.names = FALSE)
      
    }
  }
}

```
### Machine Learning - Plant Height, Growth Rates, and Growth Curve Eigenvalues

  - train on multiple hyperparameters
  - export RMSE and R^2 values
  - Calculate Cor and RMSE for every hyperparameter value

```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))
library(tidyverse)
library(caret)
library(AppliedPredictiveModeling)
library(mlbench)
library(plsRglm) # for Partial Least Squares Regression
library(elasticnet) # for Least Absolute Shrinkage and Selection Operator Regression
library(nnet) # for Artificial Neural Network
library(randomForest)
library(ranger) # for Random Forest
library(e1071) # for Random Forest
library(kernlab) # for Support Vector Machines
library(pls) # pls
library(ggfortify)

Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
csv_fname = paste0(Path,"All_Production/Machine_Models/All_Data_Types_Models_RMSE_and_R.csv") # sample csv name
# read in all data
all.data <- read.csv(paste0(Path, "All_Production/Machine_Models/All_Data_Machine_Model_Input.csv"), row.names = 1) %>%
  dplyr::select("Plot.Year","Mean.Yld.bu.ac","Year",starts_with("X"),"exponential.growth","start.expo","end.expo","eigen.1","eigen.2") # select plant height, growth rate, and growth curve eigenvalues

############################
## CHANGE HYPERPARAMETERS ## 
############################
# for loop to go through each year as test
for (a in c("2020","2021","2022")) {
  # identify the train set
  train <- all.data %>%
    filter(Year != a)
  # identify the test set
  test <- all.data %>%
    filter(Year == a)
  # set rf hyperparameters to go through
  param.rf <- c(2,3,4,5,6,7,8,9,10,11)
  # for loop to go through hyperparameters
  for(b in 1:length(param.rf)) {
    mtry.param <- param.rf[b]
    ## RF - mtry ##
    rf.model = train(Mean.Yld.bu.ac ~ .,
                     data = train %>% 
                       dplyr::select(c(2,4:ncol(train))),
                     method = 'rf',
                     metric = "RMSE",
                     tuneGrid = expand.grid(mtry = mtry.param), #mtry = c(20:30)),
                     trControl = trainControl(
                       method = "none",
                       index = groupKFold(train$Year, k = 2)))
    rf.train.RMSE <- min(rf.model$results["RMSE"])
    # predict on test set using model
    rf.preds <- predict(rf.model,test) # will automatically take columns specified in train
    # test using cor
    rf.test.train.cor <- cor(rf.preds, test$Mean.Yld.bu.ac)
    pdf(paste0(Path, "All_Production/Machine_Models/Height_Rate_Eigen/Correlation_PredictedvsActual_ParameterPick_rf_",a,"_",mtry.param,".pdf"))
    plot(rf.preds, test$Mean.Yld.bu.ac,
         xlab = "Predicted Yield",
         ylab = "Actual Yield",
         col = test$Year,
         main = paste0("rf model R^2:",round(rf.test.train.cor,2)^2)) +
      abline(lm(test$Mean.Yld.bu.ac ~ rf.preds), col = "red")
    dev.off()
    # get RMSE between test predictions and real
    rf.test.train.RMSE <- RMSE(rf.preds,test$Mean.Yld.bu.ac)
    # tells the overall variable importance in the chosen model
    rf.importance <- varImp(rf.model, scale = T)
    rf.importance <- as.data.frame(rf.importance$importance)
    rf.importance$predictors <- rownames(rf.importance)
    rf.importance$model <- "rf"
    # export the variable importance
    write.csv(rf.importance, paste0(Path, "All_Production/Machine_Models/Height_Rate_Eigen/Variable_Importance_ParameterPick_rf_",a,"_",mtry.param,".csv"), row.names = F)
    # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
    row <- data.frame('Height_Rate_Eigen', 'rf', paste0(a,"_",mtry.param), rf.test.train.RMSE, rf.test.train.cor)
    # writing row in the csv file
    write.table(row, file = csv_fname, sep = ",",
                append = TRUE, quote = FALSE,
                col.names = FALSE, row.names = FALSE)
  }
  
  # set pls hyperparameters to go through
  param.ncomp <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21)
  for (c in 1:length(param.ncomp)) {
    ncomp.param <- param.ncomp[c]
    ## PLS MODEL - ncomp ##
    pls.model = train(Mean.Yld.bu.ac ~ .,
                      data = train %>% 
                        dplyr::select(c(2,4:ncol(train))),
                      method = 'pls',
                      metric = "RMSE",
                      tuneGrid = expand.grid(ncomp=ncomp.param),
                      trControl = trainControl(## 10-fold CV
                        method = "none",
                        index = groupKFold(train$Year, k = length(unique(train$Year)))))
    pls.train.RMSE <- min(pls.model$resample[1])
    # predict on test set using model
    pls.preds <- predict(pls.model, test) # will take the columns specified in train
    # test using cor
    pls.test.train.cor <- cor(pls.preds, test$Mean.Yld.bu.ac)
    # plot correlation between test predictions and test actual yield
    pdf(paste0(Path, "All_Production/Machine_Models/Height_Rate_Eigen/Correlation_PredictedvsActual_ParameterPick_pls_",a,"_",ncomp.param,".pdf"))
    plot(pls.preds, test$Mean.Yld.bu.ac,
         xlab = "Predicted Yield",
         ylab = "Actual Yield",
         col = test$Year,
         main = paste0("pls model R^2:",round(pls.test.train.cor,2)^2)) +
      abline(lm(test$Mean.Yld.bu.ac ~ pls.preds), col = "red")
    dev.off()
    # get RMSE between test predictions and real
    pls.test.train.RMSE <- RMSE(pls.preds,test$Mean.Yld.bu.ac)
    # tells the overall variable importance in the chosen model
    pls.importance <- varImp(pls.model, scale = T)
    pls.importance <- as.data.frame(pls.importance$importance)
    pls.importance$predictors <- rownames(pls.importance)
    pls.importance$model <- "pls"
    # export the variable importance
    write.csv(pls.importance, paste0(Path, "All_Production/Machine_Models/Height_Rate_Eigen/Variable_Importance_ParameterPick_pls_",a,"_",ncomp.param,".csv"), row.names = F)
    # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
    row <- data.frame('Height_Rate_Eigen', 'pls', paste0(a,"_",ncomp.param), pls.test.train.RMSE, pls.test.train.cor)
    # writing row in the csv file
    write.table(row, file = csv_fname, sep = ",",
                append = TRUE, quote = FALSE,
                col.names = FALSE, row.names = FALSE)
  }
  # set plsRglm hyperparameters to go through
  param.alpha <- c(0.001,0.01,0.1,1)
  param.nt <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)
  # for loop to go through alpha.pvals.expli hyperparameter values
  for(d in 1:length(param.alpha)) { 
    alpha.param <- param.alpha[d]
    # for loop to go through nt hyperparameter values
    for(e in 1:length(param.nt)) {
      nt.param <- param.nt[e]
      ## PLSRGLM - nt, alpha.pvals.expli ##
      plsRglm.model = train(Mean.Yld.bu.ac ~ .,
                            data = train %>% 
                              dplyr::select(c(2,4:ncol(train))),
                            method = 'plsRglm',
                            metric = "RMSE",
                            tuneGrid = expand.grid(nt = nt.param, alpha.pvals.expli = alpha.param),
                            trControl = trainControl(
                              method = "none",
                              index = groupKFold(train$Year, k = 2)))
      plsRglm.train.RMSE <- min(plsRglm.model$resample[1])
      # predict on test set using model
      plsRglm.preds <- predict(plsRglm.model, test) # will take the columns specified in train
      # test using cor
      plsRglm.test.train.cor <- cor(plsRglm.preds, test$Mean.Yld.bu.ac)
      # plot correlation between test predictions and test actual yield
      pdf(paste0(Path, "All_Production/Machine_Models/Height_Rate_Eigen/Correlation_PredictedvsActual_ParameterPick_plsRglm_",a,"_",alpha.param,".",nt.param,".pdf"))
      plot(plsRglm.preds, test$Mean.Yld.bu.ac,
           xlab = "Predicted Yield",
           ylab = "Actual Yield",
           col = test$Year,
           main = paste0("plsRglm model R^2:",round(plsRglm.test.train.cor,2)^2)) +
        abline(lm(test$Mean.Yld.bu.ac ~ plsRglm.preds), col = "red")
      dev.off()
      # get RMSE between test predictions and real
      plsRglm.test.train.RMSE <- RMSE(plsRglm.preds,test$Mean.Yld.bu.ac)
      # tells the overall variable importance in the chosen model
      plsRglm.importance <- varImp(plsRglm.model, scale = T)
      plsRglm.importance <- as.data.frame(plsRglm.importance$importance)
      plsRglm.importance$predictors <- rownames(plsRglm.importance)
      plsRglm.importance$model <- "plsRglm"
      # export the variable importance
      write.csv(plsRglm.importance, paste0(Path,"All_Production/Machine_Models/Height_Rate_Eigen/Variable_Importance_ParameterPick_plsRglm_",a,"_",alpha.param,".",nt.param,".csv"),row.names=F)
      # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
      row <- data.frame('Height_Rate_Eigen', 'plsRglm', paste0(a,"_",alpha.param,".",nt.param), plsRglm.test.train.RMSE, plsRglm.test.train.cor)
      # writing row in the csv file
      write.table(row, file = csv_fname, sep = ",",
                  append = TRUE, quote = FALSE,
                  col.names = FALSE, row.names = FALSE)
    }
  }
}
```
### Machine Learning - Plant Heights and Growth Rates

  - train on multiple hyperparameters
  - export RMSE and R^2 values
  - Calculate Cor and RMSE for every hyperparameter value

```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))
library(tidyverse)
library(caret)
library(AppliedPredictiveModeling)
library(mlbench)
library(plsRglm) # for Partial Least Squares Regression
library(elasticnet) # for Least Absolute Shrinkage and Selection Operator Regression
library(nnet) # for Artificial Neural Network
library(randomForest)
library(ranger) # for Random Forest
library(e1071) # for Random Forest
library(kernlab) # for Support Vector Machines
library(pls) # pls
library(ggfortify)

Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
csv_fname = paste0(Path,"All_Production/Machine_Models/All_Data_Types_Models_RMSE_and_R.csv") # sample csv name
# read in all data
all.data <- read.csv(paste0(Path, "All_Production/Machine_Models/All_Data_Machine_Model_Input.csv"), row.names = 1) %>%
  dplyr::select("Plot.Year","Mean.Yld.bu.ac","Year",starts_with("X")) # select growth curve eigenvalues

############################
## CHANGE HYPERPARAMETERS ## 
############################
# for loop to go through each year as test
for (a in c("2020","2021","2022")) {
  # identify the train set
  train <- all.data %>%
    filter(Year != a)
  # identify the test set
  test <- all.data %>%
    filter(Year == a)
  # set rf hyperparameters to go through
  param.rf <- c(2,3,4,5,6,7,8,9,10,11)
  # for loop to go through hyperparameters
  for(b in 1:length(param.rf)) {
    mtry.param <- param.rf[b]
    ## RF - mtry ##
    rf.model = train(Mean.Yld.bu.ac ~ .,
                     data = train %>% 
                       dplyr::select(c(2,4:ncol(train))),
                     method = 'rf',
                     metric = "RMSE",
                     tuneGrid = expand.grid(mtry = mtry.param), #mtry = c(20:30)),
                     trControl = trainControl(
                       method = "none",
                       index = groupKFold(train$Year, k = 2)))
    rf.train.RMSE <- min(rf.model$results["RMSE"])
    # predict on test set using model
    rf.preds <- predict(rf.model,test) # will automatically take columns specified in train
    # test using cor
    rf.test.train.cor <- cor(rf.preds, test$Mean.Yld.bu.ac)
    pdf(paste0(Path, "All_Production/Machine_Models/Height_Rate/Correlation_PredictedvsActual_ParameterPick_rf_",a,"_",mtry.param,".pdf"))
    plot(rf.preds, test$Mean.Yld.bu.ac,
         xlab = "Predicted Yield",
         ylab = "Actual Yield",
         col = test$Year,
         main = paste0("rf model R^2:",round(rf.test.train.cor,2)^2)) +
      abline(lm(test$Mean.Yld.bu.ac ~ rf.preds), col = "red")
    dev.off()
    # get RMSE between test predictions and real
    rf.test.train.RMSE <- RMSE(rf.preds,test$Mean.Yld.bu.ac)
    # tells the overall variable importance in the chosen model
    rf.importance <- varImp(rf.model, scale = T)
    rf.importance <- as.data.frame(rf.importance$importance)
    rf.importance$predictors <- rownames(rf.importance)
    rf.importance$model <- "rf"
    # export the variable importance
    write.csv(rf.importance, paste0(Path, "All_Production/Machine_Models/Height_Rate/Variable_Importance_ParameterPick_rf_",a,"_",mtry.param,".csv"), row.names = F)
    # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
    row <- data.frame('Height_Rate', 'rf', paste0(a,"_",mtry.param), rf.test.train.RMSE, rf.test.train.cor)
    # writing row in the csv file
    write.table(row, file = csv_fname, sep = ",",
                append = TRUE, quote = FALSE,
                col.names = FALSE, row.names = FALSE)
  }
  
  # set pls hyperparameters to go through
  param.ncomp <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21)
  for (c in 1:length(param.ncomp)) {
    ncomp.param <- param.ncomp[c]
    ## PLS MODEL - ncomp ##
    pls.model = train(Mean.Yld.bu.ac ~ .,
                      data = train %>% 
                        dplyr::select(c(2,4:ncol(train))),
                      method = 'pls',
                      metric = "RMSE",
                      tuneGrid = expand.grid(ncomp=ncomp.param),
                      trControl = trainControl(## 10-fold CV
                        method = "none",
                        index = groupKFold(train$Year, k = length(unique(train$Year)))))
    pls.train.RMSE <- min(pls.model$resample[1])
    # predict on test set using model
    pls.preds <- predict(pls.model, test) # will take the columns specified in train
    # test using cor
    pls.test.train.cor <- cor(pls.preds, test$Mean.Yld.bu.ac)
    # plot correlation between test predictions and test actual yield
    pdf(paste0(Path, "All_Production/Machine_Models/Height_Rate/Correlation_PredictedvsActual_ParameterPick_pls_",a,"_",ncomp.param,".pdf"))
    plot(pls.preds, test$Mean.Yld.bu.ac,
         xlab = "Predicted Yield",
         ylab = "Actual Yield",
         col = test$Year,
         main = paste0("pls model R^2:",round(pls.test.train.cor,2)^2)) +
      abline(lm(test$Mean.Yld.bu.ac ~ pls.preds), col = "red")
    dev.off()
    # get RMSE between test predictions and real
    pls.test.train.RMSE <- RMSE(pls.preds,test$Mean.Yld.bu.ac)
    # tells the overall variable importance in the chosen model
    pls.importance <- varImp(pls.model, scale = T)
    pls.importance <- as.data.frame(pls.importance$importance)
    pls.importance$predictors <- rownames(pls.importance)
    pls.importance$model <- "pls"
    # export the variable importance
    write.csv(pls.importance, paste0(Path, "All_Production/Machine_Models/Height_Rate/Variable_Importance_ParameterPick_pls_",a,"_",ncomp.param,".csv"), row.names = F)
    # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
    row <- data.frame('Height_Rate', 'pls', paste0(a,"_",ncomp.param), pls.test.train.RMSE, pls.test.train.cor)
    # writing row in the csv file
    write.table(row, file = csv_fname, sep = ",",
                append = TRUE, quote = FALSE,
                col.names = FALSE, row.names = FALSE)
  }
  # set plsRglm hyperparameters to go through
  param.alpha <- c(0.001,0.01,0.1,1)
  param.nt <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)
  # for loop to go through alpha.pvals.expli hyperparameter values
  for(d in 1:length(param.alpha)) { 
    alpha.param <- param.alpha[d]
    # for loop to go through nt hyperparameter values
    for(e in 1:length(param.nt)) {
      nt.param <- param.nt[e]
      ## PLSRGLM - nt, alpha.pvals.expli ##
      plsRglm.model = train(Mean.Yld.bu.ac ~ .,
                            data = train %>% 
                              dplyr::select(c(2,4:ncol(train))),
                            method = 'plsRglm',
                            metric = "RMSE",
                            tuneGrid = expand.grid(nt = nt.param, alpha.pvals.expli = alpha.param),
                            trControl = trainControl(
                              method = "none",
                              index = groupKFold(train$Year, k = 2)))
      plsRglm.train.RMSE <- min(plsRglm.model$resample[1])
      # predict on test set using model
      plsRglm.preds <- predict(plsRglm.model, test) # will take the columns specified in train
      # test using cor
      plsRglm.test.train.cor <- cor(plsRglm.preds, test$Mean.Yld.bu.ac)
      # plot correlation between test predictions and test actual yield
      pdf(paste0(Path, "All_Production/Machine_Models/Height_Rate/Correlation_PredictedvsActual_ParameterPick_plsRglm_",a,"_",alpha.param,".",nt.param,".pdf"))
      plot(plsRglm.preds, test$Mean.Yld.bu.ac,
           xlab = "Predicted Yield",
           ylab = "Actual Yield",
           col = test$Year,
           main = paste0("plsRglm model R^2:",round(plsRglm.test.train.cor,2)^2)) +
        abline(lm(test$Mean.Yld.bu.ac ~ plsRglm.preds), col = "red")
      dev.off()
      # get RMSE between test predictions and real
      plsRglm.test.train.RMSE <- RMSE(plsRglm.preds,test$Mean.Yld.bu.ac)
      # tells the overall variable importance in the chosen model
      plsRglm.importance <- varImp(plsRglm.model, scale = T)
      plsRglm.importance <- as.data.frame(plsRglm.importance$importance)
      plsRglm.importance$predictors <- rownames(plsRglm.importance)
      plsRglm.importance$model <- "plsRglm"
      # export the variable importance
      write.csv(plsRglm.importance, paste0(Path,"All_Production/Machine_Models/Height_Rate/Variable_Importance_ParameterPick_plsRglm_",a,"_",alpha.param,".",nt.param,".csv"),row.names=F)
      # append the RMSE and R^2 (Data, model, year, RMSE,R2 )
      row <- data.frame('Height_Rate', 'plsRglm', paste0(a,"_",alpha.param,".",nt.param), plsRglm.test.train.RMSE, plsRglm.test.train.cor)
      # writing row in the csv file
      write.table(row, file = csv_fname, sep = ",",
                  append = TRUE, quote = FALSE,
                  col.names = FALSE, row.names = FALSE)
      
    }
  }
}

```

# Descriptive Plots
## Plot Curve Descriptive Graphs
```{r}
# clear the global environment
rm(list=ls(all=TRUE))

# load library
library(tidyverse)
## INPUTS ##
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- c("2020","2021","2022")

# for loop to go through all years
for (a in 1:length(Year)) {
  ## PLOT RAW DATA ##
  # read in rough plant height data
  data <- read.csv(paste0(Path,Year[a],"/Waseca/Production_1_",Year[a],"/Data_Analysis/Rough_Plant_Height_WholeField_Waseca_",Year[a],"_1.csv"))
  # make a long form of the data
  data.l <- data %>%
    pivot_longer(cols = 2:ncol(.), names_to = "Date", values_to = "Height") %>%
    separate(Date, into = c(NA,"Date"), sep = "X")
  # plot the data
  pdf(paste0(Path,Year[a],"/Waseca/Production_1_",Year[a],"/Data_Analysis/Descriptive_Plots_4Figures/Rough_Plant_Height_GrowthCurve_",Year[a],"_1.pdf"), height = 2.5, width = 3)
  temp.plot <- data.l %>%
    ggplot(aes(x = Date, y = Height, group = X)) +
    geom_line() +
    theme_classic() +
    theme(
      axis.text.x = element_text(angle = 90)
    )
  print(temp.plot)
  dev.off()
  ## PLOT NORMALIZED DATA ##
  # read in normalized plant height data
  data <- read.csv(paste0(Path,Year[a],"/Waseca/Production_1_",Year[a],"/Data_Analysis/Normalized_Plant_Height_WholeField_Waseca_",Year[a],"_1.csv"))
  # make a long form of the data
  data.l <- data %>%
    pivot_longer(cols = 2:(ncol(.)-2), names_to = "Date", values_to = "Height") %>%
    separate(Date, into = c(NA, "Date"), sep = "X")
  # plot the data
  pdf(paste0(Path,Year[a],"/Waseca/Production_1_",Year[a],"/Data_Analysis/Descriptive_Plots_4Figures/Normalized_Plant_Height_GrowthCurve_",Year[a],"_1.pdf"), height = 2.5, width = 3)
  temp.plot <- data.l %>%
    ggplot(aes(x = Date, y = Height, group = X)) +
    geom_line() +
    theme_classic() +
    theme(
      axis.text.x = element_text(angle = 90)
    )
  print(temp.plot)
  dev.off()
  ## PLOT CLEAN DATA ##
  # read in normalized and cleaned plant height data
  data <- read.csv(paste0(Path,Year[a],"/Waseca/Production_1_",Year[a],"/Data_Analysis/Final_file_b4_ANOVA_",Year[a],".csv"))
  # make a long form of the data
  data.l <- data %>%
    pivot_longer(cols = 2:ncol(.), names_to = "Date", values_to = "Height") %>%
    separate(Date, into = c(NA, "Date"), sep = "X")
  # plot the data
  pdf(paste0(Path,Year[a],"/Waseca/Production_1_",Year[a],"/Data_Analysis/Descriptive_Plots_4Figures/Cleaned_Plant_Height_GrowthCurve_",Year[a],"_1.pdf"), height = 2.5, width = 3)
  temp.plot <- data.l %>%
    ggplot(aes(x = as.numeric(Date), y = Height, group = Square)) +
    geom_line() +
    theme_classic() +
    theme(
      axis.text.x = element_text(angle = 90)
    ) +
    xlab("Date (Accumulated GDDs)")
  print(temp.plot)
  dev.off()
  ## PLOT LOESS CURVES ##
  # read in the loess curves
  data <- read.delim(paste0(Path,Year[a],"/Waseca/Production_1_",Year[a],"/Data_Analysis/Loess_Predictions_Waseca_",Year[a],".txt"))
  # make a long form of the data & remove columns with only NA's
  data.l <- data %>%
    select_if(function(x) any(!is.na(x))) %>%
    pivot_longer(cols = 2:ncol(.), names_to = "Date", values_to = "Height") %>%
    separate(Date, into = c(NA, "Date"), sep = "X")
  # plot the data
  pdf(paste0(Path,Year[a],"/Waseca/Production_1_",Year[a],"/Data_Analysis/Descriptive_Plots_4Figures/Loess_Plant_Height_GrowthCurve_",Year[a],"_1.pdf"), height = 2.5, width = 3)
  temp.plot <- data.l %>%
    ggplot(aes(x = as.numeric(Date), y = Height, group = GDD)) +
    geom_line() +
    theme_classic() +
    theme(
      axis.text.x = element_text(angle = 90)
    ) +
    xlab("Date (Accumulated GDDs)")
  print(temp.plot)
  dev.off()
}
```


## Plot Variance Explained Graphs
```{r}
library("tidyverse")

# read in GDD variance explained
g.2020 <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/GDD_Variance_Explained_Grid_2020_Mean.Yld.bu.ac.txt")
g.2020$plot <- "grid"
g.2020$year <- "2020"
g.2021 <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/GDD_Variance_Explained_Grid_2021_Mean.Yld.bu.ac.txt")
g.2021$plot <- "grid"
g.2021$year <- "2021"
g.2022 <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/GDD_Variance_Explained_Grid_2022_Mean.Yld.bu.ac.txt")
g.2022$plot <- "grid"
g.2022$year <- "2022"
m.2020 <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/GDD_Variance_Explained_Manual_2020.txt")
m.2020$plot <- "manual"
m.2020$year <- "2020"
m.2021 <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/GDD_Variance_Explained_Manual_2021.txt")
m.2021$plot <- "manual"
m.2021$year <- "2021"
m.2022 <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/GDD_Variance_Explained_Manual_2022.txt")
m.2022$plot <- "manual"
m.2022$year <- "2022"

# rbind all variance explained together
GDD.all <- rbind(g.2020,g.2021,g.2022,m.2020,m.2021,m.2022)

# plot and export variance explained over time for Plant height
pdf("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/GDD_Variance_Explained.pdf", width = 8, height = 4)
GDD.all %>%
  mutate(plot.year = paste0(plot,year)) %>%
  separate(remove = F, anova.model.term, into = c("GDD","NDVI"), sep = "_") %>%
  separate(GDD, into = c("Residual","GDD"),sep = "X") %>%
  mutate_at("GDD", ~replace_na(.,"")) %>%
  mutate_at("NDVI", ~replace_na(.,"Height")) %>%
  mutate(GDD = paste0(Residual,GDD)) %>%
  # filter(plot == "grid") %>%
  ggplot(aes(x = plot.year, y = anova.model..R.2.)) +
  #geom_point(size = 5, color = GDD, shape = NDVI)
  geom_bar(stat = "identity", aes(fill = factor(GDD, levels=c("Residuals","350","400","429","450","500","546.5","550","600","650","695.5","700","739.5","750","800","813","844","850","900","950","986.5","1000","1050","1100","1150","1250","1350")), color = NDVI)) +
  theme_classic() +
  scale_fill_discrete(breaks = c("Residuals","350","400","429","450","500","546.5","550","600","650","695.5","700","739.5","750","800","813","844","850","900","950","986.5","1000","1050","1100","1150","1250","1350")) +
  scale_color_manual(values = c("white","black")) +
  labs(color = "Data Type", fill = "") +
  xlab("Plot Type and Year") +
  ylab("Percent Variance Explained") +
  scale_x_discrete(labels = c("grid2020" = "Grid 2020","grid2021" = "Grid 2021", "grid2022" = "Grid 2022", "manual2020" = "Manual 2020", "manual2021" = "Manual 2021", "manual2022" = "Manual 2022"))
dev.off()

# read in INT variance explained
g.2020 <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/INT_Variance_Explained_Grid_2020_Mean.Yld.bu.ac.txt")
g.2020$plot <- "grid"
g.2020$year <- "2020"
g.2021 <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/INT_Variance_Explained_Grid_2021_Mean.Yld.bu.ac.txt")
g.2021$plot <- "grid"
g.2021$year <- "2021"
g.2022 <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/INT_Variance_Explained_Grid_2022_Mean.Yld.bu.ac.txt")
g.2022$plot <- "grid"
g.2022$year <- "2022"
m.2020 <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/INT_Variance_Explained_Manual_2020.txt")
m.2020$plot <- "manual"
m.2020$year <- "2020"
m.2021 <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/INT_Variance_Explained_Manual_2021.txt")
m.2021$plot <- "manual"
m.2021$year <- "2021"
m.2022 <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/INT_Variance_Explained_Manual_2022.txt")
m.2022$plot <- "manual"
m.2022$year <- "2022"

# rbind all variance explained together
INT.all <- rbind(g.2020,g.2021,g.2022,m.2020,m.2021,m.2022)

# plot and export percent variance explained for growth rate
pdf("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/INT_Variance_Explained.pdf", width = 8, height = 3)
INT.all %>%
  mutate(plot.year = paste0(plot,year)) %>%
  separate(remove = F, anova.model.term, into = c("GDD","NDVI"), sep = "_") %>%
  separate(GDD, into = c("Residual","GDD"),sep = "X") %>%
  mutate_at("GDD", ~replace_na(.,"")) %>%
  mutate(NDVI = replace(NDVI, str_detect(NDVI, "X"),"Height")) %>%
  mutate_at("NDVI", ~replace_na(.,"Height")) %>%
  mutate(GDD = paste0(Residual,GDD)) %>%
  # filter(plot == "grid") %>%
  ggplot(aes(x = plot.year, y = anova.model..R.2.)) +
  #geom_point(size = 5, color = GDD, shape = NDVI)
  geom_bar(stat = "identity", aes(fill = factor(GDD, levels=c("Residuals","350","400","429","450","500","546.5","550","600","650","695.5","700","739.5","750","800","813","844","850","900","950","986.5","1000","1050","1100","1150","1200")), color = NDVI)) +
  theme_classic() +
  scale_fill_discrete(breaks = c("Residuals","350","400","429","450","500","546.5","550","600","650","695.5","700","739.5","750","800","813","844","850","900","950","986.5","1000","1050","1100","1150","1200")) +
  scale_color_manual(values = c("white","black")) +
  labs(color = "Data Type", fill = "") +
  xlab("Plot Type and Year") +
  ylab("Percent Variance Explained") +
  scale_x_discrete(labels = c("grid2020" = "Grid 2020","grid2021" = "Grid 2021", "grid2022" = "Grid 2022", "manual2020" = "Manual 2020", "manual2021" = "Manual 2021", "manual2022" = "Manual 2022"))
dev.off()

## COMBINE PLANT HEIGHT AND GROWTH RATE ##
GDD.temp <- GDD.all %>%
  mutate(plot.year = paste0(plot,year)) %>%
  separate(remove = F, anova.model.term, into = c("GDD","NDVI"), sep = "_") %>%
  separate(GDD, into = c("Residual","GDD"),sep = "X") %>%
  mutate_at("GDD", ~replace_na(.,"")) %>%
  mutate_at("NDVI", ~replace_na(.,"Height")) %>%
  mutate(GDD = paste0(Residual,GDD)) %>%
  mutate("data.type" = "Plant.Height")

INT.temp <- INT.all %>%
  mutate(plot.year = paste0(plot,year)) %>%
  separate(remove = F, anova.model.term, into = c("GDD","NDVI"), sep = "_") %>%
  separate(GDD, into = c("Residual","GDD"),sep = "X") %>%
  mutate_at("GDD", ~replace_na(.,"")) %>%
  mutate(NDVI = replace(NDVI, str_detect(NDVI, "X"),"Height")) %>%
  mutate_at("NDVI", ~replace_na(.,"Height")) %>%
  mutate(GDD = paste0(Residual,GDD)) %>%
  mutate("data.type" = "Growth.Rate")

All.data <- rbind(GDD.temp, INT.temp)

# plot and export percent variance explained for both height and growth rate
pdf("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/Both_Variance_Explained.pdf", width = 8, height = 2.5)

All.data %>%
  filter(plot == "grid") %>%
  #mutate(year.data = paste0(year,data.type)) %>%
  filter(GDD != "Residuals") %>%
ggplot(aes(x = factor(GDD,                                        levels=c("350","400","450","500","550","600","650","700","750","800","850","900","950","1000","1050","1100","1150","1200","1250","1350")), y = anova.model..R.2.)) +
  geom_point(size = 2,aes(color = year, shape = data.type)) +
  #geom_line(aes(group = year.data, color = year.data)) +
  theme_classic() +
  scale_color_manual(values = c("#00C8AF","#AF00C8","#C8AF00"),
                     breaks = c("2020","2021","2022")) +
  scale_shape_manual(values = c(19,17),
                     breaks = c("Plant.Height","Growth.Rate"), 
                     labels = c("Height","Rate")) +
  labs(color = "Year", shape = "Data Type") +
  xlab("Variables") +
  ylab("Percent Variance Explained") +
  theme(
    axis.text.x = element_text(angle = 90),
    legend.spacing.y = unit(0,"cm")
    ) 

All.data %>%
  filter(plot == "manual") %>%
  #mutate(year.data = paste0(year,data.type)) %>%
  filter(GDD != "Residuals") %>%
ggplot(aes(x = factor(GDD,                                        levels=c("350","400","429","450","500","546.5","550","600","650","695.5","700","739.5","750","800","813","844","850","900","950","986.5","1000","1050","1100","1150","1200","1250","1350")), y = anova.model..R.2.), group = year) +
  geom_point(size = 2, aes(color = year, shape = data.type)) +
  #geom_line(aes(group = year.data, color = year.data)) +
  theme_classic() +
  scale_color_manual(values = c("#00C8AF","#AF00C8","#C8AF00"),
                     breaks = c("2020","2021","2022")) +
  scale_shape_manual(values = c(19,17),
                     breaks = c("Plant.Height","Growth.Rate"), 
                     labels = c("Height","Rate")) +
  labs(color = "Year", shape = "Data Type") +
  xlab("Variables") +
  ylab("Percent Variance Explained") +
  theme(
    axis.text.x = element_text(angle = 90),
    legend.spacing.y = unit(0,"cm")
    ) 

All.data %>%
  filter(plot == "grid") %>%
  #mutate(year.data = paste0(year,data.type)) %>%
  filter(GDD != "Residuals") %>%
ggplot(aes(x = factor(GDD,                                        levels=c("350","400","450","500","550","600","650","700","750","800","850","900","950","1000","1050","1100","1150","1200","1250","1350")), y = anova.model..R.2.)) +
  geom_point(size = 2,aes(color = year, shape = data.type, alpha = data.type)) +
  #geom_line(aes(group = year.data, color = year.data)) +
  theme_classic() +
  scale_color_manual(values = c("#00C8AF","#AF00C8","#C8AF00"),
                     breaks = c("2020","2021","2022")) +
  scale_shape_manual(values = c(19,17),
                     breaks = c("Plant.Height","Growth.Rate"), 
                     labels = c("Height","Rate")) +
  scale_alpha_manual(values = c(1,0.1),
                     breaks = c("Plant.Height","Growth.Rate"),
                     labels = c("Height","Rate")) + 
  labs(color = "Year", shape = "Data Type", alpha = "Data Type") +
  xlab("Variables") +
  ylab("Percent Variance Explained") +
  theme(
    axis.text.x = element_text(angle = 90),
    legend.spacing.y = unit(0,"cm")
    ) 

All.data %>%
  filter(plot == "grid") %>%
  #mutate(year.data = paste0(year,data.type)) %>%
  filter(GDD != "Residuals") %>%
ggplot(aes(x = factor(GDD,                                        levels=c("350","400","450","500","550","600","650","700","750","800","850","900","950","1000","1050","1100","1150","1200","1250","1350")), y = anova.model..R.2.)) +
  geom_point(size = 2,aes(color = year, shape = data.type, alpha = data.type)) +
  #geom_line(aes(group = year.data, color = year.data)) +
  theme_classic() +
  scale_color_manual(values = c("#00C8AF","#AF00C8","#C8AF00"),
                     breaks = c("2020","2021","2022")) +
  scale_shape_manual(values = c(19,17),
                     breaks = c("Plant.Height","Growth.Rate"), 
                     labels = c("Height","Rate")) +
  scale_alpha_manual(values = c(0.1,1),
                     breaks = c("Plant.Height","Growth.Rate"),
                     labels = c("Height","Rate")) + 
  labs(color = "Year", shape = "Data Type", alpha = "Data Type") +
  xlab("Variables") +
  ylab("Percent Variance Explained") +
  theme(
    axis.text.x = element_text(angle = 90),
    legend.spacing.y = unit(0,"cm")
    ) 
dev.off()

```
## Plot AIC Combination Graphs
```{r}
library(tidyverse)
library(RColorBrewer)

Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/"

######################################################################
## CALCULATE VARIABLE IMPORTANCE BASED ON DIFFERENCE IN AICC VALUES ##
######################################################################

## INITIATE FILE FOR ALL DATA ## 
all.data <- as.data.frame(matrix(ncol = 5, nrow = 0))
colnames(all.data) <- c("Plot.Type","Data.Type","Year","Term","Var.Imp")

## CREATE FOR LOOPS TO GO THROUGH ALL DATA FILES ##
# for loop to go through all years
for (a in c("2020","2021","2022")) {
  # for loop to go through data types
  for (b in c("GDD","INT")) {
    # for loop to go through plot types
    for (c in c("Grid","Manual")) {
      # read in AIC combination file
      if (c == "Grid") {
        temp.file <- read.delim(paste0(Path,"AIC_Combinations_",b,"_",a,"_Mean.Yld.bu.ac.txt"), sep = " ")
      } else if (c == "Manual"){
        temp.file <- read.delim(paste0(Path,"AIC_Combinations_",b,"_",a,"_Mean.Yld.bu.ac_Manual.txt"), sep = " ")
      } # end of if statement to read in the right file
      # isolate the combination with lowest AIC
      temp.file.best <- temp.file[1,] %>%
        select_if(function(x) any(!is.na(x)))
      # for loop to go through all values in the lowest AIC combination 
      for (d in 2:(ncol(temp.file.best) - 5)) {
        # find the lowest combination minus one of the values
        temp.file.best.adj <- temp.file %>%
          filter_at(vars((colnames(temp.file.best)[c(1,(d+1):ncol(temp.file.best))])), all_vars(!is.na(.))) %>%
          filter_at(vars(-((colnames(temp.file.best))[c(1,(d+1):ncol(temp.file.best))])), all_vars(is.na(.)))
        # make a dataframe to hold the term and the difference in AIC
        temp <- data.frame("Plot.Type" = c,
                           "Data.Type" = b,
                           "Year" = a,
                           "Term" = colnames(temp.file.best)[d],
                           "Var.Imp" = (temp.file.best.adj$AICc - temp.file.best$AICc))
        # combine with full data frame
        all.data <- rbind(all.data,temp)
      } # end of terms for loop
    } # end of plot type for loop
  } # end of data type for loop
} # end of year for loop

## READ IN AIC PRESENCE ABSENCE FILE AND MANIPULATE FILES TO GO TOGETHER ##
all.comb <- read.csv(paste0(Path,"All_AIC_Presence_Absence.csv"))
# manipulate all.comb to combine with all.data
all.comb.1 <- all.comb %>%
  separate(Data.Type, into = c("Plot.Type","Year","Data.Type"), sep = " ") %>%
  pivot_longer(cols = 4:ncol(.), names_to = "Term", values_to = "Var.Pres")
# change Data.Type column values in all.data
all.data.1 <- all.data
all.data.1$Data.Type <- ifelse(all.data.1$Data.Type == "GDD", "Height", "Rate")
# merge all.comb.1 and all.data.1 keeping all values from all.comb.1
all <- merge(all.comb.1, all.data.1, by = c("Plot.Type","Data.Type","Year", "Term"), all.x = T)
# combine Var.Pres and Var.Imp
all <- all %>%
  mutate(Var.Des = ifelse(!is.na(Var.Imp), Var.Imp, Var.Pres))

## PLOT & EXPORT DESCRIPTIVE FIGURE ##
pdf(paste0(Path,"AIC_Presence_Absence.pdf"), height = 2, width = 8)
#grid
all %>%
  separate(Term, into = c(NA,"Term"), sep = "X") %>%
  separate(Term, into = c("Term","NDVI"), sep = "_") %>%
  mutate(NDVI = replace_na(NDVI,"Height")) %>%
  mutate(NDVI = ifelse(is.na(Var.Des), NA,NDVI)) %>%
  filter(Plot.Type == "Grid") %>%
  filter(!is.na(Var.Des)) %>%
  mutate(Descrip = paste0(Year, " ", Data.Type)) %>%
  # mutate(Var.Des.Scale = scale(Var.Des, center = F)) %>%
  ggplot(aes(x = as.numeric(Term), y = Descrip)) +
  geom_point(aes(shape = as.character(Var.Pres), size = as.character(Var.Pres), fill = Var.Des)) +
  theme_classic() +
  scale_shape_manual(values = c(13,21), labels = c("Absent","Present")) +
  scale_size_manual(values = c(2,3), labels = c("Absent", "Present")) +
  scale_fill_gradientn(colours = rainbow(3)) +
  xlab("Growing Degree Days") +
  ylab("Year and Data Type") +
  labs(fill = "Variable Importance", shape = element_blank(), size = element_blank())

all %>%
  separate(Term, into = c(NA,"Term"), sep = "X") %>%
  separate(Term, into = c("Term","NDVI"), sep = "_") %>%
  mutate(NDVI = replace_na(NDVI,"Height")) %>%
  mutate(NDVI = ifelse(is.na(Var.Des), NA,NDVI)) %>%
  filter(Plot.Type == "Grid") %>%
  filter(!is.na(Var.Des)) %>%
  mutate(Descrip = paste0(Year, " ", Data.Type)) %>%
  # mutate(Var.Des.Scale = scale(Var.Des, center = F)) %>%
  ggplot(aes(x = as.numeric(Term), y = fct_relevel(Descrip, "2022 Rate","2022 Height","2021 Rate","2021 Height", "2020 Rate","2020 Height"))) +
  geom_point(aes(shape = as.character(Var.Pres), size = as.character(Var.Pres), fill = Var.Des)) +
  theme_classic() +
  scale_shape_manual(values = c(13,21), labels = c("Absent","Present")) +
  scale_size_manual(values = c(2,3), labels = c("Absent", "Present")) +
  scale_fill_gradientn(
    values = scales::rescale(c(0,100,200,300,400,500,600,1600)),
    colors = c("#201100","#402200","#603300","#804400","#9F5400","#BF6500","#DF7600","#FF8700") 
    ) +
  xlab("Growing Degree Days") +
  ylab("Year and Data Type") +
  labs(fill = "Variable Importance", shape = element_blank(), size = element_blank())
# # manual
# all %>%
#   separate(Term, into = c(NA,"Term"), sep = "X") %>%
#   separate(Term, into = c("Term","NDVI"), sep = "_") %>%
#   mutate(NDVI = replace_na(NDVI,"Height")) %>%
#   mutate(NDVI = ifelse(is.na(Var.Des), NA,NDVI)) %>%
#   filter(Plot.Type == "Manual") %>%
#   filter(!is.na(Var.Des)) %>%
#   mutate(Descrip = paste0(Year, " ", Data.Type)) %>%
#   # mutate(Var.Des.Scale = scale(Var.Des, center = F)) %>%
#   ggplot(aes(x = factor(Term,levels = c("350","400","429","450","500","546.5","550","600","650","695.5","700","739.5","750","800","813","844","850","900","950","986.5","1000","1050","1100","1150","1200","1250","1300","1350")), y = Descrip)) +
#   geom_point(aes(shape = as.character(Var.Pres), size = as.character(Var.Pres),fill = Var.Des, color = NDVI)) +
#   theme_classic() +
#   scale_shape_manual(values = c(13,21), labels = c("Absent","Present")) +
#   scale_size_manual(values = c(2,3), labels = c("Absent", "Present")) +
#   scale_fill_gradientn(colours = rainbow(3)) +
#   scale_color_manual(values = c("#26DFD0","#F62AA0"), labels = c("Height","NDVI",""), na.value = "white") +
#   xlab("Growing Degree Days") +
#   ylab("Year and Data Type") +
#   theme(
#     axis.text.x = element_text(angle = 90),
#     legend.spacing.y = unit(0,"cm")
#   ) +
#   labs(fill = "Variable Importance", shape = element_blank(), size = element_blank(), color = element_blank())

dev.off()


# # read in file
# 
# # Plot of just grid plot - color and fill differentiation 
# pdf(paste0(Path,"AIC_Presence_Absence.pdf"))
# all.comb %>%
#   #pivot_longer(cols = 2:ncol(.), names_to = "Terms", values_to = "Presence.Absence") %>%
#   #separate(Terms, into = c(NA,"Terms"), sep = "X") %>%
#   #separate(Terms, into = c("Terms","NDVI"), sep = "_") %>%
#   #mutate(NDVI = replace_na(NDVI,"Height")) %>%
#   
#   #mutate(NDVI = ifelse(is.na(Presence.Absence), NA,NDVI)) %>%
#   #filter(str_detect(Data.Type,"Grid")) %>%
#   #filter(!is.na(Presence.Absence)) %>%
#   #separate(Data.Type, into = c("Grid","Year","Data.Type"), sep = " ") %>%
#   mutate(Data.Type = paste0(Year, " ", Data.Type)) %>%
#   ggplot(aes(x = as.numeric(Terms), y = Data.Type)) +
#   geom_point(shape = 21, size = 3,aes(fill = as.character(Presence.Absence))) +
#   theme_classic() +
#   scale_fill_manual(values = c("white","black"), labels = c("Absent","Present",""), na.value = "white") +
#   xlab("Growing Degree Days") +
#   ylab("Year and Data Type") +
#   theme(
#     legend.title = element_blank()
#   )
#   
# # Plot of just manual plot - color and fill differentiation 
# all.comb %>%
#   pivot_longer(cols = 2:ncol(.), names_to = "Terms", values_to = "Presence.Absence") %>%
#   separate(Terms, into = c(NA,"Terms"), sep = "X") %>%
#   separate(Terms, into = c("Terms","NDVI"), sep = "_") %>%
#   mutate(NDVI = replace_na(NDVI,"Height")) %>%
#   mutate(NDVI = ifelse(is.na(Presence.Absence), NA,NDVI)) %>%
#   filter(str_detect(Data.Type,"Manual")) %>%
#   filter(!is.na(Presence.Absence)) %>%
#   separate(Data.Type, into = c("Manual","Year","Data.Type"), sep = " ") %>%
#   mutate(Data.Type = paste0(Year, " ", Data.Type)) %>%
#   ggplot(aes(x = factor(Terms,levels = c("350","400","429","450","500","546.5","550","600","650","695.5","700","739.5","750","800","813","844","850","900","950","986.5","1000","1050","1100","1150","1200","1250","1300","1350")), y = Data.Type)) +
#   geom_point(shape = 21, size = 3,aes(fill = as.character(Presence.Absence),color = NDVI)) +
#   theme_classic() +
#   scale_color_manual(values = c("#26DFD0","#F62AA0"), labels = c("Height","NDVI",""), na.value = "white") +
#   scale_fill_manual(values = c("white","black"), labels = c("Absent","Present",""), na.value = "white") +
#   xlab("Growing Degree Days") +
#   ylab("Year and Data Type") +
#   theme(
#     legend.title = element_blank()
#   )
# 
# # Just manual plot - shape differentiation
# all.comb %>%
#   pivot_longer(cols = 2:ncol(.), names_to = "Terms", values_to = "Presence.Absence") %>%
#   separate(Terms, into = c(NA,"Terms"), sep = "X") %>%
#   separate(Terms, into = c("Terms","NDVI"), sep = "_") %>%
#   mutate(NDVI = replace_na(NDVI,"Height")) %>%
#   mutate(NDVI = ifelse(is.na(Presence.Absence), NA,NDVI)) %>%
#   filter(str_detect(Data.Type,"Manual")) %>%
#   filter(!is.na(Presence.Absence)) %>%
#   separate(Data.Type, into = c("Manual","Year","Data.Type"), sep = " ") %>%
#   mutate(Data.Type = paste0(Year, " ", Data.Type)) %>%
#   ggplot(aes(x = factor(Terms,levels = c("350","400","429","450","500","546.5","550","600","650","695.5","700","739.5","750","800","813","844","850","900","950","986.5","1000","1050","1100","1150","1200","1250","1300","1350")), y = Data.Type)) +
#   geom_point(aes(color = NDVI, shape = as.character(Presence.Absence), size = as.character(Presence.Absence))) +
#   theme_classic() +
#   scale_color_manual(values = c("black","red")) +
#   scale_shape_manual(values = c(13,1), labels = c("Absent","Present")) +
#   scale_size_manual(values = c(2,3), labels = c("Absent", "Present")) +
#   xlab("Growing Degree Days") +
#   ylab("Year and Data Type") +
#   theme(
#     legend.title = element_blank()
#   )
#  
# # Just grid plot - shape differentiation
# all.comb %>%
#   pivot_longer(cols = 2:ncol(.), names_to = "Terms", values_to = "Presence.Absence") %>%
#   separate(Terms, into = c(NA,"Terms"), sep = "X") %>%
#   separate(Terms, into = c("Terms","NDVI"), sep = "_") %>%
#   mutate(NDVI = replace_na(NDVI,"Height")) %>%
#   mutate(NDVI = ifelse(is.na(Presence.Absence), NA,NDVI)) %>%
#   filter(str_detect(Data.Type,"Grid")) %>%
#   filter(!is.na(Presence.Absence)) %>%
#   separate(Data.Type, into = c("Grid","Year","Data.Type"), sep = " ") %>%
#   mutate(Data.Type = paste0(Year, " ", Data.Type)) %>%
#   ggplot(aes(x = factor(Terms,levels = c("350","400","450","500","550","600","650","700","750","800","850","900","950","1000","1050","1100","1150","1200","1250","1300","1350")), y = Data.Type)) +
#   geom_point(aes(shape = as.character(Presence.Absence), size = as.character(Presence.Absence))) +
#   theme_classic() +
#   #scale_color_manual(values = c("black","red")) +
#   scale_shape_manual(values = c(13,1), labels = c("Absent","Present")) +
#   scale_size_manual(values = c(2,3), labels = c("Absent", "Present")) +
#   xlab("Growing Degree Days") +
#   ylab("Year and Data Type") +
#   theme(
#     legend.title = element_blank()
#   )
# dev.off()
```


## Plot Machine Learning Graphs
```{r}
library(tidyverse)

Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/All_Production/Machine_Models/"

#####################################
## R AND RMSE MODEL AND DATA PLOTS ##
#####################################

# read in RMSE and R2 data
data <- read.csv(paste0(Path,"All_Data_Types_Models_RMSE_and_R.csv"))

# plot R and RMSE data for all models and data types
pdf(paste0(Path,"All_Data_Types_Models_R_and_RMSE.pdf"), height = 4, width = 7)
data %>%
  filter(Data == "ModelPick") %>%
  separate(Year, into = c("Year","Parameters"), sep = "_") %>%
  pivot_longer(cols = c(RMSE,R), names_to = "Data.Type", values_to = "Values") %>%
  ggplot(aes(x = Year, y = Values)) +
  geom_violin() +
  geom_jitter(aes(color = Model)) +
  # geom_bar(stat = "identity",position = "dodge") +
  theme_light() +
  theme(legend.position = "bottom") +
  facet_wrap(~ Data.Type,  nrow = 2,scales = "free_y")

data %>%
  filter(Data != "ModelPick") %>%
  separate(Year, into = c("Year","Parameters"), sep = "_") %>%
  pivot_longer(cols = c(RMSE,R), names_to = "Data.Type", values_to = "Values") %>%
  filter(Data.Type == "R") %>%
  ggplot(aes(x = Year, y = Values)) +
  geom_violin(aes(fill = Model)) +
  geom_point(aes(color = Model, alpha=0.25, group = Model)) +
  # geom_bar(stat = "identity",position = "dodge") +
  theme_light() +
  theme(legend.position = "bottom") +
  facet_wrap(~ factor(Data, levels = c("VI_Eigen","Growth_Eigen","Height_Rate_Eigen","Height_Rate")), labeller = as_labeller(c("VI_Eigen" = "Only VI", "Growth_Eigen" = "Only Curve Eigen", "Height_Rate_Eigen" = "PH, GR, Curve Eigen", "Height_Rate" = "PH and GR", "RMSE" = "RMSE", "R" = "R")), nrow = 2, ncol = 4, scales = "free_y")
dev.off()


# plot R and RMSE data for all models and data types
pdf(paste0(Path,"All_Data_Types_Models_Hyperparameter_Performance.pdf"), height = 6, width = 7)
data %>%
  filter(Data != "ModelPick") %>%
  filter(Model == "pls") %>%
  separate(Year, into = c("Year","Parameters"), sep = "_") %>%
  pivot_longer(cols = c(RMSE,R), names_to = "Data.Type", values_to = "Values") %>%
  filter(Data.Type == "R") %>%
  ggplot(aes(x = as.numeric(Parameters), y = Values, group = Year, color = Year)) + 
  geom_point() + 
  geom_line() +
  xlab("ncomp Hyperparameter Value") +
  facet_wrap(~ Data, nrow = 3, scales = "free_x")
  
data %>%
  filter(Data != "ModelPick") %>%
  filter(Model == "plsRglm") %>%
  separate(Year, into = c("Year","Parameters"), sep = "_") %>%
  pivot_longer(cols = c(RMSE,R), names_to = "Data.Type", values_to = "Values") %>%
  filter(Data.Type == "R") %>%
  separate(Parameters, into = c("zero","alpha","nt"), sep = "[.]", fill = "left") %>%
  tidyr::unite("alpha","zero","alpha",sep = ".", na.rm = T) %>%
  unique() %>%
  ggplot(aes(x = as.numeric(nt), y = Values, group = Year, color = Year)) + 
  geom_point() + 
  geom_line() +
  xlab("nt Hyperparameter Value") +
  facet_wrap(~ as.character(alpha) +Data, ncol = 5, scales = "free_x")

data %>%
  filter(Data != "ModelPick") %>%
  filter(Model == "rf") %>%
  separate(Year, into = c("Year","Parameters"), sep = "_") %>%
  pivot_longer(cols = c(RMSE,R), names_to = "Data.Type", values_to = "Values") %>%
  filter(Data.Type == "R") %>%
  ggplot(aes(x = as.numeric(Parameters), y = Values, group = Year, color = Year)) + 
  geom_point() + 
  geom_line() +
  xlab("mtry Hyperparameter Value") +
  facet_wrap(~ Data, nrow = 3, scales = "free_x")
dev.off()

## PERFORMANCE AVERAGE ACROSS YEARS
pdf(paste0(Path,"All_Data_Types_Models_Hyperparameter_Average.pdf"), height = 6, width = 7)
data %>%
  filter(Data != "ModelPick") %>%
  filter(Model == "pls") %>%
  separate(Year, into = c("Year","Parameters"), sep = "_") %>%
  pivot_longer(cols = c(RMSE,R), names_to = "Data.Type", values_to = "Values") %>%
  filter(Data.Type == "R") %>%
  group_by(Data,Model,Parameters) %>%
  mutate(Performance = mean(Values)) %>%
  ggplot(aes(x = as.numeric(Parameters), y = Performance)) +
  geom_point() +
  xlab("ncomp hyperparameter value") +
  ylab("Correlation (R)") +
  facet_wrap(~Data)

data %>%
  filter(Data != "ModelPick") %>%
  filter(Model == "plsRglm") %>%
  separate(Year, into = c("Year","Parameters"), sep = "_") %>%
  pivot_longer(cols = c(RMSE,R), names_to = "Data.Type", values_to = "Values") %>%
  filter(Data.Type == "R") %>%
  separate(Parameters, into = c("zero","alpha","nt"), sep = "[.]", fill = "left") %>%
  tidyr::unite("alpha","zero","alpha",sep = ".", na.rm = T) %>%
  group_by(Data,Model,alpha,nt) %>%
  mutate(Performance = mean(Values)) %>%
  ggplot(aes(x = as.numeric(nt), y = Performance)) +
  geom_point() +
  xlab("nt hyperparameter value") +
  ylab("Correlation (R)") +
  facet_wrap(~alpha + Data)

data %>%
  filter(Data != "ModelPick") %>%
  filter(Model == "rf") %>%
  separate(Year, into = c("Year","Parameters"), sep = "_") %>%
  pivot_longer(cols = c(RMSE,R), names_to = "Data.Type", values_to = "Values") %>%
  filter(Data.Type == "R") %>%
  group_by(Data,Model,Parameters) %>%
  mutate(Performance = mean(Values)) %>%
  ggplot(aes(x = as.numeric(Parameters), y = Performance)) +
  geom_point() +
  xlab("mtry hyperparameter value") +
  ylab("Correlation (R)") +
  facet_wrap(~Data)
dev.off()

# plot R data
pdf(paste0(Path,"All_Data_Types_Models_R.pdf"), height = 9, width = 3.23)
data %>%
  filter(Data != "ModelPick") %>%
  separate(Year, into = c("Year","Parameter"), sep = "_") %>%
  ggplot(aes(x = Year, y = R, fill = Model)) +
  geom_boxplot(alpha = 0.25) +
  #geom_violin(alpha = 0.25) +
  geom_point(aes(color = Model), alpha = 0.25,position = position_dodge(width=0.75)) +
  theme_light() +
  theme(legend.position = "bottom") +
  facet_wrap(~ factor(Data, levels = c("Height_Rate","Height_Rate_Eigen","Growth_Eigen","VI_Eigen","All_Data")),
             labeller = as_labeller(c("Height_Rate" = "Height and Rate", "Height_Rate_Eigen" = "Height, Rate, and Curve Eigen",
                                      "Growth_Eigen" = "Curve Eigen", "VI_Eigen" = "Indice Eigen", 
                                      "All_Data" = "Height, Rate, Curve Eigen, and Indice Eigen")),
             nrow = 5)
dev.off()

# plot all RMSE data faceted as R above
pdf(paste0(Path,"All_Data_Types_Models_RMSE.pdf"), height = 9, width = 3.23)
data %>%
  filter(Data != "ModelPick") %>%
  filter(RMSE < 1000) %>%
  separate(Year, into = c("Year","Parameter"), sep = "_") %>%
  ggplot(aes(x = Year, y = RMSE, fill = Model)) +
  geom_boxplot(alpha = 0.25) +
  #geom_violin(alpha = 0.25) +
  geom_point(aes(color = Model), alpha = 0.25,position = position_dodge(width=0.75)) +
  theme_light() +
  theme(legend.position = "bottom") +
  facet_wrap(~ factor(Data, levels = c("Height_Rate","Height_Rate_Eigen","Growth_Eigen","VI_Eigen","All_Data")),
             labeller = as_labeller(c("Height_Rate" = "Height and Rate", "Height_Rate_Eigen" = "Height, Rate, and Curve Eigen",
                                      "Growth_Eigen" = "Curve Eigen", "VI_Eigen" = "Indice Eigen", 
                                      "All_Data" = "Height, Rate, Curve Eigen, and Indice Eigen")),
             nrow = 5)

dev.off()

##########################################
## PLOT BEST MODEL AND DATA COMBINATION ##
##########################################
## calculate data/model/hyperparameter average performance across years
temp <- data %>%
  filter(Data != "ModelPick") %>%
  separate(Year, into = c("Year","Parameters"), sep = "_") %>%
  group_by(Data,Model,Parameters) %>%
  mutate(AverageR = mean(R),
         AverageRMSE = mean(RMSE)) 

test <- data %>%
  separate(Year, into = c("Year","Parameters"), sep = "_") %>%
  group_by(Data,Model,Parameters) %>%
  mutate(`R average` = round(mean(R),3),
         `RMSE average` = round(mean(RMSE),2),
         R = round(R,digits = 2),
         RMSE = round(RMSE,digits =1)) %>%
  pivot_longer(cols = c("R","RMSE"),names_to = "metric", values_to = "value") %>%
  tidyr::unite(year.metric, Year, metric) %>%
  pivot_wider(names_from = year.metric, values_from = value) %>%
  tidyr::unite(`R (2020,2021,2022)`,`2020_R`,`2021_R`,`2022_R`, sep = ",") %>%
  mutate(`R (2020,2021,2022)` = paste0("(",`R (2020,2021,2022)`,")")) %>%
  tidyr::unite(`R average (2020,2021,2022)`,`R average`,`R (2020,2021,2022)`, sep = " ") %>%
  tidyr::unite(`RMSE (2020,2021,2022)`,`2020_RMSE`,`2021_RMSE`,`2022_RMSE`, sep = ",") %>%
  mutate(`RMSE (2020,2021,2022)` = paste0("(",`RMSE (2020,2021,2022)`,")")) %>%
  tidyr::unite(`RMSE average (2020,2021,2022)`,`RMSE average`,`RMSE (2020,2021,2022)`, sep = " ")

write.csv(test,paste0(Path,"All_Data_Performance_Rough_Table.csv"))

## Filter to just the best performing data/model/hyperparameter and plot
pdf(paste0(Path,"Best_Performing_Model_DataType_R_RMSE.pdf"), height = 1.5, width = 4)
# R
data %>%
  separate(Year, into = c("Year","Parameters"), sep = "_") %>% # separate year into year and parameter
  filter(Data == "All_Data", # filter data combination
         Model == "plsRglm", # filter model
         Parameters == "0.01.1" # filter to parameter values
         ) %>%
  ggplot(aes(x = Year, y = R, fill = as.character(Year))) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("#00C8AF","#AF00C8","#C8AF00")) +
  theme_light() +
  theme(
    legend.position = "none"
  )
# RMSE
data %>%
  separate(Year, into = c("Year","Parameters"), sep = "_") %>%
  filter(Data == "All_Data",
         Model == "plsRglm",
         Parameters == "0.01.1"
         ) %>%
  ggplot(aes(x = Year, y = RMSE, fill = as.character(Year))) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("#00C8AF","#AF00C8","#C8AF00")) +
  theme_light() +
  theme(
    legend.position = "none"
  )
dev.off()

## EVALUATE VARIABLE IMPORTANCE FOR BEST DATA/MODEL/HYPERPARAMETER COMBINATION
# read in variable importance files
variable.imp.2020 <- read.csv(paste0(Path,"All_Data/Variable_Importance_ParameterPick_plsRglm_2020_0.01.1.csv")) %>%
  mutate(year = "2020")
variable.imp.2021 <- read.csv(paste0(Path,"All_Data/Variable_Importance_ParameterPick_plsRglm_2021_0.01.1.csv")) %>%
  mutate(year = "2021")
variable.imp.2022 <- read.csv(paste0(Path,"All_Data/Variable_Importance_ParameterPick_plsRglm_2022_0.01.1.csv")) %>%
  mutate(year = "2022")
var.imp <- rbind(variable.imp.2020,variable.imp.2021,variable.imp.2022)
# Plot
pdf(paste0(Path,"Best_Performing_Model_DataType_VarImp.pdf"), height = 3.5, width = 8)
var.imp %>%
  ggplot(aes(x = factor(predictors, levels = unique(predictors)), y = Overall, color = year)) +
  geom_point() + 
  scale_color_manual(values = c("#00C8AF","#AF00C8","#C8AF00"), name = "Year") +
  theme_light() +
  theme(
    legend.position = "bottom",
    axis.text.x = element_text(angle = 90, size = 8)
  ) +
  xlab("Predictors") +
  ylab("Scaled Variable Importance") 

dev.off()

```
